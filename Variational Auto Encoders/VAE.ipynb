{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Variational Autoencoder\n",
    "\n",
    "### Generative models model a distribution of data over high dimensional space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #matrix math\n",
    "import tensorflow as tf #machine learning\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Import MINST data\n",
    "#The MNIST data is split into three parts: 55,000 data points of training data \n",
    "#10,000 points of test data and 5,000 points of validation data \n",
    "#very MNIST data point has two parts: an image of a handwritten digit \n",
    "#and a corresponding label. \n",
    "#We'll call the images \"x\" and the labels \"y\". \n",
    "#Both the training set and test set contain images and their corresponding labels; \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "n_pixels = 28*28\n",
    "# Input to the graph -- Tensorflow's MNIST images are (1, 784) vectors\n",
    "#x isn’t a specific value. \n",
    "#It’s a placeholder, a value that we’ll input when we ask TensorFlow \n",
    "#to run a computation. We want to be able to input any number of MNIST images, \n",
    "#each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of \n",
    "#floating-point numbers\n",
    "X = tf.placeholder(tf.float32, shape=([None, n_pixels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer creation functions\n",
    "#we could do this inline but cleaner to wrap it in respective functions\n",
    "\n",
    "#represent the strength of connections between units.\n",
    "def weight_variable(shape, name):\n",
    "    #Outputs random values from a truncated normal distribution.\n",
    "    #truncated means the value is either bounded below or above (or both)\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    #A Variable is a modifiable tensor that lives in TensorFlow’s graph of \n",
    "    #interacting operations. It can be used and even modified by the computation. \n",
    "    #For machine learning applications, one generally has the model parameters \n",
    "    #be Variables.\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "#Bias nodes are added to increase the flexibility of \n",
    "#the model to fit the data. Specifically, it allows the \n",
    "#network to fit the data when all input features are equal to 00, \n",
    "#and very likely decreases the bias of the fitted values elsewhere in the data space\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "#Neurons in a fully connected layer have full connections to \n",
    "#all activations in the previous layer, as seen in regular Neural Networks. \n",
    "#Their activations can hence be computed with a matrix multiplication followed by a \n",
    "#bias offset. \n",
    "def FC_layer(X, W, b):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "The encoder consists of a 2 layer, fully connected feed forward network that reduces the dimensionality from the original number of features (e.g. pixel) to the dimensionality of the latent space. This network has two outputs -- the mean and (log) standard deviation of a gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/2000/1*op0VO_QK4vMtCnXtmigDhA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "#our VAE model can parse the information spread thinly over the high-dimensional \n",
    "#observed world of pixels, and condense the most meaningful features into a \n",
    "#structured distribution over reduced (20) latent dimensions\n",
    "#latent = embedded space, we just see latent used in stochastic models in papers a lot\n",
    "#latent means not directly observed but are rather inferred\n",
    "latent_dim = 20\n",
    "#num neurons\n",
    "h_dim = 500\n",
    "\n",
    "#layer 1\n",
    "W_enc = weight_variable([n_pixels, h_dim], 'W_enc')\n",
    "b_enc = bias_variable([h_dim], 'b_enc')\n",
    "# tanh activation function to replicate original model\n",
    "#The tanh function, a.k.a. hyperbolic tangent function, \n",
    "#is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.\n",
    "#tanh or sigmoid? Whatever avoids the vanishing gradient problem!\n",
    "h_enc = tf.nn.tanh(FC_layer(X, W_enc, b_enc))\n",
    "\n",
    "#layer 2\n",
    "W_mu = weight_variable([h_dim, latent_dim], 'W_mu')\n",
    "b_mu = bias_variable([latent_dim], 'b_mu')\n",
    "mu = FC_layer(h_enc, W_mu, b_mu) #mean\n",
    "\n",
    "#instead of the encoder generating a vector of real values, \n",
    "#it will generate a vector of means and a vector of standard deviations.\n",
    "#for reparamterization\n",
    "W_logstd = weight_variable([h_dim, latent_dim], 'W_logstd')\n",
    "b_logstd = bias_variable([latent_dim], 'b_logstd')\n",
    "logstd = FC_layer(h_enc, W_logstd, b_logstd)\n",
    "\n",
    "# reparameterization trick - lets us backpropagate successfully\n",
    "#since normally gradient descent expects deterministic nodes\n",
    "#and we have stochastic nodes\n",
    "#distribution\n",
    "noise = tf.random_normal([1, latent_dim])\n",
    "#sample from the standard deviations (tf.exp computes exponential of x element-wise) \n",
    "#and add the mean \n",
    "#this is our latent variable we will pass to the decoder\n",
    "z = mu + tf.multiply(noise, tf.exp(.5*logstd))\n",
    "#the less information we can pass using that one variable.\n",
    "#The more efficiently we can encode the original image, \n",
    "#the higher we can raise the standard deviation on our gaussian until it reaches one.\n",
    "#This constraint forces the encoder to be very efficient, \n",
    "#creating information-rich latent variables. \n",
    "#This improves generalization, so latent variables that we either randomly generated, \n",
    "#or we got from encoding non-training images, will produce a nicer result when decoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder (or, in the language of LFADS, the 'generator') is also a feedforward, fully connected network -- however, it goes from the dimensionality of the latent space back to the original dimensionality of the data (e.g. pixels). The output of the network is squashed between 0 and 1 with a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "\n",
    "#layer 1\n",
    "W_dec = weight_variable([latent_dim, h_dim], 'W_dec')\n",
    "b_dec = bias_variable([h_dim], 'b_dec')\n",
    "#pass in z here (and the weights and biases we just defined)\n",
    "h_dec = tf.nn.tanh(FC_layer(z, W_dec, b_dec))\n",
    "\n",
    "\n",
    "#layer 2, using the original n pixels here since thats the dimensiaonlty\n",
    "#we want to restore our data to\n",
    "W_reconstruct = weight_variable([h_dim, n_pixels], 'W_reconstruct')\n",
    "b_reconstruct = bias_variable([n_pixels], 'b_reconstruct')\n",
    "#784 bernoulli parameters output\n",
    "reconstruction = tf.nn.sigmoid(FC_layer(h_dec, W_reconstruct, b_reconstruct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Each of these outputs is taken to be the mean of a Bernoulli distribution (in this example, a Bernoulli distribution is appropriate because our data is binary). The variational lower bound is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\mathbb{E}_{z\\sim Q(z|X)}\\log P(X|z) - D(Q(z|X)||P(z))\n",
    "\\end{equation}\n",
    "\n",
    "When $P$ is a Bernouli distribution, the log likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(X|z) = \\sum_i^N X^{(i)}\\log y^{(i)} + (1 − X^{(i)}) \\cdot \\log(1 − y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of training samples (in our case, the batchsize), and $y^{(i)}$ is the reconstruction from the latent code $z^{(i)}$. The KL divergence between a gaussian $Q$ with mean $\\mu$ and standard deviation $\\sigma$ and a standard normal distribution $P$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "D(Q||P) = -\\frac{1}{2}\\sum_j^J \\big(1 + \\log((\\sigma_j)^2) - (\\mu_j)^2 - (\\sigma_j)^2\\big)\n",
    "\\end{equation}\n",
    "\n",
    "We want to maximize this lower bound, but because tensorflow doesn't have a 'maximizing' optimizer, we minimize the negative lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define our loss function\n",
    "\n",
    "# variational lower bound\n",
    "\n",
    "# add epsilon to log to prevent numerical overflow\n",
    "#Information is lost because it goes from a smaller to a larger dimensionality. \n",
    "#How much information is lost? We measure this using the reconstruction log-likelihood \n",
    "#This measure tells us how effectively the decoder has learned to reconstruct\n",
    "#an input image x given its latent representation z.\n",
    "log_likelihood = tf.reduce_sum(X*tf.log(reconstruction + 1e-9)+(1 - X)*tf.log(1 - reconstruction + 1e-9), reduction_indices=1)\n",
    "#KL Divergence\n",
    "#If the encoder outputs representations z that are different \n",
    "#than those from a standard normal distribution, it will receive \n",
    "#a penalty in the loss. This regularizer term means \n",
    "#‘keep the representations z of each digit sufficiently diverse’. \n",
    "#If we didn’t include the regularizer, the encoder could learn to cheat\n",
    "#and give each datapoint a representation in a different region of Euclidean space. \n",
    "KL_term = -.5*tf.reduce_sum(1 + 2*logstd - tf.pow(mu,2) - tf.exp(2*logstd), reduction_indices=1)\n",
    "\n",
    "# This allows us to use stochastic gradient descent with respect to the variational parameters\n",
    "variational_lower_bound = tf.reduce_mean(log_likelihood - KL_term)\n",
    "optimizer = tf.train.AdadeltaOptimizer().minimize(-variational_lower_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now all we have to do is run the optimizer until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init all variables and start the session!\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#Create a saver instance\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #lets clock training time..\n",
    "\n",
    "num_iterations = 1000000\n",
    "recording_interval = 1000\n",
    "#store value for these 3 terms so we can plot them later\n",
    "variational_lower_bound_array = []\n",
    "log_likelihood_array = []\n",
    "KL_term_array = []\n",
    "iteration_array = [i*recording_interval for i in range(int(num_iterations/recording_interval))]\n",
    "for i in range(num_iterations):\n",
    "    # np.round to make MNIST binary\n",
    "    #get first batch (200 digits)\n",
    "    x_batch = np.round(mnist.train.next_batch(200)[0])\n",
    "    #run our optimizer on our data\n",
    "    sess.run(optimizer, feed_dict={X: x_batch})\n",
    "    if (i%recording_interval == 0):\n",
    "        #every 1K iterations record these values\n",
    "        vlb_eval = variational_lower_bound.eval(feed_dict={X: x_batch})\n",
    "        print \"Iteration: {}, Loss: {}\".format(i, vlb_eval)\n",
    "        variational_lower_bound_array.append(vlb_eval)\n",
    "        log_likelihood_array.append(np.mean(log_likelihood.eval(feed_dict={X: x_batch})))\n",
    "        KL_term_array.append(np.mean(KL_term.eval(feed_dict={X: x_batch})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Once training is done, save the model.\n",
    "\n",
    "For more info on saving and restoring models, please follow this [link](https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess, 'Your model name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing a session\n",
    "\n",
    "To close a session, first reset the graph and close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#for the number of iterations we had \n",
    "#plot these 3 terms\n",
    "plt.plot(iteration_array, variational_lower_bound_array)\n",
    "plt.plot(iteration_array, KL_term_array)\n",
    "plt.plot(iteration_array, log_likelihood_array)\n",
    "plt.legend(['Variational Lower Bound', 'KL divergence', 'Log Likelihood'], bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.title('Loss per iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here, we plot the reconstructed image on test set images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_model = True\n",
    "if load_model:\n",
    "    saver = tf.train.import_meta_graph('Trained Bernoulli VAE.meta')\n",
    "    saver.restore(sess, os.path.join(os.getcwd(), \"Trained Bernoulli VAE\"))\n",
    "\n",
    "num_pairs = 10\n",
    "image_indices = np.random.randint(0, 200, num_pairs)\n",
    "#Lets plot 10 digits\n",
    "for pair in range(num_pairs):\n",
    "    #reshaping to show original test image\n",
    "    x = np.reshape(mnist.test.images[image_indices[pair]], (1,n_pixels))\n",
    "    plt.figure()\n",
    "    x_image = np.reshape(x, (28,28))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(x_image)\n",
    "    #reconstructed image, feed the test image to the decoder\n",
    "    x_reconstruction = reconstruction.eval(session=sess, feed_dict={X: x})\n",
    "    #reshape it to 28x28 pixels\n",
    "    x_reconstruction_image = (np.reshape(x_reconstruction, (28,28)))\n",
    "    #plot it!\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(x_reconstruction_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rDHfg_djSVA"
   },
   "source": [
    "# One-Shot Learning of Omniglot Dataset with First-Order MAML \n",
    "\n",
    "This notebook explores one-shot learning of Omniglot Dataset using MAML (Model-Agnostic Meta-Learning). \n",
    "\n",
    "## Overview\n",
    "\n",
    "Artificial intelligence is trying to learn how to learn from the way humans learn. We can quickly and easily recognize a new object from just seeing one or few pictures of it, or even from only reading about it without having ever seen it before. We can learn quickly a new skill as well as master many different tasks. This seems easy for the human intelligence but for machines, it is quite a challenge to overcome. Can machine learning do better?\n",
    "\n",
    "## One-Shot Learning\n",
    "\n",
    "Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of **one-shot learning** - if you take a human who’s never seen a bird before, and show them a single picture of a bird, they will probably be able to distinguish birds from other animals with astoundingly high precision. We want machines to also learn and perform tasks in **One Shot Learning**\n",
    "\n",
    "## Meta Learning\n",
    "\n",
    "Best put, **Learning to Learn** is Meta Learning.Deep learning has a great success in mastering one task using a large dataset. In meta-learning, there is a meta-learner and a learner. The meta-learner (or the agent) trains the learner (or the model) on a training set that contains a large number of different tasks. We really want to achieve Few-Shot Meta Learning, an algorithm that trains a neural network to learn many different tasks using only a small data per task.\n",
    "\n",
    "## MAML(Model-Agnostic Meta Learning)\n",
    "\n",
    "Model-Agnostic ~  Model Independent \n",
    "\n",
    "MAML, short for Model-Agnostic Meta-Learning is a fairly general optimization algorithm, compatible with any model that learns through gradient descent. MAML does not learn on batches of samples like most deep learning algorithms but batches of tasks AKA meta-batches. For each task in a meta-batch we first initialise a new “fast model” using the weights of the base meta-learner. We then compute the gradient and hence a parameter update from samples drawn from that task and update the weights of the fast model i.e. perform typical mini-batch stochastic gradient descent on the weights of the \"fast model\".The brilliance of this approach is that it can not only work for supervised regression and classification problems but also for reinforcement learning using any differentiable model!\n",
    "\n",
    ">**First-Order MAML**\n",
    "\n",
    "> To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as First-Order MAML (FOMAML).\n",
    "\n",
    "For this one-shot learning problem, we are going to use First-Order MAML.\n",
    "\n",
    "## Omniglot Dataset\n",
    "\n",
    "Omniglot dataset is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fRd4Sc6cT0_KFm6IhB3Bqw.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rbld2gy8Th-j"
   },
   "source": [
    "# Let's get started!\n",
    "\n",
    "Let us structure the whole code , so that it would it easier to understand. This is how we are going to break it down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUlvBYKg65Xt"
   },
   "source": [
    "## Mount Goolge Drive to Colab\n",
    "\n",
    "If using Google Colab, mount the drive and go to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aEH4iUUajPIY",
    "outputId": "81267d74-373c-4ea9-b33c-181db2304e57"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZRimCF2yjCEP",
    "outputId": "e5c45175-d0b1-4df4-aa8e-d22bff82909f"
   },
   "outputs": [],
   "source": [
    "cd '/content/drive/My Drive/One-Shot Learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOkkRtKgP8vn"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kynkNg31yi9C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Gannavarapu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDbXkSguzcRl"
   },
   "source": [
    "## Preparing our Dataset\n",
    "\n",
    "[Omniglot](https://https://github.com/brendenlake/omniglot). \n",
    "\n",
    "Download and Extract: \n",
    "\n",
    "Background set of [30 alphabets](https://https://github.com/brendenlake/omniglot/blob/master/python/images_background.zip) for training and save it in 'train' folder.\n",
    "\n",
    "Evaluation on set of [20 alphabets](https://https://github.com/brendenlake/omniglot/blob/master/python/images_evaluation.zip) for testing and save it in 'test' folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWNw3ilT0j47"
   },
   "outputs": [],
   "source": [
    "train_dir = 'train' #Directory for training samples \n",
    "test_dir = 'test' #Directory for testing samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jjFyd-hQu87"
   },
   "source": [
    "## Initialize required arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLDcJ3Bgyjg-"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "\"shots\":1, #Number of shots for Meta-Learning (1-shot)\n",
    "\"classes\":20, #Number of classes in the sample (20-way)\n",
    "\"learning_rate\":0.0005,# Learning Rate\n",
    "    \n",
    "#Meta-Training arguments\n",
    "\"train_shots\":10, # Shots in a training batch\n",
    "\"meta_batch\":5, # Batch size for Meta-training\n",
    "\"meta_iters\":20000, # Iteration for Meta-training\n",
    "\"meta_step\":1.0,# Meta-Training step size\n",
    "\"meta_step_final\":0.0, # Meta-Training step size at the end\n",
    "    \n",
    "#Inner Loop Training arguments\n",
    "\"inner_batch\":20, # Inner Batch size for train step\n",
    "\"inner_iters\":10, # Inner Iteration for train step\n",
    "    \n",
    "#Evaluation arguments\n",
    "\"eval_batch\":10, #Inner Batch size for evaluation\n",
    "\"eval_interval\":10, #Train steps for evaluation\n",
    "\"eval_iters\":50, #Inner Iterations for evaluation\n",
    "\"eval_samples\":10000,# Evaluation Samples\n",
    "    \n",
    "#FOML\n",
    "\"foml_tail\":None, # Declare tail-shots for FOML\n",
    "\"pretrained\":False, \n",
    "\"replacement\":False, # Replacement of Sample data\n",
    "\"seed\":0, # Random seed\n",
    "\n",
    "\"transductive\":False, # Flag to evaluate all the samples at once\n",
    "\"weight_decay\":1 # Rate of decay of weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzyHiQvzTBHo"
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "* Helper function to plot images in a row\n",
    "*   Helper Functions for variable manipulation\n",
    "*   Helper Functions to read and augment our dataset\n",
    "* Helper Functions for returning (key,value) pair arguments (kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot images in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_display(list_of_images, list_of_titles=[], no_of_columns=2, figsize=(10,10)):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    column = 0\n",
    "    for i in range(len(list_of_images)):\n",
    "        column += 1\n",
    "        #  check for end of column and create a new figure\n",
    "        if column == no_of_columns+1:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            column = 1\n",
    "        fig.add_subplot(1, no_of_columns, column)\n",
    "        plt.imshow(list_of_images[i])\n",
    "        plt.axis('off')\n",
    "        if len(list_of_titles) >= len(list_of_images):\n",
    "            plt.title(list_of_titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGoqzrxCRT08"
   },
   "source": [
    "### Helper functions to  read and augment our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvSfs6DtQ8AQ"
   },
   "source": [
    "#### Character Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VXAe49Z3aeA"
   },
   "outputs": [],
   "source": [
    "#We will declare a class Character for every character in training and testing samples making it easy to read and visualize.\n",
    "\n",
    "#A single character class.\n",
    "\n",
    "class Character:\n",
    "    def __init__(self, dir_path, rotation=0):\n",
    "        self.dir_path = dir_path\n",
    "        self.rotation = rotation\n",
    "        self._cache = {}\n",
    "\n",
    "    def sample(self, num_images):\n",
    "      \n",
    "        \"\"\"\n",
    "        Sample images (as numpy arrays) from the class.\n",
    "        A sequence of 28x28 numpy arrays.\n",
    "        \"\"\"\n",
    "        \n",
    "        names = [f for f in os.listdir(self.dir_path) if f.endswith('.png')]\n",
    "        random.shuffle(names)\n",
    "        images = []\n",
    "        for name in names[:num_images]:\n",
    "            images.append(self._read_image(os.path.join(self.dir_path, name)))\n",
    "        return images\n",
    "\n",
    "    def _read_image(self, path):\n",
    "      \n",
    "        \"\"\"\n",
    "        Read images from the given path.\n",
    "        28x28 numpy arrays. Each pixel ranges from 0 to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        if path in self._cache:\n",
    "            return self._cache[path]\n",
    "        with open(path, 'rb') as in_file:\n",
    "            img = Image.open(in_file).resize((28, 28)).rotate(self.rotation)\n",
    "            self._cache[path] = np.array(img).astype('float32')\n",
    "            return self._cache[path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXLnl9VOTGSs"
   },
   "source": [
    "#### Functions to read and augment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPizJubXR9Y7"
   },
   "outputs": [],
   "source": [
    "# Iterate over the characters in a data directory and read them..\n",
    "\n",
    "def read_dataset(data_dir):\n",
    "    for alphabet_name in sorted(os.listdir(data_dir)):\n",
    "        alphabet_dir = os.path.join(data_dir, alphabet_name)\n",
    "        if not os.path.isdir(alphabet_dir):\n",
    "            continue\n",
    "        for char_name in sorted(os.listdir(alphabet_dir)):\n",
    "            if not char_name.startswith('character'):\n",
    "                continue\n",
    "            yield Character(os.path.join(alphabet_dir, char_name), 0)\n",
    "\n",
    "#Augment the dataset by adding 90 degree rotations.\n",
    "\n",
    "def augment_dataset(dataset):\n",
    "    for character in dataset:\n",
    "        for rotation in [0, 90, 180, 270]:\n",
    "            yield Character(character.dir_path, rotation=rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0v7CU4MPl81"
   },
   "source": [
    "### Helper functions for variable manipulation\n",
    "\n",
    "We create a class called VariableState to manage the state and to save and restore session variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpmWKe8WTBmX"
   },
   "outputs": [],
   "source": [
    "def interpolate(old_vars, new_vars, epsilon):\n",
    "    \"\"\"\n",
    "    Interpolate between two sequences of variables.\n",
    "    \"\"\"\n",
    "    return add(old_vars, scale(subtract(new_vars, old_vars), epsilon))\n",
    "\n",
    "def average(var_seqs):\n",
    "    \"\"\"\n",
    "    Average a sequence of variable sequences.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for variables in zip(*var_seqs):\n",
    "        res.append(np.mean(variables, axis=0))\n",
    "    return res\n",
    "\n",
    "def subtract(var_seq_1, var_seq_2):\n",
    "    \"\"\"\n",
    "    Subtract one variable sequence from another.\n",
    "    \"\"\"\n",
    "    return [v1 - v2 for v1, v2 in zip(var_seq_1, var_seq_2)]\n",
    "\n",
    "def add(var_seq_1, var_seq_2):\n",
    "    \"\"\"\n",
    "    Add two variable sequences.\n",
    "    \"\"\"\n",
    "    return [v1 + v2 for v1, v2 in zip(var_seq_1, var_seq_2)]\n",
    "\n",
    "def scale(var_seq, scale):\n",
    "    \"\"\"\n",
    "    Scale a variable sequence.\n",
    "    \"\"\"\n",
    "    return [v * scale for v in var_seq]\n",
    "  \n",
    "def weight_decay(rate, variables=None):\n",
    "    \"\"\"\n",
    "    Create an Op that performs weight decay.\n",
    "    \"\"\"\n",
    "    if variables is None:\n",
    "        variables = tf.trainable_variables()\n",
    "    ops = [tf.assign(var, var * rate) for var in variables]\n",
    "    return tf.group(*ops)\n",
    "\n",
    "class VariableState:\n",
    "    \"\"\"\n",
    "    Manage the state of a set of variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, session, variables):\n",
    "        self._session = session\n",
    "        self._variables = variables\n",
    "        self._placeholders = [tf.placeholder(v.dtype.base_dtype, shape=v.get_shape()) for v in variables]\n",
    "        assigns = [tf.assign(v, p) for v, p in zip(self._variables, self._placeholders)]\n",
    "        self._assign_op = tf.group(*assigns)\n",
    "\n",
    "    def export_variables(self):\n",
    "        \"\"\"\n",
    "        Save the current variables.\n",
    "        \"\"\"\n",
    "        return self._session.run(self._variables)\n",
    "\n",
    "    def import_variables(self, values):\n",
    "        \"\"\"\n",
    "        Restore the variables.\n",
    "        \"\"\"\n",
    "        self._session.run(self._assign_op, feed_dict=dict(zip(self._placeholders, values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iz4t92puCarE"
   },
   "source": [
    "### Helper Functions for returning (key,value) pair arguments  (kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KNO4X1hCc08"
   },
   "outputs": [],
   "source": [
    "#Declare helper functions to build kwargs\n",
    "\n",
    "def model_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build the kwargs for model constructors.\n",
    "    \"\"\"\n",
    "    res = {'learning_rate': args['learning_rate']}\n",
    "    return res\n",
    "  \n",
    "def train_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build kwargs for the train() function.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'num_classes': args['classes'],\n",
    "        'num_shots': args['shots'],\n",
    "        'train_shots': (args['train_shots'] or None),\n",
    "        'inner_batch_size': args['inner_batch'],\n",
    "        'inner_iters': args['inner_iters'],\n",
    "        'replacement': args['replacement'],\n",
    "        'meta_step_size': args['meta_step'],\n",
    "        'meta_step_size_final': args['meta_step_final'],\n",
    "        'meta_batch_size': args['meta_batch'],\n",
    "        'meta_iters': args['meta_iters'],\n",
    "        'eval_inner_batch_size': args['eval_batch'],\n",
    "        'eval_inner_iters': args['eval_iters'],\n",
    "        'eval_interval': args['eval_interval'],\n",
    "        'weight_decay_rate': args['weight_decay'],\n",
    "        'transductive': args['transductive'],\n",
    "        'model_fn': _args_model(args)\n",
    "    }\n",
    "\n",
    "  \n",
    "def evaluate_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build kwargs for the evaluate() function.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'num_classes': args['classes'],\n",
    "        'num_shots': args['shots'],\n",
    "        'eval_inner_batch_size': args['eval_batch'],\n",
    "        'eval_inner_iters': args['eval_iters'],\n",
    "        'replacement': args['replacement'],\n",
    "        'weight_decay_rate': args['weight_decay'],\n",
    "        'num_samples': args['eval_samples'],\n",
    "        'transductive': args['transductive'],\n",
    "        'model_fn': _args_model(args)\n",
    "    }\n",
    "\n",
    "def _args_model(args):\n",
    "    \"\"\"\n",
    "    This return the FOML class (First-Order MAML)\n",
    "    \"\"\"\n",
    "    return partial(FOML, tail_shots=args['foml_tail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1EdHROGSN3n"
   },
   "source": [
    "## Defining our Model\n",
    "\n",
    "We create a class OmniglotModel to define our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKaJv8WNSNWy"
   },
   "outputs": [],
   "source": [
    "# We are going to use Adam optimizer as our default optimzer.\n",
    "DEFAULT_OPTIMIZER = partial(tf.train.AdamOptimizer, beta1=0)\n",
    "\n",
    "#A model for Omniglot classification.\n",
    "class OmniglotModel:\n",
    "    def __init__(self, num_classes, optimizer=DEFAULT_OPTIMIZER, **optim_kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create input and label placeholders.Structure the model.\n",
    "        model.summary gives you an overview of the model\n",
    "        \"\"\"\n",
    "        self.input_ph = tf.placeholder(tf.float32, shape=(None, 28, 28))\n",
    "        out = tf.reshape(self.input_ph, (-1, 28, 28, 1))\n",
    "        self.summary = []\n",
    "        self.summary.append(out)\n",
    "        for _ in range(4):\n",
    "            out = tf.layers.conv2d(out, 64, 3, strides=2, padding='same')\n",
    "            self.summary.append(out)\n",
    "            out = tf.layers.batch_normalization(out, training=True)\n",
    "            self.summary.append(out)\n",
    "            out = tf.nn.relu(out)\n",
    "            self.summary.append(out)\n",
    "        out = tf.reshape(out, (-1, int(np.prod(out.get_shape()[1:]))))\n",
    "        self.summary.append(out)\n",
    "        self.logits = tf.layers.dense(out, num_classes)\n",
    "        self.summary.append(self.logits)\n",
    "        self.label_ph = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_ph,\n",
    "                                                                   logits=self.logits)\n",
    "        self.predictions = tf.argmax(self.logits, axis=-1)\n",
    "        self.minimize_op = optimizer(**optim_kwargs).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Di9-cWimTB9g"
   },
   "source": [
    "## First-Order MAML implementation\n",
    "\n",
    "We'll approch this implementation in two steps\n",
    "\n",
    "*   Creating helper functions\n",
    "\n",
    "*   Declaring a class FOML.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTaGIO69VNnn"
   },
   "source": [
    "### Creating helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PN5AmBkTCR7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    _mini_batches():\n",
    "    \n",
    "    This fucntion generates mini-batches from data.\n",
    "\n",
    "    Returns:\n",
    "      An iterable of sequences of (input, label) pairs,\n",
    "        where each sequence is a mini-batch.\n",
    "    \"\"\"\n",
    "def _mini_batches(samples, batch_size, num_batches, replacement):\n",
    "    samples = list(samples)\n",
    "    if replacement:\n",
    "        for _ in range(num_batches):\n",
    "            yield random.sample(samples, batch_size)\n",
    "        return\n",
    "    cur_batch = []\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        random.shuffle(samples)\n",
    "        for sample in samples:\n",
    "            cur_batch.append(sample)\n",
    "            if len(cur_batch) < batch_size:\n",
    "                continue\n",
    "            yield cur_batch\n",
    "            cur_batch = []\n",
    "            batch_count += 1\n",
    "            if batch_count == num_batches:\n",
    "                return\n",
    "\n",
    "\"\"\"\n",
    "    _sample_mini_dataset_():\n",
    "    \n",
    "    This function creates a few shot task from the dataset, given number of shots and classes. .\n",
    "\n",
    "    Returns:\n",
    "      An iterable of (input, label) pairs.\n",
    "    \"\"\"\n",
    "\n",
    "def _sample_mini_dataset(dataset, num_classes, num_shots):\n",
    "   \n",
    "    shuffled = list(dataset)\n",
    "    random.shuffle(shuffled)\n",
    "    for class_idx, class_obj in enumerate(shuffled[:num_classes]):\n",
    "        for sample in class_obj.sample(num_shots):\n",
    "            yield (sample, class_idx)\n",
    "            \n",
    "\"\"\"\n",
    "    _split_train_test():\n",
    "    \n",
    "    This fucntion splits a few-shot task into a train and a test set, given an iterable of (input, label) pairs.\n",
    "    test_shots: the number of examples per class in the test set.\n",
    "\n",
    "    Returns:\n",
    "      A tuple (train, test), where train and test are\n",
    "        sequences of (input, label) pairs.\n",
    "    \"\"\"\n",
    "def _split_train_test(samples, test_shots=1):\n",
    "    train_set = list(samples)\n",
    "    test_set = []\n",
    "    labels = set(item[1] for item in train_set)\n",
    "    for _ in range(test_shots):\n",
    "        for label in labels:\n",
    "            for i, item in enumerate(train_set):\n",
    "                if item[1] == label:\n",
    "                    del train_set[i]\n",
    "                    test_set.append(item)\n",
    "                    break\n",
    "    if len(test_set) < len(labels) * test_shots:\n",
    "        raise IndexError('not enough examples of each class for test set')\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3AjdjwwTrKM"
   },
   "source": [
    "### Declaring a class FOML (FOML implementation) \n",
    "\n",
    "Let's break this down. (Please check comments in the code for respective steps)\n",
    "\n",
    "Please refer to 'Helper functions for variable manipulation' in Helper Functions section to know about the functions used in this section.\n",
    "\n",
    "\n",
    "\n",
    "*   Step 1 - Initialize a session of FOML\n",
    "*   Step 2 -  In the train_step, we initialize old variables from model state and new variables array to save them.\n",
    "* Step 3 - Looping through our meta bacth size, we create a mini dataset from the dataset according to number of classes and shots.\n",
    "* Step 4 - Now we create mini-batches from this mini dataset created.\n",
    "* Step 5 - For every batch in mini batches, take input, labels from batch and run the session on our minimize_op from model (Adam Optimizer).\n",
    "* Step 6 - Once it is done, we append the model's state varibales into the new_vars array.\n",
    "* Step 7 - Restore the state variables for next iteration.\n",
    "* Step 8 -  Once all the training set is looped through the meta bacth size, we take an average of all the new variables saved until now.\n",
    "* Step 9 -  We perform update of state variables by interpolating the old and new variables with meta step size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7RpNO1E7YRdQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A basic implementation of \"first-order MAML\" (FOML).\n",
    "\n",
    "    FOML uses the gradient from the last mini-batch as the update\n",
    "    direction.\n",
    "\n",
    "    There are two ways to sample batches for FOML.\n",
    "    By default, FOML samples batches such that final mini-batch may overlap with\n",
    "    the previous mini-batches.\n",
    "    Alternatively, if tail_shots is specified, then a\n",
    "    separate mini-batch is used for the final step.\n",
    "    This final mini-batch is guaranteed not to overlap\n",
    "    with the training mini-batches.\n",
    "    \n",
    "    \n",
    "    It can operate in two evaluation modes: normal\n",
    "    and transductive. In transductive mode, information is\n",
    "    allowed to leak between test samples via BatchNorm.\n",
    "    Typically, MAML is used in a transductive manner.\n",
    "    \"\"\"\n",
    "\n",
    "class FOML():\n",
    "\n",
    "    def __init__(self, session, variables=None, transductive=False, pre_step_op=None, tail_shots=None):\n",
    "      \n",
    "        #STEP-1\n",
    "        \"\"\"\n",
    "        Create a first-order MAML session.\n",
    "\n",
    "        Args:\n",
    "          tail_shots: if specified, this is the number of\n",
    "            examples per class to reserve for the final\n",
    "            mini-batch.\n",
    "          pre_step_op : if specified, this is the pre-step operation we do,which is generally the weight decay.\n",
    "          transductive : this is to specify to evaluate all samples at once.\n",
    "        \"\"\"\n",
    "        self.session = session\n",
    "        self._model_state = VariableState(self.session, variables or tf.trainable_variables())\n",
    "        self._full_state = VariableState(self.session, tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "        self._transductive = transductive\n",
    "        self._pre_step_op = pre_step_op\n",
    "        self.tail_shots = tail_shots\n",
    "        \n",
    "    #This is the training function for each step which takes the dataset create mini batches and train them.\n",
    "    #Also updates the weights to model state.\n",
    "\n",
    "    def train_step(self,\n",
    "                   dataset,\n",
    "                   input_ph,\n",
    "                   label_ph,\n",
    "                   minimize_op,\n",
    "                   num_classes,\n",
    "                   num_shots,\n",
    "                   inner_batch_size,\n",
    "                   inner_iters,\n",
    "                   replacement,\n",
    "                   meta_step_size,\n",
    "                   meta_batch_size):\n",
    "        #STEP-2\n",
    "        old_vars = self._model_state.export_variables()\n",
    "        new_vars = []\n",
    "        for _ in range(meta_batch_size):\n",
    "            #STEP-3\n",
    "            mini_dataset = _sample_mini_dataset(dataset, num_classes, num_shots)\n",
    "            #STEP-4\n",
    "            mini_batches = _mini_batches(mini_dataset, inner_batch_size, inner_iters,\n",
    "                                              replacement)\n",
    "            #STEP-5\n",
    "            for batch in mini_batches:\n",
    "                inputs, labels = zip(*batch)\n",
    "                if self._pre_step_op:\n",
    "                    self.session.run(self._pre_step_op)\n",
    "                self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "            #STEP-6\n",
    "            new_vars.append(self._model_state.export_variables())\n",
    "            #STEP-7\n",
    "            self._model_state.import_variables(old_vars)\n",
    "        #STEP-8\n",
    "        new_vars = average(new_vars)\n",
    "        #STEP-9\n",
    "        self._model_state.import_variables(interpolate(old_vars,new_vars,meta_step_size))\n",
    "        \n",
    "    \n",
    "    def _mini_batches(self, mini_dataset, inner_batch_size, inner_iters, replacement):\n",
    "        \"\"\"\n",
    "        Generate inner-loop mini-batches for the task.\n",
    "        \"\"\"\n",
    "        if self.tail_shots is None:\n",
    "            #This call the external _mini_batch function if tail_shots is none.\n",
    "            for value in _mini_batches(mini_dataset, inner_batch_size, inner_iters, replacement):\n",
    "                yield value\n",
    "            return\n",
    "        train, tail = _split_train_test(mini_dataset, test_shots=self.tail_shots)\n",
    "        for batch in _mini_batches(train, inner_batch_size, inner_iters - 1, replacement):\n",
    "            yield batch\n",
    "        yield tail\n",
    "        \n",
    "        \n",
    "    def make_one_shot(self,\n",
    "                 dataset,\n",
    "                 input_ph,\n",
    "                 label_ph,\n",
    "                 minimize_op,\n",
    "                 predictions,\n",
    "                 num_classes,\n",
    "                 num_shots,\n",
    "                 inner_batch_size,\n",
    "                 inner_iters,\n",
    "                 replacement):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate One-Shot tasks from the given dataset.\n",
    "        \n",
    "         Args:\n",
    "          dataset: a sequence of data classes, where each data\n",
    "            class has a sample(n) method.\n",
    "          input_ph: placeholder for a batch of samples.\n",
    "          label_ph: placeholder for a batch of labels.\n",
    "          minimize_op: TensorFlow Op to minimize a loss on the\n",
    "            batch specified by input_ph and label_ph.\n",
    "          predictions: a Tensor of integer label predictions.\n",
    "          num_classes: number of data classes to sample.\n",
    "          num_shots: number of examples per data class.\n",
    "          inner_batch_size: batch size for every inner-loop\n",
    "            training iteration.\n",
    "          inner_iters: number of inner-loop iterations.\n",
    "          replacement: sample with replacement.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        A tuple of (predictions,samples) generated from the dataset. Few-Shot tasks comprise of samples and support set.\n",
    "        \"\"\"\n",
    "        train_set, test_set = _split_train_test(\n",
    "            _sample_mini_dataset(dataset, num_classes, num_shots+1))\n",
    "        for batch in _mini_batches(train_set, inner_batch_size, inner_iters, replacement):\n",
    "            inputs, labels = zip(*batch)\n",
    "            if self._pre_step_op:\n",
    "                self.session.run(self._pre_step_op)\n",
    "            self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "        test_preds = self._test_predictions(train_set, test_set, input_ph, predictions)\n",
    "        pred_sample = zip(test_preds, test_set)\n",
    "        return pred_sample\n",
    "        \n",
    "    def evaluate(self,\n",
    "                 dataset,\n",
    "                 input_ph,\n",
    "                 label_ph,\n",
    "                 minimize_op,\n",
    "                 predictions,\n",
    "                 num_classes,\n",
    "                 num_shots,\n",
    "                 inner_batch_size,\n",
    "                 inner_iters,\n",
    "                 replacement):\n",
    "        \"\"\"\n",
    "        Run a single evaluation of the model.\n",
    "\n",
    "        Samples a few-shot learning task and measures\n",
    "        performance.\n",
    "\n",
    "        Args:\n",
    "          dataset: a sequence of data classes, where each data\n",
    "            class has a sample(n) method.\n",
    "          input_ph: placeholder for a batch of samples.\n",
    "          label_ph: placeholder for a batch of labels.\n",
    "          minimize_op: TensorFlow Op to minimize a loss on the\n",
    "            batch specified by input_ph and label_ph.\n",
    "          predictions: a Tensor of integer label predictions.\n",
    "          num_classes: number of data classes to sample.\n",
    "          num_shots: number of examples per data class.\n",
    "          inner_batch_size: batch size for every inner-loop\n",
    "            training iteration.\n",
    "          inner_iters: number of inner-loop iterations.\n",
    "          replacement: sample with replacement.\n",
    "\n",
    "        Returns:\n",
    "          The number of correctly predicted samples.\n",
    "            This always ranges from 0 to num_classes.\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        old_vars = self._full_state.export_variables()\n",
    "        pred_sample = self.make_one_shot(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes, num_shots,\n",
    "                                          inner_batch_size,\n",
    "                                          inner_iters, replacement=replacement)\n",
    "        num_correct = sum([pred == sample[1] for pred,sample in pred_sample])\n",
    "        self._full_state.import_variables(old_vars)\n",
    "        return num_correct\n",
    "    \n",
    "    #This functions return the predictions on the train and test set.\n",
    "    def _test_predictions(self, train_set, test_set, input_ph, predictions):\n",
    "        if self._transductive:\n",
    "            inputs, _ = zip(*test_set)\n",
    "            return self.session.run(predictions, feed_dict={input_ph: inputs})\n",
    "        res = []\n",
    "        for test_sample in test_set:\n",
    "            inputs, _ = zip(*train_set)\n",
    "            inputs += (test_sample[0],)\n",
    "            res.append(self.session.run(predictions, feed_dict={input_ph: inputs})[-1])\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXl3jnkRFBb2"
   },
   "source": [
    "## Train function\n",
    "\n",
    "Let's break this down. (Please check comments in the code for respective steps)\n",
    "\n",
    "\n",
    "*   Step 1 - Create a directory to save checkpoints of our model, if it doesn't exist\n",
    "*   Step 2 - Create a save instance to save checkpoints.\n",
    "* Step 3 - Load the FOML class.\n",
    "* Step 4 - Create a accuracy placeholder and a summary for this placeholder.\n",
    "* Step 5 - Create a variable to merge summary and save graph of sessions in seperate file writers for train and test sets.\n",
    "* Step 6 - Initialze the session with global variables.\n",
    "* Step 7 - Loop through the meta iterations for training and create a curated meta step size from the fraction of training done to reduce the meta step size as we proceed through training.\n",
    "* Step 8 - Call the train_step function of our FOML class. This trains the model over several steps through the number of meta iterations.\n",
    "* Step 9 - For every number of evaluation interations, evaluate the model on the training set to get correct predictions.\n",
    "* Step 10 - Loop through the dataset and writer of train and test sets, call the evaluvate function of the class and save the predictions in a variable.\n",
    "* Step 11 - Feed the accuracy placeholder with the ratio of correct predictions to number of classes . Save the summary using merged and the accuracy placeholder.\n",
    "* Step 12 - Add this summary to the writer and append the accuracy into the accuracies array.\n",
    "* Step 13 - Print the batch number with train accuracy  and test accuracy.\n",
    "* Step 14 - For every 100 iterations, save a checkpoint of training for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Z3PX8SkFCOm"
   },
   "outputs": [],
   "source": [
    "def train(sess,\n",
    "          model,\n",
    "          train_set,\n",
    "          test_set,\n",
    "          save_dir,\n",
    "          num_classes,\n",
    "          num_shots,\n",
    "          inner_batch_size,\n",
    "          inner_iters,\n",
    "          replacement,\n",
    "          meta_step_size,\n",
    "          meta_step_size_final,\n",
    "          meta_batch_size,\n",
    "          meta_iters,\n",
    "          eval_inner_batch_size,\n",
    "          eval_inner_iters,\n",
    "          eval_interval,\n",
    "          weight_decay_rate,\n",
    "          train_shots,\n",
    "          transductive,\n",
    "          model_fn,\n",
    "          time_deadline=None,\n",
    "          log_fn=print):\n",
    "    \"\"\"\n",
    "    Train a model on a dataset.\n",
    "    \"\"\"\n",
    "    #STEP-1\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    #STEP-2\n",
    "    saver = tf.train.Saver()\n",
    "    #STEP-3\n",
    "    foml = model_fn(sess, transductive = transductive, pre_step_op = weight_decay(weight_decay_rate))\n",
    "    #STEP-4\n",
    "    accuracy_ph = tf.placeholder(tf.float32, shape=())\n",
    "    tf.summary.scalar('accuracy', accuracy_ph)\n",
    "    #STEP-5\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(save_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(save_dir, 'test'), sess.graph)\n",
    "    #STEP-6\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #STEP-7\n",
    "    for i in range(meta_iters):\n",
    "        frac_done = i / meta_iters\n",
    "        cur_meta_step_size = frac_done * meta_step_size_final + (1 - frac_done) * meta_step_size\n",
    "        #STEP-8\n",
    "        foml.train_step(train_set, model.input_ph, model.label_ph, model.minimize_op,\n",
    "                           num_classes=num_classes, num_shots=(train_shots or num_shots),\n",
    "                           inner_batch_size=inner_batch_size, inner_iters=inner_iters,\n",
    "                           replacement=replacement,\n",
    "                           meta_step_size=cur_meta_step_size, meta_batch_size=meta_batch_size)\n",
    "        #STEP-9\n",
    "        if i % eval_interval == 0:\n",
    "            accuracies = []\n",
    "            #STEP-10\n",
    "            for dataset, writer in [(train_set, train_writer), (test_set, test_writer)]:\n",
    "                correct = foml.evaluate(dataset, model.input_ph, model.label_ph,\n",
    "                                           model.minimize_op, model.predictions,\n",
    "                                           num_classes=num_classes, num_shots=num_shots,\n",
    "                                           inner_batch_size=eval_inner_batch_size,\n",
    "                                           inner_iters=eval_inner_iters, replacement=replacement)\n",
    "                #STEP-11\n",
    "                summary = sess.run(merged, feed_dict={accuracy_ph: correct/num_classes})\n",
    "                #STEP-12\n",
    "                writer.add_summary(summary, i)\n",
    "                writer.flush()\n",
    "                accuracies.append(correct / num_classes)\n",
    "            #STEP-13\n",
    "            log_fn('batch %d: train=%f test=%f' % (i, accuracies[0], accuracies[1]))\n",
    "        #STEP-14\n",
    "        if i % 100 == 0 or i == meta_iters-1:\n",
    "            saver.save(sess, os.path.join(save_dir, 'model.ckpt'), global_step=i)\n",
    "        if time_deadline is not None and time.time() > time_deadline:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jK0PRHaNHWB"
   },
   "source": [
    "## Evaluate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EiYF-NaNHuD"
   },
   "outputs": [],
   "source": [
    "def evaluate(sess,\n",
    "             model,\n",
    "             dataset,\n",
    "             num_classes,\n",
    "             num_shots,\n",
    "             eval_inner_batch_size,\n",
    "             eval_inner_iters,\n",
    "             replacement,\n",
    "             num_samples,\n",
    "             transductive,\n",
    "             model_fn,\n",
    "             weight_decay_rate):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset.\n",
    "    \"\"\"\n",
    "    foml = model_fn(sess,\n",
    "                         transductive=transductive,\n",
    "                         pre_step_op=weight_decay(weight_decay_rate))\n",
    "    print(\"In Evaluate\")\n",
    "    total_correct = 0\n",
    "    for _ in range(num_samples):\n",
    "        total_correct += foml.evaluate(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes=num_classes, num_shots=num_shots,\n",
    "                                          inner_batch_size=eval_inner_batch_size,\n",
    "                                          inner_iters=eval_inner_iters, replacement=replacement)\n",
    "    return total_correct / (num_samples * num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73ZL9rxwSDrr"
   },
   "source": [
    "## Read the train and test data into lists\n",
    "\n",
    "Please refer to 'Helper Functions to read and augment our dataset' of Helper Functions sections to know the functions used in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EndfIfx3SKl5"
   },
   "outputs": [],
   "source": [
    "#Reading the dataset and converting into train and test lists.\n",
    "train_set = read_dataset(train_dir)\n",
    "test_set = list(read_dataset(test_dir))\n",
    "train_set = list(augment_dataset(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to show samples of characters.\n",
    "def plot_images(path):\n",
    "    \"\"\"\n",
    "        Plot all 20 samples of a particular character of a language\n",
    "    \"\"\"\n",
    "    f, axarr = plt.subplots(5,4, figsize=(10,10))\n",
    "    images_list = []\n",
    "    for image in os.listdir(path):\n",
    "        image_path = os.path.join(path, image)\n",
    "        img = cv2.imread(image_path)\n",
    "        images_list.append(img)\n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            axarr[i,j].imshow(images_list.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arcadian language, 20 samples of the third character.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAJCCAYAAAAoUng9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3T+sHPW5//HP8zOCwqLA8bFlAQ5GshSRIlHukVMgRYmiAEljGiKnsnSR3NwUtzSiSJuk+zUprJ+Q3QSHhosLboLlhhIfS0ExCAcHHLCO5WNCCpSCBPT8ijMb1uud3Zmd7+z33/slrfac8fqcZ3Y+5zvPfmd21txdAAAANfk/sQsAAABYNxogAABQHRogAABQHRogAABQHRogAABQHRogAABQndEaIDN7xsyumdl1Mzs91u9BucgQQiBHGIoMlcnGuA6Qme2R9GdJP5J0U9JlST9z93eD/zIUiQwhBHKEochQucaaATom6bq7f+Du/5R0XtLxkX4XykSGEAI5wlBkqFD3jfRzH5b08dT3NyV9t+3B+/fv98cee2ykUtDmypUrn7j7Ruw6WvTKkESOYikpR2QojpIyJJGjWPrmaKwGyOYsu+tYm5mdknRKkg4fPqytra2RSkEbM/tr7BoWWJohiRylIPcckaH4cs+QRI5S0DdHYx0Cuynp0anvH5G0Pf0Adz/j7pvuvrmxkWrjj4iWZkgiR1iKsQhDMRYVaqwG6LKko2Z2xMzul3RC0oWRfhfKRIYQAjnCUGSoUKMcAnP3L8zs55L+IGmPpJfc/Z0xfhfKRIYQAjnCUGSoXGOdAyR3f13S62P9fJSPDCEEcoShyFCZuBI0AACoDg0QAACoDg0QAACoDg0QAACoDg0QAACoDg0QAACoDg0QAACozmjXAQJKY/bVRwK53/NRQACAjNAAAR1MNz+T70M1QbM/m+YKAMbHITBgRbONS6yfgfyYGdseiIwGCFhi0Y5qyE6MHWBdJk3P9HYnA0A8NEDAAl12UKvsxNjx1WWsJhrA6jgHaAHOzUBXXU+QZmdXH7Y5kCYaICCwVXZ4NNcAsF5FN0DzdkS8OscQk/yQFYQ0yRONMLA+RTdA87DjQgjTO6pVM8XOrl7uPjc3bZdX6JIx8oR16Tu5kKpiT4IO3ejkuHExTNcMufu/b30ejzpNtn1bBniRBqxHdTNAfXTZSXGiNKax/dHHopkgIEUlZZMGaAY7MLQhG+ir66GrknYqKFdpOa2qAWIHBmBd1rWzYFzD2JZlOdcMFtkAlXKCFuIp7ZUOysFYhnUqtfmRCm2AgCFooDGWthx1PQxGDjEEbw66W3EN0Lp3XrkHAMB6LBsrGEswppDNTylZLfZt8BOlbCjEQ4YA5IzmZ76iZoB4SzpWxTk/AEoTalwrdV9aTAPEDgyrWpSdUv/wAZStbVxjTPtKMQ3QLDYyuqBxxhhSzxUn+teJbXy3Is4BSn2wQZpKfnsn0kKWsE7sE7spdgYIWFWMnRWvyAGEwKGv7rKfAaLTRUgMEgBQhyJngNiJYRXkBkCJmNWeL+sGaN2zP9O/L7UNCQBdMX4BmTdA84z1hz3bbJkZgwiCIUsAhuKUkH6yPQeIDQ0AwGK8uGpX1AwQGxoAgPhy2B9nOwMEAACwqkEzQGZ2Q9Jnkr6U9IW7b5rZPkm/k/SYpBuSfurufx9WZlw5nM2es1pyhHGRIwxFhuoSYgboB+7+bXffbL4/LemSux+VdKn5HliGHCEEcoShyFAlxjgEdlzSuebrc5KeHeF3oHzkCCGQIwxFhgo1tAFySW+Y2RUzO9UsO+jutySpuT8w7z+a2Skz2zKzrTt37gwsA5kjRwhhpRyRIUxhLKrI0HeBPenu22Z2QNJFM3uv63909zOSzkjS5uZmsifUcP7PWhSfI6zFSjkiQ5jCWFSRQTNA7r7d3O9IelXSMUm3zeyQJDX3O0OL7FjLOn4NRpBSjpCvlHPEdcvykHKGEN7KDZCZ7TWzBydfS3pK0lVJFySdbB52UtJrQ4uMhUFrfDXkCONLLUe8IMtPahnC+IYcAjso6dWmSbhP0m/d/fdmdlnSK2b2vKSPJD03vMx0MLAFV2WOEBw5wlBkqDIrN0Du/oGkb81Z/jdJPxxSFOpBjhACOcJQZKg+xVwJ2sxGP2TF7A+AVXFIHUhLMQ1QaAxWAEJjXAHSUVwDNNZMELM/ZWFHhLExZgBpy/bT4N194U7MzFYegNg5Yojp/LATrNuycWosbb+TPAJfyXoGaNkf8yoDDxc+rFOIbTxv9pFmGrPIBJCGbGeAJiY7rrZBpc+rcZofAOswZIa6y88GxlTKLHfWM0DT3P3ftzaLzg+i+cEYyBDaMkCjghyVNMud/QzQPF3ODwKA2MacCZpGI44QFk0g5JixYmaAACBHvCDDmMhXu2IboGWHw7r8fwAIYV3jCTs7jGXROwtz3V8W2wBN5LphAJRl7B0Fb33Hus83yz1bxTdAUrcTpGcfDwDrso6P8kHdyNe9ijwJehGaGwAxLXqTxqonk7JzA/qrYgYIAFKy7HIdIX5+zudmYHWLtjuN8t1ogAAggmU7qmU7q8lj2KlhnpBNUKkZowECgIhWmQ0qdYeEsEJ9xM9YPzs2GiAgAHZIGEvfbJWwY0I48/LQdeaw9HGtupOggXVhR4SuQl29nsxhnrZ8TZa1NUmLfl4JaICAgUp/lYT1WPbBzl3/P9BXrWMYh8CAEbAzwqpWyQ55wzKhMlJS1miAACAxfXYyJe2QgHWiAQKABC1rbLjOD/oKkZmSDpdxDhCgxScDArGQR4yBXO2iAUKVlr0rAgBQNg6BAQCA6tAAAYExvQwA6aMBQrXGaFRofgAgDzRAqFrId9LQ/ABAPjgJGhDNCwDUhhkgAABQHRogAABQHRogAABQHRogAABQHRogAABQHRogAABQnaUNkJm9ZGY7ZnZ1atk+M7toZu839w9N/dsLZnbdzK6Z2dNjFY68kCMMRYYQAjnCRJcZoLOSnplZdlrSJXc/KulS873M7AlJJyR9s/k/vzGzPcGqRc7OihxhmLMiQxjurMgR1KEBcvc3JX06s/i4pHPN1+ckPTu1/Ly7f+7uH0q6LulYoFqRMXKEocgQQiBHmFj1HKCD7n5Lkpr7A83yhyV9PPW4m80yYB5yhKHIEEIgRxUKfRK0zVk29zMGzOyUmW2Z2dadO3cCl4HMkSMMRYYQAjkq2KoN0G0zOyRJzf1Os/ympEenHveIpO15P8Ddz7j7prtvbmxsrFgGMkeOMBQZQgjkqEKrNkAXJJ1svj4p6bWp5SfM7AEzOyLpqKS3hpWIgpEjDEWGEAI5qtDST4M3s5clfV/SfjO7KekXkn4p6RUze17SR5KekyR3f8fMXpH0rqQvJP2Xu385Uu3ICDnCUGQIIZAjTJj73MOZ6y3C7I6kf0j6JHYtPexXXvVK99b8dXcvZq7WzD6TdC12HT2Ro4RkOhZJ+eWo2AxJjEVrNChHSTRAkmRmW+6+GbuOrnKrV8qz5j5yXD9qTk+O65dbzbnV21eO61djzXwUBgAAqA4NEAAAqE5KDdCZ2AX0lFu9Up4195Hj+lFzenJcv9xqzq3evnJcv+pqTuYcIAAAgHVJaQYIAABgLaI3QGb2jJldM7PrZnY6dj1tzOyGmf3JzP5oZlvNsn1mdtHM3m/uH4pc40tmtmNmV6eWtdZoZi80z/s1M3s6TtVh5JAjMpS2HDIkkaPUkaNg9Y2fIXePdpO0R9JfJD0u6X5Jb0t6ImZNC2q9IWn/zLJfSzrdfH1a0q8i1/g9Sd+RdHVZjZKeaJ7vByQdabbDntjPc8k5IkPp3nLJEDlK+0aO8spQ7BmgY5Kuu/sH7v5PSeclHY9cUx/HJZ1rvj4n6dmItcjd35T06czithqPSzrv7p+7+4eSrmt3e+Qo5xyRoTTknCGJHKWCHAWyjgzFboAelvTx1Pc3m2UpcklvmNkVMzvVLDvo7rckqbk/EK26dm015vTcL5PLupChdOW0LuQoXTmtS445CpqhpZ8FNjKbsyzVt6U96e7bZnZA0kUzey92QQPl9Nwvk8u6kKF05bQu5ChdOa1LSTla6XmPPQN0U9KjU98/Imk7Ui0Luft2c78j6VXtTq/dNrNDktTc78SrsFVbjdk89x1ksS5kKGnZrAs5Slo265JpjoJmKHYDdFnSUTM7Ymb3Szoh6ULkmu5hZnvN7MHJ15KeknRVu7WebB52UtJrcSpcqK3GC5JOmNkDZnZE0lFJb0WoL4Tkc0SGkpd8hiRylAFyNK6wGUrgTPSfSPqzds/afjF2PS01Pq7dM8zflvTOpE5JX5N0SdL7zf2+yHW+LOmWpH9ptyN+flGNkl5snvdrkn4c+3kuOUdkKP1b6hkiR3ncyFE+GeJK0AAAoDqjHQLL5WJQSBcZQgjkCEORoTKNMgNkZnu0OwX4I+1OXV2W9DN3fzf4L0ORyBBCIEcYigyVa6wZoNwvBoX4yBBCIEcYigwVaqzrAM27KNF3px/QXHjplCTt3bv3P77xjW+MVAraXLly5RN334hdR4ulGZLIUQpyzxEZii/3DEnkKAV9czRWA7T0okTufkbSGUna3Nz0ra2tkUpBGzP7a+waFuh0YStyFF/uOSJD8eWeIYkcpaBvjsY6BJbNxaCQLDKEEMgRhiJDhRqrAcriYlBIGhlCCOQIQ5GhQo1yCMzdvzCzn0v6g6Q9kl5y93fG+F0oExlCCOQIQ5Ghco32Yaju/rqk18f6+SgfGUII5AhDkaEyxf4sMAAAgLUbbQYIAACkz2zeG91Wk9PHa9EAAQBQiZDNTtvPz6UJogECAKBwYzc+835X6o0QDRAAAIVaZ+PT9rtTbYRogAAAKEjMpmeeVA+L0QABI5gdgFL84wdQjq5NzzrGotQasDa8DR4AgIyl1PzkhAYIWINcXhEByEuXscXdaX7m4BAYsCapHgcHkJ9ljQ9jzXI0QMAa0QQBGILGJxwaIGAEk0Fo3mBFE4SuVj10Sr7K0/VQF7rjHCBgRG0DEucEYREzIyPAyJgBAiJhJgizaHowD4e9xkEDBIyky85s6JVSp38Hg2CeaHrQpi0bqf+tp17fBA0QMIK+O7VVGhl2nPnj+i3oiyyEQwMUwKJBjLDWZ2hjQmNTBw5roC8yERYN0ArYQaFN1ynrMTLEOUX54EUTEB8NUAuaHPTV53j9orfJo2y5ntcBlIa3wc/BTgl9DbleS6gdHztQoBx8oPL4mAGaQfODkLoOWtOPI4P1YecGrB8N0JQhOx4GsHqFbljIUrloboF00AA1VhmY2FFhEfKBLsgJEAcNkDgpEeGRHUzLaeZnXq3kGUOkmilOgm6RwsZB+lL9w0YeyAoQDzNAgbFDrEeK2zrFmjAf2wWIixkgYAU5HdKYyLHmEvC8o3apNvvMAAE95XbOGDtgIC8l/s2mOD7SAAE95Nb8zOLiamlI+XlPuTYgJA6BAR3l3vzMY2ZFvtoEgGWYAQos550h8rfqZ4xNHk9+AdSCBghYIseZn76NUMrrAgBj4BAYsEDOh4e6Nm40PwBqxAwQsILUm4ZFjZuZJV8/AIyNGSCgp9Sbh5xnrWqQen6AWgyaATKzG5I+k/SlpC/cfdPM9kn6naTHJN2Q9FN3//uwMlGynHKU+s6r5uYnxRzVvD1ylGKGMJ4QM0A/cPdvu/tm8/1pSZfc/aikS833wDLJ5Si3nVdu9Y4kuRwhO2SoEmMcAjsu6Vzz9TlJz47wO1C+5HKU+uwP5kouR8gOGSrU0AbIJb1hZlfM7FSz7KC735Kk5v7AwN+B8iWXoxxnU9z9rluFksvRrEq3S06SzxDCGfousCfdfdvMDki6aGbvdf2PTbhOSdLhw4cHloHMJZ8jdlxZWClHjEWYkvxYhHAGzQC5+3ZzvyPpVUnHJN02s0OS1NzvtPzfM+6+6e6bGxsbQ8pA5lLLUY6zP/PMzgiVPju0ao4YizCR2liEca3cAJnZXjN7cPK1pKckXZV0QdLJ5mEnJb02tEiUK4ccldowlCSHHCFtZKg+Qw6BHZT0avNq+T5Jv3X335vZZUmvmNnzkj6S9NzwMlEwcoQQyFEH07ObNPb3IEOVWbkBcvcPJH1rzvK/SfrhkKJQj9RyVMrhr9qklqMUke3FyFB9uBI0sACvklECmh/gXjRAALAmNCJAOmiAAGCNUmiCmNkEaIAAYDQpNBopNFxAioZeCBEBTAaoFAZLAOWY1/wwzqCPkjPEDFBk0+HilRpmmdldN6CrkndcWI+2MaeUsYgGCAAAdFZKI00DFFEpXTSAtDC2AMvRACWklK4awGJjNihtP5vxBSGUlCMaIGABXkljLGQLbUpqMlJGAxQJg1+aGHgQ2qJMrWscINfAvWiAEsEABZRrXU0QL6yA7miAWsQ4Ro80sb0Qwtgvcjj3B+iHBkjrGyDaruXCAJWWedsjRhM0+zvJSf7atuHQfNH8AP3RAK0Jswh5YceBXND8AKuhAVqDRc0Pg1Q+1tnE0jCXa9EsENsdWB8+C2yBsQcjmp/8mBnbDYNNMjRvjAmRMTKaP3e/56OS2K5hMQMUCUFO31jnayzDLAC6ZIBzCoFhaIAa6xw0GKDyt+4mhcyUadnb4/t+GCU5AbqjAZqyjsGDASov676IHbM/9Vk2JkwaoekbMLYackYDNGOsBsXdaX4yFftKvuSmfCG2MTnB2ErLGCdBz1HaRsZwsyckhlbDqy0sNnbGkL91nQhdSw5pgICO2t65M+R8jGUDDc14Xaa3d5edEPko27ymOFZzUmLWaICAnrq+Uh86UJU44KA7tj9SUWoWOQcIWMHYA0KpAw6AfmKfP1ryWMQMELCiRRezG/LzACC2GsYjZoCABNQw2ABY3brGiNgzTuvEDBAw0NBPj69lsAEwDGNFWDRAwAgYqAAgbRwCAwAA1aEBAgAA1aEBAgAA1aEBAgAA1aEBAgAA1aEBAgAA1aEBAgAA1VnaAJnZS2a2Y2ZXp5btM7OLZvZ+c//Q1L+9YGbXzeyamT09VuHICznCUGQIIZAjTHSZATor6ZmZZaclXXL3o5IuNd/LzJ6QdELSN5v/8xsz2xOsWuTsrMgRhjkrMoThzoocQR0aIHd/U9KnM4uPSzrXfH1O0rNTy8+7++fu/qGk65KOBaoVGSNHGIoMIQRyhIlVzwE66O63JKm5P9Asf1jSx1OPu9ksu4eZnTKzLTPbunPnzoplIHPkCEORIYRAjioU+iToeZ8AOfdDkdz9jLtvuvvmxsZG4DKQOXKEocgQQiBHBVu1AbptZockqbnfaZbflPTo1OMekbS9enkoHDnCUGQIIZCjCq3aAF2QdLL5+qSk16aWnzCzB8zsiKSjkt4aViIKRo4wFBlCCOSoQuY+dzbvqweYvSzp+5L2S7ot6ReS/kfSK5IOS/pI0nPu/mnz+Bcl/aekLyT9t7v/79IizO5I+oekT1ZdkQj2K696pXtr/rq7r2Wudk05+kzStTHqHxE56oixaKHccsRYlJ7cMiQNzNHSBmhdzGzL3Tdj19FVbvVKedbcR47rR83pyXH9cqs5t3r7ynH9aqyZK0EDAIDq0AABAIDqpNQAnYldQE+51SvlWXMfOa4fNacnx/XLrebc6u0rx/WrruZkzgECAABYl5RmgAAAANaCBggAAFQnegNkZs+Y2TUzu25mp2PX08bMbpjZn8zsj2a21SzbZ2YXzez95v6hyDW+ZGY7ZnZ1allrjWb2QvO8XzOzp+NUHUYOOSJDacshQxI5Sh05Clbf+Bly92g3SXsk/UXS45Lul/S2pCdi1rSg1huS9s8s+7Wk083XpyX9KnKN35P0HUlXl9Uo6Ynm+X5A0pFmO+yJ/TyXnCMylO4tlwyRo7Rv5CivDMWeATom6bq7f+Du/5R0XtLxyDX1cVzSuebrc5KejViL3P1NSZ/OLG6r8bik8+7+ubt/KOm6drdHjnLOERlKQ84ZkshRKshRIOvIUOwG6GFJH099f7NZliKX9IaZXTGzU82yg+5+S5Ka+wPRqmvXVmNOz/0yuawLGUpXTutCjtKV07rkmKOgGboveHn92Jxlqb4v/0l33zazA5Iumtl7sQsaKKfnfplc1oUMpSundSFH6cppXUrK0UrPe+wZoJuSHp36/hFJ25FqWcjdt5v7HUmvand67baZHZKk5n4nXoWt2mrM5rnvIIt1IUNJy2ZdyFHSslmXTHMUNEOxG6DLko6a2REzu1/SCUkXItd0DzPba2YPTr6W9JSkq9qt9WTzsJOSXotT4UJtNV6QdMLMHjCzI5KOSnorQn0hJJ8jMpS85DMkkaMMkKNxhc1QAmei/0TSn7V71vaLsetpqfFx7Z5h/rakdyZ1SvqapEuS3m/u90Wu82VJtyT9S7sd8fOLapT0YvO8X5P049jPc8k5IkPp31LPEDnK40aO8snQaB+FYWbPSPq/2n1b4P9z91+O8otQLDKEEMgRhiJDZRqlATKzPdrtgH+k3c7tsqSfufu7wX8ZikSGEAI5wlBkqFxjnQOU+7UQEB8ZQgjkCEORoUKN9Tb4ee/J/27bg/fv3++PPfbYSKWgzZUrVz5x943YdbTolSGJHMVSUo7IUBwlZUgiR7H0zdFYDdDS9+Q3F146JUmHDx/W1tbWSKWgjZn9NXYNC3S6rgM5ii/3HJGh+HLPkESOUtA3R2MdAlv6nnx3P+Pum+6+ubGRauOPiDpd14EcYQnGIgzFWFSosRqgLK6FgKSRIYRAjjAUGSrUKIfA3P0LM/u5pD9o922DL7n7O2P8LpQplwyZfTU7PtYlJbC6XHKEdJGhco32WWDu/rqk18f6+Shf6hmabn4m39MEpSf1HCF9ZKhMsT8KA8jSbPOzbDkAIC00QEBgNEEAkD4aIKCnLg2OmdEIAUDCaICAHvo2NTRBAJAmGiBgoGUnPtMEAUB6aICAjhY1Ml2aIA6LAUA6aICADtoal+nGp+tb4GmCACC+0a4DBJSiS/PTtiz02+W5zhAAhMEMELBAn+ZnyOMAAOtFAwSMzN1phAAgMVUfAhvrXAx2dmVbdftO/h/nAGGRMfLBmATcq9oGaN07oXm/j0EpbWNtM7Y7ZtEUA+tXZQPEYINV0LhgDIxHWJfZrNU+plV3DlCMwYYBDsA8jA1APFU1QKkNNlwYD6gXf/tAXNUcAkv5HBwzS6YW7GKqGDGQs3qF3g/QYC9XRQNEEACkjuanbu5+175q1Tx03d+Rt0oaoHnY+ACAVK3SDNH89FNtAwQAQEpmZ4EmQh3FoPG5W1UnQQMAUCOan3sVPwPE+T8AsBgn/aejbRZo1Z+FdsU3QPMQCixC0wwgZ+zjuim6AUplR7asoyesactp+6R8uQcA3aw6C8Tfej9FN0ApIZgAJlJ5cYa8sB8Jq9gGqG2AIUAAUhN7XIr9+3GvkOcCYT7eBQYAa8RODUhDsTNA8/AqB6Uj4wBiyPGdhMwAAQCA6lTTAOXQjQIAgPWopgECAADh5XpeGw0QsESuf9wAgHY0QAAAYCU5v0CkAQJmcL4YxkS+UIpFzU8OjRENEAAACC71JogGCAAAVIcGCAAAVGfQlaDN7IakzyR9KekLd980s32SfifpMUk3JP3U3f8+rEyUjBwhhJxzZGacG5SAnDO0bl0Pb6Wc7RAzQD9w92+7+2bz/WlJl9z9qKRLzffAMuQIIZAjDEWGKjHGIbDjks41X5+T9OwIvwPlI0cIIckcpfqKGHMlmaGcpHoy9NAGyCW9YWZXzOxUs+ygu9+SpOb+wLz/aGanzGzLzLbu3LkzsAxkjhwhhJVylEqGUt1JVIaxaCQp5nvop8E/6e7bZnZA0kUze6/rf3T3M5LOSNLm5iYvh+pGjhDCSjlKKUOxzpeY7JyYmWIsqsmgGSB3327udyS9KumYpNtmdkiSmvudoUXmxsz+fcNy5Gix6TyRqXa55cjdk2g4pjNVe75yy1BM09mdZHnZLTUrN0BmttfMHpx8LekpSVclXZB0snnYSUmvDS0yJ7MDSO0DyjLkaDHy001JOWKbx1FShtDNkENgByW92vyx3ifpt+7+ezO7LOkVM3te0keSnhteZvoYtFZGjlqQqV6yzZG7t75wGvtVMxm7S7YZwmpWboDc/QNJ35qz/G+SfjikqNwsGkRSnPZLCTlCCKXmaN3nBNU8XpWaobHlnBmuBD0Qr6AwhrZc5TzYYDWMMcA4aIBGxM4KIZGncsV85xdQKxqgFS17Rw47K6yKHVOdFo0ZoTNBxhBC7vs5GqAVLGt8cg8F0kOm6rBo/KBpAcKqpgFax+DBTgpDsZOD1D6WjHktKMYv1GbolaCzMvQdFZyYWq/pbT/W9iZfmDbZ7vNyMeRt8vN+HhlDjYqdAQo9jczOCRNjvAInX2gT8twgZhiBrxTbAAFDpNB4pFAD0je0qSFnqFXRDVCoWSBendeJ7YtULHtzBZ8TB/RXdAO0CK+a0AXvyEFKlo07NEJAd8U3QENPemYwAU0QUtJlTJs3dnHyc/5yHXMmeUyt/uIbICn8DoxBAxND/6jb/j8ZwyJd8zHJFhlDLKk1PdOqehv8PClvHKRj3id2T1vlEgtkD0Msepv8NHKWr3njzpBLIKRg3R/wu0g1DdD0E87MD1bRpQmafXyXx3X5P0CbvmMbGctL27iTQyOU+sxjNQ3QtGU7srb/A/TJTp+MkS+EsCyf5CxPi7ZrSjMquaniHKB5ugaGz/bCrEkmyAZSRCbLtM4Pyw0hxZpmVTkDNMFAgRSQQ4S2yiw30pf7TFBq9VU7AwSEwmwQUjSdR/JZjhy2Yy7NNw0QEFCfwYmdEsZGxsqU47XJUsxh1YfAgDGk+IcOoCw5vzssFcwAAQCQoVyanFTrpAECACBTiw6HxTgklvJhuFkcAgMAIGOLrgq+rosRtjU+qc7+SDRAAABUZbpZGdqg5DTjM4sGCACAAqxy/acxG5iUZ38kGiAAAIoR4nMvQ9eRKk6CBgCgQLGuA5VD8yMxAwQAQNHWNSuUS+MzQQMEAEAlxmiGcmt8JmiAAACoUK6NSyicAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKqztAEys5fMbMfMrk4t22dmF80aZWr3AAAdC0lEQVTs/eb+oal/e8HMrpvZNTN7eqzCkRdyhKHIEEIgR5joMgN0VtIzM8tOS7rk7kclXWq+l5k9IemEpG82/+c3ZrYnWLXI2VmRIwxzVmQIw50VOYI6NEDu/qakT2cWH5d0rvn6nKRnp5afd/fP3f1DSdclHQtUKzJGjjAUGUII5AgTq54DdNDdb0lSc3+gWf6wpI+nHnezWQbMQ44wFBlCCOSoQqFPgp53Xe25l5o0s1NmtmVmW3fu3AlcBjJHjjAUGUII5KhgqzZAt83skCQ19zvN8puSHp163COStuf9AHc/4+6b7r65sbGxYhnIHDnCUGQIIZCjCq3aAF2QdLL5+qSk16aWnzCzB8zsiKSjkt4aViIKRo4wFBlCCOSoQks/DNXMXpb0fUn7zeympF9I+qWkV8zseUkfSXpOktz9HTN7RdK7kr6Q9F/u/uVItSMj5AhDkSGEQI4wYSl8GqyZ3ZH0D0mfxK6lh/3Kq17p3pq/7u7FzNWa2WeSrsWuoydylJBMxyIpvxwVmyGJsWiNBuUoiQZIksxsy903Y9fRVW71SnnW3EeO60fN6clx/XKrObd6+8px/WqsmY/CAAAA1aEBAgAA1UmpAToTu4CecqtXyrPmPnJcP2pOT47rl1vNudXbV47rV13NyZwDBAAAsC4pzQABAACsRfQGyMyeMbNrZnbdzE7HrqeNmd0wsz+Z2R/NbKtZts/MLprZ+839Q5FrfMnMdszs6tSy1hrN7IXmeb9mZk/HqTqMHHJEhtKWQ4YkcpQ6chSsvvEz5O7RbpL2SPqLpMcl3S/pbUlPxKxpQa03JO2fWfZrSaebr09L+lXkGr8n6TuSri6rUdITzfP9gKQjzXbYE/t5LjlHZCjdWy4ZIkdp38hRXhmKPQN0TNJ1d//A3f8p6byk45Fr6uO4pHPN1+ckPRuxFrn7m5I+nVncVuNxSefd/XN3/1DSde1ujxzlnCMylIacMySRo1SQo0DWkaHYDdDDkj6e+v5msyxFLukNM7tiZqeaZQfd/ZYkNfcHolXXrq3GnJ77ZXJZFzKUrpzWhRylK6d1yTFHQTO09LPARmZzlqX6trQn3X3bzA5Iumhm78UuaKCcnvtlclkXMpSunNaFHKUrp3UpKUcrPe+xZ4BuSnp06vtHJG1HqmUhd99u7nckvard6bXbZnZIkpr7nXgVtmqrMZvnvoMs1oUMJS2bdSFHSctmXTLNUdAMxW6ALks6amZHzOx+SSckXYhc0z3MbK+ZPTj5WtJTkq5qt9aTzcNOSnotToULtdV4QdIJM3vAzI5IOirprQj1hZB8jshQ8pLPkESOMkCOxhU2Qwmcif4TSX/W7lnbL8aup6XGx7V7hvnbkt6Z1Cnpa5IuSXq/ud8Xuc6XJd2S9C/tdsTPL6pR0ovN835N0o9jP88l54gMpX9LPUPkKI8bOconQ1wJGgAAVGe0Q2C5XAwK6SJDCIEcYSgyVKZRZoDMbI92pwB/pN2pq8uSfubu7wb/ZSgSGUII5AhDkaFyjTUDlPvFoBAfGUII5AhDkaFCjXUdoHkXJfru9AOaCy+dkqS9e/f+xze+8Y2RSkGbK1eufOLuG7HraLE0QxI5SkHuOSJD8eWeIYkcpaBvjsZqgJZelMjdz0g6I0mbm5u+tbU1UiloY2Z/jV3DAp0ubEWO4ss9R2QovtwzJJGjFPTN0ViHwLK5GBSSRYYQAjnCUGSoUGM1QFlcDApJI0MIgRxhKDJUqFEOgbn7F2b2c0l/kLRH0kvu/s4YvyskMxPXRUpDrhlCWsgRhiJD5Rrtw1Dd/XVJr4/180Mys7u+pglKQ04ZQrrIEYYiQ2WK/Vlg0U03P4uWAQCAclTdANHoAABQp6obIAAAUCcaIAAAUB0aoBZmxiEyAAAKNdq7wFLXtbmZ97gQ7xKb/bm88wwAgPWptgGa5e6DmiIAAJCPKhugtgZmehaGJgcAgHJV2QB10WdGCACAWo11qsjYaIDUvqHalodsjHIICQAAs3KfJKiuAQqxwWhaAAC5CjFjk3vzI/E2eJoZAAB6KKH5kSprgErZaAAA9LXo+nYh3wWdy8RCFYfASmh8urxzDQCAVZnZoHNfc9sfFdkAdW14Ut5YfbrxlNcDABDfWNe5y3n/U2QD1EXKG62EGSsA4UyPCSmPXUjToiMIq+5vSshhcQ1Q7tN0ND8AFuFjdNDVov3JJDerNEGlZK64BmiR1Ddal7DSIKEPdpbl47pkmKfL/mT6+9wnD1ZRfAOUywajsakT53ABCK1P8zO7PNerOq+i+AaoFDRI5ZjdlmM1QWSmDGxH9LFK89P3MaWo6jpAQC3YaQKYVlNj0xUzQEACQs4C0fyUbTYnbG9McL24fmiAgEQMbYLYEdaJnRskmp9V0AAlYllI2bmVY9E7LvqcgNgnEwyCeeJdfFhm6Dk/NSu+ASr9XTYlrxt2DW1+yQhQJpqfYYprgIZc2TJl896iSMDRhmwA5Vq2j+Pvv5viGqDSTRo8Ao42ZANjYtYhrhxf4Kd6bSEaoAylEByEFeKdPeSiLDnu6BAXY0A/XAcISMCqJ7u6+79vKFsO25imLZ4c8pGaKmaAOGSE1HQ5V43M1otGAn0wVqymyBkgwgCgJIxpQHhVzAABOWCmEkCJUh3XipwBAnKQ6qAALLIot2QaOWEGCADQC40OSkADtGZc2h4AgPgGNUBmdkPSZ5K+lPSFu2+a2T5Jv5P0mKQbkn7q7n8fVmYZ5r2zg/M+yBHCIEcYigzVJcQ5QD9w92+7+2bz/WlJl9z9qKRLzffAMuRoDczsrluByBGGIkOVGOMk6OOSzjVfn5P07Ai/oxi1z/4sQI4CK7ThWYYcYSgyVKihDZBLesPMrpjZqWbZQXe/JUnN/YGBv6O3FAf6FGtKSJI5qkFhuSRHGIoMVWToSdBPuvu2mR2QdNHM3uv6H5twnZKkw4cPDywjT8z+/Bs5GtmiRqeg89BWyhEZwhTGoooMmgFy9+3mfkfSq5KOSbptZockqbnfafm/Z9x90903NzY2hpTRpc5Rf/4yqX4SbipyyVFI6zwHp7BZnlar5ijXDCG8Gseimq3cAJnZXjN7cPK1pKckXZV0QdLJ5mEnJb02tMihatkB5CinHI2Npnh1JeWI8SqOkjKEboYcAjso6dXmj/U+Sb9199+b2WVJr5jZ85I+kvTc8DL7mfdBkwVN85cm2RytE9kcjBxhKDJUmZUbIHf/QNK35iz/m6QfDimqJBz+WqzGHPEKP7wac4SwyFB9qvossHXveNjRAQCQpqoaIAAAAKnCBij2FXA5/FWvWNlbljkyCaBGxTZAywb1sXdEnPuDlLn7v28AUKOqPw2ed4ahFuQcXUxeuJEX1KDoBmj2j7jt09i7/v+umP3BrLackQt0sY7GZDqjvDjMC9trNUU3QLMmAel6+CvUYTKCWTfeDYhQmKHBvOvcYTXFngO0CIMHgFR1GZ8mJ9THflMH0kEO+quyAZLECaCIjvwhFHZ+QH9VHQKbZ3onxCCCdaH5wSJ9D9fPeywZK1fbYTAOkfZTfQM0bYxmiCDWjaYaQwwZk6YfzzhUnkXnAnFSdDc0QC0ID4CUhGqGUI5YJ0SXMtNU7TlAQCy5DxqIjwtZYqItA2M1RrOXS8gZM0DAGrHDQmjzMjV0x0RO87LKOWN95d7szEMDBIykxAEDeVj1cBmNT1mGngtU+hhGAwSMoPSBA/mgqUFfXcev3LPFOUDAmuQ+WADIzxgnzJdy/hkNELAGJQwWANI29ITo2mauOQQGAEDhll0XqpbDXtNogAAAqAgnxu/iEBgAAIUI1bCUcp7PIjRAAAAUpPTGJRQOgQEAUJhVz/OpqXmiAQIAoAI1NTddcAgMAABUhwYIAABUhwYIqISZVXehMwBoQwMErEFKjUdKtQBALDRAwAhSO9mQpgcA7kYDBAAAqsPb4IGRuPtdMy+zszDrmCVi5gcA5mMGCIgk1knJqR2eA4AYaICAEXVpNpilAYD1owECEjDGbNC8n8fsDwDs4hwgICHTTcuQZoVZJQBYjAYIGNlsI9O1OTGz3k0QjQ8AdLP0EJiZvWRmO2Z2dWrZPjO7aGbvN/cPTf3bC2Z23cyumdnTYxWOvJCjr7j7v2/L9GloSm9+yBBCIEeY6HIO0FlJz8wsOy3pkrsflXSp+V5m9oSkE5K+2fyf35jZnmDVImdnRY7u0bUJ6nJb9nsKOP/nrMgQhjsrcgR1aIDc/U1Jn84sPi7pXPP1OUnPTi0/7+6fu/uHkq5LOhaoVmSMHLUbuzkpoPGRRIYQBjnCxKrvAjvo7rckqbk/0Cx/WNLHU4+72Sy7h5mdMrMtM9u6c+fOimUgc+RoSp9DY11/TinNzwJkCCGQowqFfhv8vHn4uSOwu59x901339zY2AhcBjJXfY5WbV4qaHi6qj5DCIIcFWzVBui2mR2SpOZ+p1l+U9KjU497RNL26uWhcORoidnZnGW3CpEhhECOKrRqA3RB0snm65OSXptafsLMHjCzI5KOSnprWIkoGDnCUGQIIZCjCtmyV41m9rKk70vaL+m2pF9I+h9Jr0g6LOkjSc+5+6fN41+U9J+SvpD03+7+v0uLMLsj6R+SPll1RSLYr7zqle6t+evuvpa52jXl6DNJ18aof0TkqCPGooVyyxFjUXpyy5A0MEdLG6B1MbMtd9+MXUdXudUr5VlzHzmuHzWnJ8f1y63m3OrtK8f1q7FmPgsMAABUhwYIAABUJ6UG6EzsAnrKrV4pz5r7yHH9qDk9Oa5fbjXnVm9fOa5fdTUncw4QAADAuqQ0AwQAALAWNEAAAKA60RsgM3vGzK6Z2XUzOx27njZmdsPM/mRmfzSzrWbZPjO7aGbvN/cPRa7xJTPbMbOrU8taazSzF5rn/ZqZPR2n6jByyBEZSlsOGZLIUerIUbD6xs9Q30vth7xJ2iPpL5Iel3S/pLclPRGzpgW13pC0f2bZryWdbr4+LelXkWv8nqTvSLq6rEZJTzTP9wOSjjTbYU/s57nkHJGhdG+5ZIgcpX0jR3llKPYM0DFJ1939A3f/p6Tzko5HrqmP45LONV+fk/RsxFrk7m9K+nRmcVuNxyWdd/fP3f1DSde1uz1ylHOOyFAacs6QRI5SQY4CWUeGYjdAD0v6eOr7m82yFLmkN8zsipmdapYddPdbktTcH4hWXbu2GnN67pfJZV3IULpyWhdylK6c1iXHHAXN0H3By+vH5ixL9X35T7r7tpkdkHTRzN6LXdBAOT33y+SyLmQoXTmtCzlKV07rUlKOVnreY88A3ZT06NT3j0jajlTLQu6+3dzvSHpVu9Nrt83skCQ19zvxKmzVVmM2z30HWawLGUpaNutCjpKWzbpkmqOgGYrdAF2WdNTMjpjZ/ZJOSLoQuaZ7mNleM3tw8rWkpyRd1W6tJ5uHnZT0WpwKF2qr8YKkE2b2gJkdkXRU0lsR6gsh+RyRoeQlnyGJHGWAHI0rbIYSOBP9J5L+rN2ztl+MXU9LjY9r9wzztyW9M6lT0tckXZL0fnO/L3KdL0u6Jelf2u2In19Uo6QXm+f9mqQfx36eS84RGUr/lnqGyFEeN3KUT4ZG+ygMM3tG0v/V7tsC/5+7/3KUX4RikSGEQI4wFBkq0ygNkJnt0W4H/CPtdm6XJf3M3d8N/stQJDKEEMgRhiJD5RrrHKDcr4WA+MgQQiBHGIoMFWqst8HPe0/+d9sevH//fn/sscdGKgVtrly58om7b8Suo0WvDEnkKJaSckSG4igpQxI5iqVvjsZqgJa+J7+58NIpSTp8+LC2trZGKgVtzOyvsWtYoNN1HchRfLnniAzFl3uGJHKUgr45GusQ2NL35Lv7GXffdPfNjY1UG39E1Om6DuQISzAWYSjGokKN1QBlcS0EJI0MIQRyhKHIUKFGOQTm7l+Y2c8l/UG7bxt8yd3fGeN3oUxkCCGQIwxFhso12meBufvrkl4f6+ejfGQIIZAjDEWGyhT7ozAAAADWjgYIAABUZ7RDYAAAoBuzee+21+RzrjACZoAAAEhUW2OE4WiAAACIaFmTQxM0DhogAABQneobIDprAEBMnOcTR7UnQU83PiFPPuNENgBAzubtx0rch1XZAHWd9WF2CACAMlV3CIymBgCQmhJnWFJX5QwQANSulsMcOXH36C/SY//+dapqBqjtD54/egC1MLOqdnK5adsfrWOb1ZYLZoAas6EbEgQaKgB9TY85Y40hte3gcrRoG03+LXQ+luWi1H1aNQ1Q3+neUjc4gPTMjk+hd3Q0PmUxsyDZ6JKLkveFVR0CA4CcDGlcJv+X5icffbbV0Gx0mfUpufmRKpkB4mQ/5Gw2v2S3LiGaoHnIUVqWXUNu3r8vGhtWzU1NuSi+AeLVD3JEbuuy7nf/1LSTK0WXjHDuaj9FN0BclRk5oenB2Bj78jK7vcZqlGOcdJ9CFottgNiZYN1yyRyHhOvENs5L2/ZadEgsxM8PJYfxsMgGKPWuE+XJ4Y8d+ekzXjHjXZdVGqF1ZSGX8bC4BijX5odX5fka+4+dHKCL2cMj5KYOixohMrBYcQ1QG4KA3JBZ9L3ey6QJIjv1SWWb5zL7IxXWADEFjNyR1TqF3GmQIaCbohqgWQwEiCXl7HGoJB9sG2A8xTRAuV8sLrd6kTfylja2D3LV5a36qeSbj8IAAABrkUrzIxUyA5TTSVcAAJQspSZnEWaAAABIDC/sx5d9A0RIAOQs9/MXgVxl3wDNwwACAMgdL/DHVWQDBABACWiCxkMDBABAwmiCxlHEu8CAlHAIFl2xY8PE9LhBLtaDGSAAAFAdGiAASASzh5DIwbrQAAEDMFUNAHkadA6Qmd2Q9JmkLyV94e6bZrZP0u8kPSbphqSfuvvfh5WJkpWUI165xVNSjhBHyhkyM8aXwELMAP3A3b/t7pvN96clXXL3o5IuNd8Dy5AjhECOMFQSGaLZGd8Yh8COSzrXfH1O0rMj/A6UjxwhBHKEochQoYY2QC7pDTO7YmanmmUH3f2WJDX3B+b9RzM7ZWZbZrZ1586dgWUgc+QIIayUo9oyZGZ33XAXxqKKDL0O0JPuvm1mByRdNLP3uv5Hdz8j6YwkbW5uMtdXN3KEEFbKUU0ZmtfwcG7JXRiLKjJoBsjdt5v7HUmvSjom6baZHZKk5n5naJEoGzlCCOQIQ5GhxUqbOVy5ATKzvWb24ORrSU9JuirpgqSTzcNOSnptaJEoFzlCCORodcz+7CJDi5XS9EwbcgjsoKRXmyflPkm/dfffm9llSa+Y2fOSPpL03PAyUTByhBDI0Qpofu5ChiqzcgPk7h9I+tac5X+T9MMhRdVkuquucTAiRwiBHHVT4xjTFRmqD1eCjqjEKUWko6Rj9UBtUvrbTamWkGiAgAJND1ilDl5AqVL/my1lJrHIBij18Eh51IhykDcgb6U0HSkZeh0g9MSOCACA+LKfAWrrilNsNBbVRHePUFLMPoBu+Ptdn+wbIGm3eZjXQKR0EijND2IiYwBWkco+dAxFNEATOc0GoUxkDUBovIAZR1ENkJRmE9T2u9tmrgAAwLiKa4CkPLrlHGrEapgFAoD0FdkAtWHHhNBSa2TJOAB0U2wDlMqOad4OKZXaMJ6UTsAnb0DeYo0lqYxhYym2AWqzzg1K81OHRdu09AEE+Zo06dM3xJf6PiL1+voougFK8YRoYCzkGgC6K7oBkuI1QbM/n3d8lW3R9o35SpvMAflpu67dOtVwBKOKj8Jw9+iHvlCHyQCxLAOLLo0AAPP2W2a2ljGiln1Y8TNAE+vqqGsJDhZbdZDiXIy6se2B9aliBmhi7JkgXtVj2pC8sSMEEGMWqKb9WDUzQG1C7WjYYWGeyblBMQaPEges0rCNsEwKGUmhhjFU3wBJw5sXmh90sc5mqNQBC8B4+5za9mXVNUCh3xW27P+xI8I8081Q6KaIzAFlGfsc1kXnHpY8nlTXAC3SN1CLHs/b3tFXiMyQOQB9LNuPlayqk6AnFr1VucsJZsz6YEzkB0AXQ06IZj9WaQO0zGyouswM1RAWAEAcbe8qnV7WZT9E4/OVqhugZTNBfX8OAABjGXopF5qfu3EO0EC1BQYAEM+yD19ua3I4Z/VeVc8ATazaVdcYGADjWtfHHSBfyz5yhyMY3TADtIJau2UAZajtei+l4l2jw9AANbo0NTQ+AEJb95hC81OWVfPDvoxDYPcgFABKRfNTpmWHxOY9FswAAQDEjrEGbOO7MQMEAIkJfXXemq/2WxO2ZT80QABQgL6Ht9hZonY0QAAQWZ9LcYQ4j4fmB+AcIABIwrqaEpofYBcNEAAkYuzmhOYH+MrSBsjMXjKzHTO7OrVsn5ldNLP3m/uHpv7tBTO7bmbXzOzpsQpHXsgRhiJDCIEcYaLLDNBZSc/MLDst6ZK7H5V0qfleZvaEpBOSvtn8n9+Y2Z5g1SJnZ0WOMMxZVZChMWZpuIjrXc6qghxhuaUNkLu/KenTmcXHJZ1rvj4n6dmp5efd/XN3/1DSdUnHAtWKjJEjDFVThtqalUkjM+/W92fVqqYcYbFVzwE66O63JKm5P9Asf1jSx1OPu9ksA+YhRxiq2AzNNi5dG5mujRHuUmyO0C70SdDz3p859y/QzE6Z2ZaZbd25cydwGcgcOcJQRWSoTyNDwzOKInKE+VZtgG6b2SFJau53muU3JT069bhHJG3P+wHufsbdN919c2NjY8UykDlyhKHIEEIgRxVatQG6IOlk8/VJSa9NLT9hZg+Y2RFJRyW9NaxEFIwcYSgyhBDIUYWWXgnazF6W9H1J+83spqRfSPqlpFfM7HlJH0l6TpLc/R0ze0XSu5K+kPRf7v7lSLUjI+QIQ5EhhECOMGEpHDM2szuS/iHpk9i19LBfedUr3Vvz1929mLlaM/tM0rXYdfREjhKS6Vgk5ZejYjMkMRat0aAcJdEASZKZbbn7Zuw6usqtXinPmvvIcf2oOT05rl9uNedWb185rl+NNfNRGAAAoDo0QAAAoDopNUBnYhfQU271SnnW3EeO60fN6clx/XKrObd6+8px/aqrOZlzgAAAANYlpRkgAACAtYjeAJnZM2Z2zcyum9np2PW0MbMbZvYnM/ujmW01y/aZ2UUze7+5fyhyjS+Z2Y6ZXZ1a1lqjmb3QPO/XzOzpOFWHkUOOyFDacsiQRI5SR46C1Td+hhZ9uvDYN0l7JP1F0uOS7pf0tqQnYta0oNYbkvbPLPu1pNPN16cl/Spyjd+T9B1JV5fVKOmJ5vl+QNKRZjvsif08l5wjMpTuLZcMkaO0b+QorwzFngE6Jum6u3/g7v+UdF7S8cg19XFc0rnm63OSno1Yi9z9TUmfzixuq/G4pPPu/rm7fyjpuna3R45yzhEZSkPOGZLIUSrIUSDryFDsBuhhSR9PfX+zWZYil/SGmV0xs1PNsoPufkuSmvsD0apr11ZjTs/9MrmsCxlKV07rQo7SldO65JijoBla+llgI7M5y1J9W9qT7r5tZgckXTSz92IXNFBOz/0yuawLGUpXTutCjtKV07qUlKOVnvfYM0A3JT069f0jkrYj1bKQu2839zuSXtXu9NptMzskSc39TrwKW7XVmM1z30EW60KGkpbNupCjpGWzLpnmKGiGYjdAlyUdNbMjZna/pBOSLkSu6R5mttfMHpx8LekpSVe1W+vJ5mEnJb0Wp8KF2mq8IOmEmT1gZkckHZX0VoT6Qkg+R2QoeclnSCJHGSBH4wqboQTORP+JpD9r96ztF2PX01Lj49o9w/xtSe9M6pT0NUmXJL3f3O+LXOfLkm5J+pd2O+LnF9Uo6cXmeb8m6cexn+eSc0SG0r+lniFylMeNHOWTIa4EDQAAqhP7EBgAAMDa0QABAIDq0AABAIDq0AABAIDq0AABAIDq0AABAIDq0AABAIDq0AABAIDq/H+3D6bWiwmgrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images('train/Arcadian/character03/')\n",
    "print(\"Arcadian language, 20 samples of the third character.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kannada language, 20 samples of the fifth character.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAJCCAYAAAAoUng9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3T+sHPX57/HPc42gQBQ4/iMEOBjJUkSKRPkdkQIpShQFSBrTEDmVpYvk5pfilkYUaZN0t0lhXSG7CYSGiwtuguWGEo6loJgIBwccsGz5mJACpSAhem5xZpPleGd3/nxnvv/eL+loj9d7dp/Z+ex3nv3O7Ky5uwAAAGryP2IXAAAAMDcaIAAAUB0aIAAAUB0aIAAAUB0aIAAAUB0aIAAAUJ3JGiAze9rMrpjZVTM7PdXjoFxkCCGQI4xFhspkU5wHyMz2SfqTpB9Iui7pbUk/cfc/Bn8wFIkMIQRyhLHIULmmmgF6XNJVd//A3f8h6WVJxyd6LJSJDCEEcoSxyFCh7profh+U9PHSv69L+nbbjQ8cOOCPPPLIRKWgzaVLlz5x94Ox62jRK0MSOYqlpByRoThKypBEjmLpm6OpGiBbcd2X9rWZ2SlJpyTpyJEj2t7enqgUtDGzv8SuYY2NGZLIUQpyzxEZii/3DEnkKAV9czTVLrDrkh5e+vdDkm4s38Ddz7j7lrtvHTyYauOPiDZmSCJH2IixCGMxFhVqqgbobUnHzOyomd0t6YSk8xM9FspEhhACOcJYZKhQk+wCc/cvzOynkn4naZ+kF9393SkeC2UiQwiBHGEsMlSuqY4Bkru/Lun1qe4f5SNDCIEcYSwyVCbOBA0AAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpzV+wCgNSYWev/ufuMlQDpWX598HpAzpgBApasa34AAOWgAQIaXZofGiTUjPyjJOwCA1THwL53Gdl9AWDT2FfyOEEDhOrV2PwsX1fyAFeDLvllHWOVrrPepeaHXWBAi1Je9JsGuRoawNKY2b9/+twekPrnodTsMAOUAD5VEU/bC5v1gFSVujHCPIbmZ+iMccq73pkBSgyDG0JLacDBcMziYKwQ+Rl7HyllmAYoQSkFpEYlNgzu/u8f5CfUmMDYgjbLY8TU40QqOSx2F1juu5VKPvAsZXufc3e/48U6ZN0MfcGTAazTJR8h8ov5zXFC1nX3s/i/sc3KqjF0+X5jZrHIGaBVL/hUOs5VGIzimCsTKWcP6VuVH2bzytblgwtzjSurctb3sddlNeb4WMwMUJ+T2OUycPBObX6pNStkIH+xP6be9g4caer76awx2en69yEytO4+Yo1zRcwA9V0xKQ4GbSs/xVoxLzKQrz4fU++KhhjLYo0PQx533cxljOXIvgEa+5G+HORUaw36bIDW7b5Y9dPnfpC2Et6YYV4xtmcxcpdKE5R9A7RObu+UcqsX67U1P+uQAQBDTN08lDg2ZX0MUJej5Mfsu4zxHSnsr0/bHIMAGcgf6w+h7B1zxmQrpbFl3afD5mq2sm6A2nT5KPMmtX9HSo02DTRzruuUBiqMl8qUP9KVYxbGbgNjj3PZNkB9vsJgyu88meKTZaHOPYNxQj/frD8sy22Dl/JXGpRs1fMeKzuxG5bQijoGKMT5CoYK/TgMLnmbInclDTy4E695dME4EE62M0CrjP0oaZeDVmOGj1mgfA1Zb6W928J/zLleyRA2qXXbUlQD1NXYfZbSPIMKG8A8sc6wV4wPVLQ9bo0bOkwn5+ZpVANkZtckfSbpX5K+cPctM9sv6TeSHpF0TdKP3f1v48oMp8+K2vQ9KWzowsgxR33kOjjkpvQcYXopZWh53Ai5rRm77Spp2xfiGKDvufs33X2r+fdpSRfd/Ziki82/k8CJ5pKWTY6QNHLUCDn7U9n4mFyGupw/bOwbrcrWsaRpDoI+Lulc8/s5Sc9M8Bi9lPTFgRWFNLkcpaCi9R8KOZpAKeNpR0lkqG07Vtm6CGpsA+SS3jCzS2Z2qrnusLvflKTm8tCqPzSzU2a2bWbbt2/fHlnGsK8XyEHu9XeUTI5SU8n6D2VQjmJmaKoxikZ5sOTHohDbN8aVXWMPgn7C3W+Y2SFJF8zsva5/6O5nJJ2RpK2tLdZG3ZLIUc4H80HSwBzNMRalkKsUashAEmMR5jFqBsjdbzSXO5JelfS4pFtm9oAkNZc7Y4tE2WLliA1CWRiPMBYZGibXGcfBDZCZ3Wtm9y1+l/SkpMuSzks62dzspKTXxhaJcpEjhJB6jlLYQMT4pvGcpJ6h1A3JSexsjdkFdljSq80C3CXp1+7+WzN7W9IrZvacpI8kPTu+zGnFXgmVKyZH0jxZGrOrruCsJ5+jFHaxhlj/sZdhQslnKJSxOZjyo/Bz5mtwA+TuH0j6xorr/yrp+2OKSkHBL/KklJ6jEKY+70YJWU8tR23rrG8TVHDDmpzUMpRCw9xXbjVXeSZoYApsrLBsXRM05j7HPDbQpm/jMjZjKeSzqC9DHSKFlbBO6vWVhucbpcjpnThWy/FLledo8EOpvgEC9gp5MB8bobqFXP9Dv1C3hPOh1SzlN2W554pdYJnJPXCIhy/GjCOV3VGs67SlkpM5pDIWMQO0AgNFPdrWdZ+BaOrZn1oGxZKNzQJjUh1CjEchH3coM/vST6qYAQoo9BHwe4PDIDiNUJ/Y2XufIYR6p5TKO66a8Xyji00Hz4/J0RTNyKKedfe96XFjvTaYARphypWWctdcoqFNxdyNBc0PUL51r8+hMytTz1THfrM4RNUNEE0GNmnLSC7ZyaVOAF/W5eD1rs3QXB/S6HvAfew3YuwCSxDv2OPoMvWc6lTuKnwyDchf14OjU3qzk8tYSQO0x9iVMnY/bUohrtG6F24OL+gFmh+gHFN8QmzqsSCHsaaYBqjvrMmcJ4PqGgSanzT0HWzmfKF3OTCbHAHlCdkE5dCczKGYBmiVvmGZKhRjPx1GWOeXyjk5VtWxLk/raiZHQN66fOKq632g8Aaoj6lDMTSwhDWeTU1QzHWTSnMPYH57X89dxwPGgS/LtgEK9Q499AnKUpg1QDgpDBhj3vWlUD+AafE6HybbBkgaf4zPFKEJMUW5976AIcgPALTLugFaJ/bgv/z47K5ACF1nPckPAGxWbAOUEjZICIUsAUAYVZ8JGgAA1IkGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVIcGCAAAVGdjA2RmL5rZjpldXrpuv5ldMLP3m8v7l/7veTO7amZXzOypqQpHXsgRxiJDCIEcYaHLDNBZSU/vue60pIvufkzSxebfMrPHJJ2Q9PXmb35lZvuCVYucnRU5wjhnRYYw3lmRI6hDA+Tub0r6dM/VxyWda34/J+mZpetfdvfP3f1DSVclPR6oVmSMHGEsMoQQyBEWhh4DdNjdb0pSc3mouf5BSR8v3e56cx2wCjnCWGQIIZCjCoU+CNpWXOcrb2h2ysy2zWz79u3bgctA5sgRxiJDCIEcFWxoA3TLzB6QpOZyp7n+uqSHl273kKQbq+7A3c+4+5a7bx08eHBgGcgcOcJYZAghkKMKDW2Azks62fx+UtJrS9efMLN7zOyopGOS3hpXIgpGjjAWGUII5KhCd226gZm9JOm7kg6Y2XVJP5P0c0mvmNlzkj6S9Kwkufu7ZvaKpD9K+kLSf7v7vyaqHRkhRxiLDCEEcoQFc1+5O3PeIsxuS/q7pE9i19LDAeVVr3RnzV9192Lmas3sM0lXYtfREzlKSKZjkZRfjorNkMRYNKNROUqiAZIkM9t2963YdXSVW71SnjX3kePyUXN6cly+3GrOrd6+cly+GmvmqzAAAEB1aIAAAEB1UmqAzsQuoKfc6pXyrLmPHJePmtOT4/LlVnNu9faV4/JVV3MyxwABAADMJaUZIAAAgFlEb4DM7Gkzu2JmV83sdOx62pjZNTP7g5n93sy2m+v2m9kFM3u/ubw/co0vmtmOmV1euq61RjN7vnner5jZU3GqDiOHHJGhtOWQIYkcpY4cBatv+gy5e7QfSfsk/VnSo5LulvSOpMdi1rSm1muSDuy57peSTje/n5b0i8g1fkfStyRd3lSjpMea5/seSUeb9bAv9vNcco7IULo/uWSIHKX9Q47yylDsGaDHJV119w/c/R+SXpZ0PHJNfRyXdK75/ZykZyLWInd/U9Kne65uq/G4pJfd/XN3/1DSVe2ujxzlnCMylIacMySRo1SQo0DmyFDsBuhBSR8v/ft6c12KXNIbZnbJzE411x1295uS1FweilZdu7Yac3ruN8llWchQunJaFnKUrpyWJcccBc3Qxu8Cm5ituC7Vj6U94e43zOyQpAtm9l7sgkbK6bnfJJdlIUPpymlZyFG6clqWknI06HmPPQN0XdLDS/9+SNKNSLWs5e43mssdSa9qd3rtlpk9IEnN5U68Clu11ZjNc99BFstChpKWzbKQo6RlsyyZ5ihohmI3QG9LOmZmR83sbkknJJ2PXNMdzOxeM7tv8bukJyVd1m6tJ5ubnZT0WpwK12qr8bykE2Z2j5kdlXRM0lsR6gsh+RyRoeQlnyGJHGWAHE0rbIYSOBL9R5L+pN2jtl+IXU9LjY9q9wjzdyS9u6hT0lckXZT0fnO5P3KdL0m6Kemf2u2In1tXo6QXmuf9iqQfxn6eS84RGUr/J/UMkaM8fshRPhniTNAAAKA6k+0Cy+VkUEgXGUII5AhjkaEyTTIDZGb7tDsF+APtTl29Lekn7v7H4A+GIpEhhECOMBYZKtdUM0C5nwwK8ZEhhECOMBYZKtRU5wFadVKiby/foDnx0ilJuvfee//ra1/72kSloM2lS5c+cfeDsetosTFDEjlKQe45IkPx5Z4hiRyloG+OpmqANp6UyN3PSDojSVtbW769vT1RKWhjZn+JXcManU5sRY7iyz1HZCi+3DMkkaMU9M3RVLvAsjkZFJJFhhACOcJYZKhQUzVAWZwMCkkjQwiBHGEsMlSoSXaBufsXZvZTSb+TtE/Si+7+7hSPhTKRIYRAjjAWGSrXZF+G6u6vS3p9qvtH+cgQQiBHGIsMlSn2d4EBAADMjgYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYIAABUhwYoA2YWuwQAQERmxrYgsLtiF4D1FoE3M7l75GqQi7aBkgwBeWNbEA4NUML2bsQIPrrgXWK9aHzrwHoOg11gmWHjhjHIT7nWrduQ650MzY/nfBo0QAlr6+Z5MWAM8lOeLus0xHpf3iVPjqbX93lOZZ0s6k49J0XsAtv0BOc8LejuK5eP3WFlWrWu+67ntsygTH03kIwbeRj6Go69jnPaXmU9A9S1u0y9C90kxeAgrHUZHZJfd//Sz6r7RP6GrEfWffrGrqNY63jTbtjUspftDNDYFz5NBWrCrFBccx+0urjfdU11iMdO9Z090pVSZrKeARqjhI1BCctQuz7visa+g9o76JCf6W1aZ2PWadvu0uX1vGkWcMjMYpc6MNy6ZjmVxqHNurwtSyUzxTRAm6b8c1fiMmG1TTnmEz15mPvg1U1jxJR5IkfTynH8z2F7nO0usE3mere77n5TXvEIY8xBy13+loPg81RjQ7BYZnI5rSl2Z5f8QaJ1spwBGrLyp1iBm+qYYxCscaBNwaaDlrv8/V5tGZ16JogMzSf2O+IQWSp1Y1ibPh9THztGpLrrNMsGKAV9jtsIhYEnvrkGjL1CrXsyNL0aztJb0rLUZMy5eaZogmIrpgGas5tMoXPF/KaccYk9OJDpaQ09PqePruuQZjp/sWcSQ4k97hTTAEnxn0yUK9XzrXBAdPpK/vRdCRvhEqW6yyk12TVAXY67mXJFr/voKSedK1OXAwTn+MQW79zzl/JYkHJt6G7INmjvNmyq8Sy1safYT4FN8WLuuuti1VH6fGonT32O54h1ssEh2eLEiPOZ6hM2Y9Yh678MXbIzdJd7DRnJbgZor1SbCmaCyrUuc1PnMeT9p/rayVmIEwvOhfVfJ9b7f2TfAEnDVugcIaAJAtAmxqk5APxHEQ2Q1O+oeDpgdJHip7VQBo7nwhxoiNcbdQyQmV2T9Jmkf0n6wt23zGy/pN9IekTSNUk/dve/jSuzu9QGhBr2o46VYo5WGbq/HfMoKUeII5cMIYwQM0Dfc/dvuvtW8+/Tki66+zFJF5t/A5skn6N1JxHr8+lDNoCTSi5HpX9PYYGSy1Ao5O/LptgFdlzSueb3c5KemeAxssYsQSdRc9T1dAtTn3YBozEeYSwyVKixDZBLesPMLpnZqea6w+5+U5Kay0MjHwPlI0cIgRxhLDJUkbHnAXrC3W+Y2SFJF8zsva5/2ITrlCQdOXJkZBn54bxAX1JsjljHsxqUo9QzhFkVOxbhTqNmgNz9RnO5I+lVSY9LumVmD0hSc7nT8rdn3H3L3bcOHjzY5zG/9O8cdj/McZbgnMXI0Tp710vXYzi6nk0V0xiaoykyNJfYY0jsxw8ttbEI0xrcAJnZvWZ23+J3SU9KuizpvKSTzc1OSnptbJEdapn6IUZjY7haSjnqalWjk+P6zeF101UqOYp97q8+OSxp/YeQSoYwnzG7wA5LerV5Ed0l6dfu/lsze1vSK2b2nKSPJD07vswv46PlRYmWIxSFHGEsMlSZwQ2Qu38g6Rsrrv+rpO+PKWoIjqnJU2o5WiWHbPWtsbQ3ECnlaNUbtCkyFHsdxn780FLKEOZRzJmggVBSb3bG1tfnC16RprFnKZ+qeSFD5SitwV2lqAYoxxWWY801KmU9lbIcqYt9LNDUSloWdFdag1tUAyRN98LkBV+X1F/ooetLfXlzNFUTlOrsDRnCJqltR7NtgFJ4sfWtIYWaMVxqL969upy9ehVyOa8xOUohgynUgM1yWE+xx55sGyCJ8+tgfillq88MQ0p112SuMSr2hgTTKuH1m+IyZN0ASfMMMGMPONz0dykGA9NlK+T6bstTl+8qy/X8RblZl6OuWdi0HkPoUkvbWEiOwgnxXOawTUkhM2O/CiMJbecFWlyXwhO9rO1jsqtuhzQtr6+u62nKYzdyGPBqtm4djRmnGCOwV4q7ulMdn7KfAVpYt3L7vNNKyaZ38JjenO+ux+B4tPRtes777r4csw6HzG7yJg2lKWIGaGHTO+FV3/G0SSrv2mmC4umaq6G7N+faiLCxiq9PltbdLsS6XDdzvnz/jD35YPann6IaIKlfYzF0xYQKU4hdF2zU5tFlXQ1ZlyHXH1nIw9gshc7M0APnyVtaUmx+2qRSUzG7wJal8uR2kVOttQt5sCcHjtYt5IcopkDzE1ff5zfVWZZU61oosgGSpnuBTnG/Q79VnEEojrHNC+sNUv8cpTKmkd842nZXzjVbWKJiGyBJg5qKFHSpO7dlKtGQDRjrDXt1ycTUuel6/+R3PpuOKUx5lm6O0zaEUNwxQG1SetL7yLXumrCOMFYKGVrUMOTDIphPzMantGNWq2mAAACbpbaRqlnfD8qkvO5SrI0GCACARHVpglJsLhZSrq3oY4AAAMhdjONBx95vDsc8MgMEAEDiYjQTqTcwYzEDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqkMDBAAAqrOxATKzF81sx8wuL12338wumNn7zeX9S//3vJldNbMrZvbUVIUjL+QIY5EhhECOsNBlBuispKf3XHda0kV3PybpYvNvmdljkk5I+nrzN78ys33BqkXOzoocYZyzIkMY76zIEdShAXL3NyV9uufq45LONb+fk/TM0vUvu/vn7v6hpKuSHg9UKzJGjjAWGUII5AgLQ48BOuzuNyWpuTzUXP+gpI+Xbne9ue4OZnbKzLbNbPv27dsDy0DmyBHGIkMIgRxVKPRB0LbiOl91Q3c/4+5b7r518ODBwGUgc+QIY5EhhECOCja0AbplZg9IUnO501x/XdLDS7d7SNKN4eWhcOQIY5EhhECOKjS0ATov6WTz+0lJry1df8LM7jGzo5KOSXprXIkoGDnCWGQIIZCjCpn7ytm8/9zA7CVJ35V0QNItST+T9H8lvSLpiKSPJD3r7p82t39B0v+U9IWk/+Xu/29jEWa3Jf1d0idDFySCA8qrXunOmr/q7rPM1c6Uo88kXZmi/gmRo44Yi9bKLUeMRenJLUPSyBxtbIDmYmbb7r4Vu46ucqtXyrPmPnJcPmpOT47Ll1vNudXbV47LV2PNnAkaAABUhwYIAABUJ6UG6EzsAnrKrV4pz5r7yHH5qDk9OS5fbjXnVm9fOS5fdTUncwwQAADAXFKaAQIAAJgFDRAAAKhO9AbIzJ42sytmdtXMTseup42ZXTOzP5jZ781su7luv5ldMLP3m8v7I9f4opntmNnlpetaazSz55vn/YqZPRWn6jByyBEZSlsOGZLIUerIUbD6ps+Qu0f7kbRP0p8lPSrpbknvSHosZk1rar0m6cCe634p6XTz+2lJv4hc43ckfUvS5U01Snqseb7vkXS0WQ/7Yj/PJeeIDKX7k0uGyFHaP+QorwzFngF6XNJVd//A3f8h6WVJxyPX1MdxSeea389JeiZiLXL3NyV9uufqthqPS3rZ3T939w8lXdXu+shRzjkiQ2nIOUMSOUoFOQpkjgzFboAelPTx0r+vN9elyCW9YWaXzOxUc91hd78pSc3loWjVtWurMafnfpNcloUMpSunZSFH6cppWXLMUdAM3RW8vH5sxXWpfi7/CXe/YWaHJF0ws/diFzRSTs/9JrksCxlKV07LQo7SldOylJSjQc977Bmg65IeXvr3Q5JuRKplLXe/0VzuSHpVu9Nrt8zsAUlqLnfiVdiqrcZsnvsOslgWMpS0bJaFHCUtm2XJNEdBMxS7AXpb0jEzO2pmd0s6Iel85JruYGb3mtl9i98lPSnpsnZrPdnc7KSk1+JUuFZbjeclnTCze8zsqKRjkt6KUF8IyeeIDCUv+QxJ5CgD5GhaYTOUwJHoP5L0J+0etf1C7HpaanxUu0eYvyPp3UWdkr4i6aKk95vL/ZHrfEnSTUn/1G5H/Ny6GiW90DzvVyT9MPbzXHKOyFD6P6lniBzl8UOO8snQZF+FYWZPS/rf2v1Y4P9x959P8kAoFhlCCOQIY5GhMk3SAJnZPu12wD/Qbuf2tqSfuPsfgz8YikSGEAI5wlhkqFxTHQOU+7kQEB8ZQgjkCGORoUJN9TH4VZ/J/3bbjQ8cOOCPPPLIRKWgzaVLlz5x94Ox62jRK0MSOYqlpByRoThKypBEjmLpm6OpGqCNn8lvTrx0SpKOHDmi7e3tiUpBGzP7S+wa1uh0XgdyFF/uOSJD8eWeIYkcpaBvjqbaBbbxM/nufsbdt9x96+DBVBt/RNTpvA7kCBswFmEsxqJCTdUAZXEuBCSNDCEEcoSxyFChJtkF5u5fmNlPJf1Oux8bfNHd353isVAmMoQQyBHGIkPlmuy7wNz9dUmvT3X/KB8ZQgjkCGORoTLF/ioMAACA2dEAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6tAAAQCA6kz2ZagAAAALZvbv3909YiW7aIAAAMCsUmiG2AUGAAAmtdzw9Pm/KTEDtGTTSkhhyg4AgFXbq5S3UYva2razi+vnXIbqG6A+nWeMFQQAwMKmmZSh26c+28Ix20B3n2wZ+qq6ARo67TbnCgKALmMVY1L5uuRgyBv1vtvCsdvATU3QXKpugMagCQKwybpBPvT4wQx12aZqUmI1IuuaoLm2rxwEPUIKHSzSZGb//kF9uqz7rhkZsuELablO8pyX0NkK9bcL7t7a6MyRtWpngPq8M+NFj65WZYV35nUJOV6kuJuePKdjeR3EbpSnMPVMUBENUNuK7PvEtd1+09HrQzCIlKfLuy3Wd9lCNixjx5upxxjyPJ8un/jqelzNmE877/3bUBmIdUxQ9g1Ql3MLdFlBXW8zNgBTBQgYYm8eyWIahrypi308BfIXMydTTDRsku0xQH32R4d8QhlIsEoO08mY1rqmZe/PEGPGnjH5jH2cRu1Cnu9n7EH5c2z/5jwEJcsGKPaLbqqpapQlxGCx9wBUDkZNU98Zm1C75/feZspmpe2+yeK8Qjc/Y5ry3GXZALUZ+w5rDqnXh/6mOiNr108RIa4U18HcY0yKz0EJ1n2oosv1m3JQ+7You2OAQh3wvOp+Y4Yh9uMjvLk+YsoB9WkKuT5Yx1gW6iPotStiBmjIiow9lRv78TEt1iVim2qM4XigPNDgbJbVDFCMg57nxkwQUIacx6GhGL+QkyJmgMac16DvfYXEQFGXset76oNckY5QxwpONcYwdqWL40y7K6IBkhj8kbbQTTgDXFlyXJ851lw61kk/We0C26RvE9R29sk5p3FT+VZcTCNUjlbdz9jskDsgLzQ4YWXTAK06Y22Id8pDmiDOnotNyES9Qq77veMTx9igdHNuX7PeBRZjIOBdMwDsohlDzrJugKQwL0BexAAA1CX7BkgKc9R7l7+f6oy/ANAVs9BAGKOOATKza5I+k/QvSV+4+5aZ7Zf0G0mPSLom6cfu/rdxZXbDAcV5Si1HMZHf4UrNEePafErN0JRyPi4txAzQ99z9m+6+1fz7tKSL7n5M0sXm37MJeQ4EBp1ZJZWjschONEXlCFGQoUpMsQvsuKRzze/nJD0zwWNMItcutlDZ5ig0cjkKOcJYZGhJSV/jNLYBcklvmNklMzvVXHfY3W9KUnN5aNUfmtkpM9s2s+3bt2+PLGNFYYE3Grmu4Ew1u0EkAAAgAElEQVQkm6MucmxQCs3zoBylkKG+Cl1/Kch6LEI/Y88D9IS73zCzQ5IumNl7Xf/Q3c9IOiNJW1tbk2xBpt4w5bjhS1TSOapFAXkelCMyhCVJj0UpH2+zaMrH1Dd3Yz9qBsjdbzSXO5JelfS4pFtm9oAkNZc7Y4tchXdA5YiZoyn1zehcmS71tVNqjlJWWpZSy1Db7qbYz/u6JmdofTE+ZT24ATKze83svsXvkp6UdFnSeUknm5udlPTa2CKlIt6dYoW5czSVsV9Q2na7Ob+SJWexchR7Q4RwShmL5rJpzBjbqM0xJo3ZBXZY0qvNAt4l6dfu/lsze1vSK2b2nKSPJD07vszVUp4ORGfRc4QiJJGjmsajApu/JDK0V8qnQehSW4hdY1MZ3AC5+weSvrHi+r9K+v6YomJJNWQlKylHoQeqIQNGrW8KSspRSHN/b2HO2cstQ31e61Nu27qOeyluX7M6E3RJH79byL1+fNncGe27wSFvQH7W7WLvOgMzpcX593JrgLNqgOYWY2XmFiB00zYITXFAY9/7JHNA3taNL3PLaTzJrgEqcRYIZel6QPS63IYYRFL4tAiAMLoedLz8M/S+xprjuzlDyK4BkvLqMNdh41SuTVPWIQenLgNjqMdCGmKvtxgfWUaYxmLOT5YOebw5czT2RIjJGHvwZwrNCAMIhmZg04GIKeQbw7DusGwxRuSUi1S3bcU0QNLwJihGkOb+dAbm12egCrH+l+9jrses3d7GM+RHfmOfGwpp69sIkZs7ZdsAtb3j7dsEdR1kVj1eTg0X4tk0OzPFwBTjMRFOymNEyrXViNfycFkeA7SQ49l3GTzqtCpTU++Pb7tvBszpjfk6gFQbVw6qR2mynQEqCRukOsQ6rcLyRouspWmuXZZTzVqTK+Qo+wZo3a6w5dus+/+999f3sbri3RNiyPGgyZysGxdCPechGwxyAOzKvgGS4n4CZuynz3jnhLmQtemk/H1NUyJTyFkRDVAIXV/IQw6GrnFgBGozRRMU4rwvqdUEpCLrg6CXpfqiTPWARgDhhTqdQcgD5BlngNWKmgHa+0Kf6sC9tlmgvvcBoDwpvrZTrAmIragGaK+pP2I8ZGqZgQgAgPiK2QUWQ8rfcQIAANrRAM2E5gcAgHTQAI20qbGZ89t3AQBAN0UfAzQXGhwAAPLCDBAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKgODRAAAKjOxgbIzF40sx0zu7x03X4zu2Bm7zeX9y/93/NmdtXMrpjZU1MVjryQI4xFhhACOcJClxmgs5Ke3nPdaUkX3f2YpIvNv2Vmj0k6Ienrzd/8ysz2BasWOTsrcoRxzooMYbyzIkdQhwbI3d+U9Omeq49LOtf8fk7SM0vXv+zun7v7h5KuSno8UK3IGDnCWGQIIZAjLAw9Buiwu9+UpObyUHP9g5I+Xrrd9eY6YBVyhLHIEEIgRxUKfRC0rbjOV97Q7JSZbZvZ9u3btwOXgcyRI4xFhhACOSrY0Abolpk9IEnN5U5z/XVJDy/d7iFJN1bdgbufcfctd986ePDgwDKQOXKEscgQQiBHFRraAJ2XdLL5/aSk15auP2Fm95jZUUnHJL01rkQUjBxhLDKEEMhRhe7adAMze0nSdyUdMLPrkn4m6eeSXjGz5yR9JOlZSXL3d83sFUl/lPSFpP92939NVDsyQo4wFhlCCOQIC+a+cnfmvEWY3Zb0d0mfxK6lhwPKq17pzpq/6u7FzNWa2WeSrsSuoydylJBMxyIpvxwVmyGJsWhGo3KURAMkSWa27e5bsevoKrd6pTxr7iPH5aPm9OS4fLnVnFu9feW4fDXWzFdhAACA6tAAAQCA6qTUAJ2JXUBPudUr5VlzHzkuHzWnJ8fly63m3OrtK8flq67mZI4BAgAAmEtKM0AAAACziN4AmdnTZnbFzK6a2enY9bQxs2tm9gcz+72ZbTfX7TezC2b2fnN5f+QaXzSzHTO7vHRda41m9nzzvF8xs6fiVB1GDjkiQ2nLIUMSOUodOQpW3/QZcvdoP5L2SfqzpEcl3S3pHUmPxaxpTa3XJB3Yc90vJZ1ufj8t6ReRa/yOpG9JurypRkmPNc/3PZKONuthX+znueQckaF0f3LJEDlK+4cc5ZWh2DNAj0u66u4fuPs/JL0s6Xjkmvo4Lulc8/s5Sc9ErEXu/qakT/dc3VbjcUkvu/vn7v6hpKvaXR85yjlHZCgNOWdIIkepIEeBzJGh2A3Qg5I+Xvr39ea6FLmkN8zskpmdaq477O43Jam5PBStunZtNeb03G+Sy7KQoXTltCzkKF05LUuOOQqaoY3fBTYxW3Fdqh9Le8Ldb5jZIUkXzOy92AWNlNNzv0kuy0KG0pXTspCjdOW0LCXlaNDzHnsG6Lqkh5f+/ZCkG5FqWcvdbzSXO5Je1e702i0ze0CSmsudeBW2aqsxm+e+gyyWhQwlLZtlIUdJy2ZZMs1R0AzFboDelnTMzI6a2d2STkg6H7mmO5jZvWZ23+J3SU9KuqzdWk82Nzsp6bU4Fa7VVuN5SSfM7B4zOyrpmKS3ItQXQvI5IkPJSz5DEjnKADmaVtgMJXAk+o8k/Um7R22/ELuelhof1e4R5u9IendRp6SvSLoo6f3mcn/kOl+SdFPSP7XbET+3rkZJLzTP+xVJP4z9PJecIzKU/k/qGSJHefyQo3wyxJmgAQBAdSbbBZbLyaCQLjKEEMgRxiJDZZpkBsjM9ml3CvAH2p26elvST9z9j8EfDEUiQwiBHGEsMlSuqWaAcj8ZFOIjQwiBHGEsMlSoqc4DtOqkRN9evkFz4qVTknTvvff+19e+9rWJSkGbS5cufeLuB2PX0WJjhiRylILcc0SG4ss9QxI5SkHfHE3VAG08KZG7n5F0RpK2trZ8e3t7olLQxsz+EruGNTqd2IocxZd7jshQfLlnSCJHKeibo6l2gWVzMigkiwwhBHKEschQoaZqgLI4GRSSRoYQAjnCWGSoUJPsAnP3L8zsp5J+J2mfpBfd/d0pHgtlIkMIgRxhLDJUrsm+DNXdX5f0+lT3j/KRIYRAjjAWGSpT7O8CAwAAmB0NEAAAqA4NEAAAqA4NEAAAqA4NEAAAqA4NEAAAqA4NEAAAqA4NEAAAqA4NEAAAqM5kZ4IGAGxmturLxr/M/Y4vHwcwEg0QAMysS9Oz6vY0QkA47AIDgBn1bX5C/S2AL6MBAoCM0AQBYdAAAcBMQjUvNEHAeMUdA7R3YMhtn3nbwJbbcgDopstre9W4YGaDx4VV98cYg9oU1QC1DRLLeJEjNJpWTM3dg8z6rLuPMQ0VkKNidoF1HRxynTo2s2xrL9mmDQrrDKnokkXyipoU0wCVYtM7MAaodJTedCOsuXOwaMBpxIHVitoFVopFE9Q2aDFVnR/WWdmm+Gj7cl6GnjcIQDtmgBLGBjNdQzYwbJTK5e5f+glhkZc+uekz2xOyViBHRcwArftEwxwnHZtyEAl18COmtTcDrLO6jZm9Wbbub8dkjsYnH30ygH6YAWrR913XlAh5Wrp+d9PY9caxG2gzJltTjSccbxQez+W0ipgBSgHHeNSrz3rvmpPlgY/vgSrPmBmivjmYIzdsqONguzNO1g3Qpo8gz40w5mnKKWZ2YWKTtowt52ZoDuf4VCn5nsaQT5mOGa+6zmyXJOsGaE6rVnzos7Nifpte9FOsz6H3SbbqMvW6nups0GR0vE3rJnTT2afZKmn9cgzQCCUFAWkhWwDmUPMMHjNAEyitS0Yc7D4D6tNnV9TY8WHo6TyGfn+dlNabu6xngIZ80ib0k5/SykQ+hg5cNEQAphTinFa5fEVQsTNAsRsTZoHyNMWsS9t9khFMiWyVLcbsT2myngFKRZdPcQBDsBFDF4w1CCHUeDNmF9mcaICAGdDIAEgFZw3fVewusLm1HZTGbo78zLm+yAemMFemUngXX5p1u8xDGHJw8t6auoxbXU4dE3v8YwYImAmNDqawd6NC81OnFL8epcvjxMwRDVBgqa1ghDXFuiQfKAVNfhw878PQAE2AMKIN2QCwSZ9xIrcxJaV6Rx0DZGbXJH0m6V+SvnD3LTPbL+k3kh6RdE3Sj939b+PKRMlSylHsfdJ7MTvUXUo5Kl2puUw5QymNS6UIMQP0PXf/prtvNf8+Lemiux+TdLH5N7BJNTlq20266ge9VZOjWHI4w+9ISWaI8SC8KXaBHZd0rvn9nKRnJngMlC9ajlIeaArayMyF8QhjRckQr/XpjW2AXNIbZnbJzE411x1295uS1FweGvkYKF9yOZp6JobBbRLJ5agWBeW56Ayl/OYuhrHnAXrC3W+Y2SFJF8zsva5/2ITrlCQdOXJkZBnIXLQcdf3qi6maIE5IFtSgHDEWYUl127Sax5VRM0DufqO53JH0qqTHJd0yswckqbncafnbM+6+5e5bBw8eHFNGkh8931vTmNmEWOf5mEvsHI350r+x62LvFw+u+8F6Q3MUcixC3mKPRZjX4AbIzO41s/sWv0t6UtJlSeclnWxudlLSa2OL7CLFJqgNB7j+R0o5otnIV0o5KlXp41aNGap9rBuzC+ywpFebF8Rdkn7t7r81s7clvWJmz0n6SNKz48vM06ZdHF0+cl3ygNNIMkfL62XVOqh94EhQkjlqs8hU6jnqMv6kvgw9ZJUhjDe4AXL3DyR9Y8X1f5X0/TFFDbWq4Yh9XpcuTVDf+ytJijnaq7TnvEQp52jdGLDu9d8ld7HPTF7SayPlDIVQwZvp3or7MtQQTVDooPQ92BUAhowZfc8gvGqsnOKxEEbIN/Sx1l9K20K+CmMmIcLGgAPkqbTXbmnLgzoV2QANPSC67SC/kB13rE8bAYgrxddw35r4kMC8Qn24J5VZl9SOpyxuF9g6U08p973PrvUw4ABl2HRwfaj77vt3Y49FwrzG7gqLsU5Ta36kyhqg1MRe+QDiCfUJ0FC715cfj7EpHW0NatcmKJXZnxQV2wCNPfCYAQBATHOPQYx56VrXBC3+v899zSX1mcUijwFaGLK/mn3cAIDUrNsuDTnNwtRymHkqdgZoGQ0NACB3m84ptWlbN9e2cFPzk8o2uYoGCACAmkw9A7Nq91tuZw6nAQIAIBPrPkUcY7dTzifPLPoYIAAASpRaM7FOqsfW0gABAJChOZuKIY+VauOzQAMEAECmujQYoRqRrveReuOzwDFAAABkLPWZoFQxAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKpDAwQAAKqzsQEysxfNbMfMLi9dt9/MLpjZ+83l/Uv/97yZXTWzK2b21FSFIy/kCGORIYRAjrDQZQborKSn91x3WtJFdz8m6WLzb5nZY5JOSPp68ze/MrN9wapFzs6KHGGcsyJDGO+syBHUoQFy9zclfbrn6uOSzjW/n5P0zNL1L7v75+7+oaSrkh4PVCsyRo4wFhlCCOQIC0OPATrs7jclqbk81Fz/oKSPl253vbnuDmZ2ysy2zWz79u3bA8tA5sgRxiJDCIEcVSj0QdC24jpfdUN3P+PuW+6+dfDgwcBlIHPkCGORIYRAjgo2tAG6ZWYPSFJzudNcf13Sw0u3e0jSjeHloXDkCGORIYRAjio0tAE6L+lk8/tJSa8tXX/CzO4xs6OSjkl6a1yJKBg5wlhkCCGQowqZ+8rZvP/cwOwlSd+VdEDSLUk/k/R/Jb0i6YikjyQ96+6fNrd/QdL/lPSFpP/l7v9vYxFmtyX9XdInQxckggPKq17pzpq/6u6zzNXOlKPPJF2Zov4JkaOOGIvWyi1HjEXpyS1D0sgcbWyA5mJm2+6+FbuOrnKrV8qz5j5yXD5qTk+Oy5dbzbnV21eOy1djzZwJGgAAVIcGCAAAVCelBuhM7AJ6yq1eKc+a+8hx+ag5PTkuX24151ZvXzkuX3U1J3MMEAAAwFxSmgECAACYBQ0QAACoTvQGyMyeNrMrZnbVzE7HrqeNmV0zsz+Y2e/NbLu5br+ZXTCz95vL+yPX+KKZ7ZjZ5aXrWms0s+eb5/2KmT0Vp+owcsgRGUpbDhmSyFHqyFGw+qbPkLtH+5G0T9KfJT0q6W5J70h6LGZNa2q9JunAnut+Kel08/tpSb+IXON3JH1L0uVNNUp6rHm+75F0tFkP+2I/zyXniAyl+5NLhshR2j/kKK8MxZ4BelzSVXf/wN3/IellSccj19THcUnnmt/PSXomYi1y9zclfbrn6rYaj0t62d0/d/cPJV3V7vrIUc45IkNpyDlDEjlKBTkKZI4MxW6AHpT08dK/rzfXpcglvWFml8zsVHPdYXe/KUnN5aFo1bVrqzGn536TXJaFDKUrp2UhR+nKaVlyzFHQDN0VvLx+bMV1qX4u/wl3v2FmhyRdMLP3Yhc0Uk7P/Sa5LAsZSldOy0KO0pXTspSUo0HPe+wZoOuSHl7690OSbkSqZS13v9Fc7kh6VbvTa7fM7AFJai534lXYqq3GbJ77DrJYFjKUtGyWhRwlLZtlyTRHQTMUuwF6W9IxMztqZndLOiHpfOSa7mBm95rZfYvfJT0p6bJ2az3Z3OykpNfiVLhWW43nJZ0ws3vM7KikY5LeilBfCMnniAwlL/kMSeQoA+RoWmEzlMCR6D+S9CftHrX9Qux6Wmp8VLtHmL8j6d1FnZK+IumipPeby/2R63xJ0k1J/9RuR/zcuholvdA871ck/TD281xyjshQ+j+pZ4gc5fFDjvLJ0GRfhWFmT0v639r9WOD/cfefT/JAKBYZQgjkCGORoTJN0gCZ2T7tdsA/0G7n9rakn7j7H4M/GIpEhhACOcJYZKhcUx0DlPu5EBAfGUII5AhjkaFCTfUx+FWfyf92240PHDjgjzzyyESloM2lS5c+cfeDseto0StDEjmKpaQckaE4SsqQRI5i6ZujqRqgjZ/Jb068dEqSjhw5ou3t7YlKQRsz+0vsGtbodF4HchRf7jkiQ/HlniGJHKWgb46m2gW28TP57n7G3bfcfevgwVQbf0TU6bwO5AgbMBZhLMaiQk3VAGVxLgQkjQwhBHKEschQoSbZBebuX5jZTyX9TrsfG3zR3d+d4rFQJjKEEMgRxiJD5Zrsu8Dc/XVJr091/ygfGUII5AhjkaEyxf4qDAAAgNnRAAEAgOrQAAEAgOrQAAEAgOrQAAEAgOrQAAEAgOrQAAEAgOrQAAEAgOpMdiJEAAAwnNmq72Hd5X7H97FGtVxrarW1oQFCNXIaTADUa91YtXybVMatvfWmVNs67AJDFTYNKGb27x8AiKXPGJTymJVqXctogFC8vi/EHF64AMozdOxJdcxKta4FGiAUrbQBBUCZxo45McesHHZ3rcIxQECLXPZjAyjX3jEoVKOz6X76jn3uvvI+Ux5HaYBQrLYX+FQDyrr7S2EASHkgAnCnVa/XxXVjmo2uB1mvq2OVtiYoVTRAM8jx44Gl6jug9LHu72M2RcuPTRMEpKXrG7W2/1/1Cay2vw9xSECXunL5VBjHAE1sVRBy6pBrEGKdDPl7cgBglT7NQoqNRS6YAYpkXZeOvIxpZMgBgGVzjwV9Z4pSnc0ZggYospLClIM5Z11C7VoDUJeuu8yH7j7bZIqxK8VtHbvAJhbqgDSkqcsA5O4bc0AGgPqEmD2e8rFCNyypjXPFzQCl+HUHuR0Zj3HacrbpXVWK75AApCXkx9e73HbI9mvd36Q0zhXVAHX5uoOFmPtZcztXArrrOqBI7Tnoej9AaHxiNa51z3mXJiSlg6dzeONfzC6wnL7uoC14qYcFXzZ2fY0d7ICQ2j5ODYSWSraKaIBSeTL7oAmKq8txOXPV0YYsIDZO2xHHkLMwpzCe7ZViTcuy3wW26SDUlF+8605klXpwcrX3eV03Tdu3ORm6zkKc2RX1GTq2rTtGbd1xG+v+FuENHZtWib3HI9Xd/Vk3QF0/gbPutqliwzefEM9zqPugCcImU45lORy3UZOpXvcxjoFN8aDorBugKU11foW+NbDhg0QWctC1cRizHudoTpiRxBRSbK6LOAZoWYjvP+n7nU5jMaBggSzkqc+4MPS4mrm+BXyBLCK01DKV7QxQ1xfxFJ8Om+JdUIrdMVabej2RhTr0GUfGnvuFPAF3Km4GaGHTu6zUOlHMgw0BQpvzbL6rTDmWMU7mjzGvXbYzQG3mWtkpHMGONJABxEDuUIKYOS52BmidPufgSfX8Ctgs5/WWc+34z7ix/NOmzxnsl+9/DGYF0hR6vUyRnZJU1QCtG4g2BWWO0IQ4gBubxd5lgXL02cCEejM15D7Y6KVp7jF/SA5CfyP8sti5LG4X2LLYTy4AAH2F+KANb9Y2K6YBmmuGhlDlhXWGOYUeh8hu+aYYo8hNN1XtAmtDWOrDOkeOmNUuE+s1jlEzQGZ2TdJnkv4l6Qt33zKz/ZJ+I+kRSdck/djd/zauzPkRyPmUnCPMZ+4cxTyZYS6PmxvGorqEmAH6nrt/0923mn+flnTR3Y9Jutj8G9hkshz1OfB9HTYiWYg2Hg09GWGMN1u8wVsriW3a0PGGcaq7KXaBHZd0rvn9nKRnJniMZBC2yQTNUa0DPvmcZjya46svQiEDo0XbpsVed1N+AiwFYxsgl/SGmV0ys1PNdYfd/aYkNZeHVv2hmZ0ys20z2759+/bIMuZT64Z0YsnnKKV372PkVm9Pg3I0ZYbGfoUFZpfcWNS32V4lxfPZpVDP2E+BPeHuN8zskKQLZvZe1z909zOSzkjS1tZWtGcipa40pVpmFi1HXRqbitdLbgblKMRYNOR8LilsAHCHaGPRuk+DdflY/FRf1B3iflMdQ0fNALn7jeZyR9Krkh6XdMvMHpCk5nJnbJFzS2lgSqmWqcyVoz7P5WK3Bd8nl4+Y49FyXtblpsuZoUPXtaoGrBZ7m7bpjOHrftrub+z6HvP3sXf/bjK4ATKze83svsXvkp6UdFnSeUknm5udlPTa2CJTl/IKTt3cOQp1Jl42ImnJYTxKITMp1JCqVDJUyjrK4c3jmF1ghyW92izkXZJ+7e6/NbO3Jb1iZs9J+kjSs+PLTAsnrgpq9hzVcnLEGpZxSbLjUSqDPTZKJkOLzAx5Dc+Rt7117X3MHJofaUQD5O4fSPrGiuv/Kun7Y4rCrpSCMpVYORry5ZQ5rY/Kmh/GI4yWYoZyebPWtcbUxtBivgpjiByChfml9iLti1zPL/fMIF19ZldSluJrpOoGaJUUVxIgjR/4yDaQv1gnzhwz/qQ69vBdYIGMCUeuHT3ykeoABCAPQz78kfoHRoppgEprIlIODeYRKgNkCUAoXU7lkHrjs1DULrAuJ4tap8/frpoSHPv4wF6lTj0DyF/u40u2DVDbhqFrExJixqitCVp3e6AvclOn0ma1gdRkvQts3bd89x085jp3Qpfa2OAB2ItxAQgr2xmgLpYbjdS+2ymFGgAAqFX2DVDXYySmajjGnLETAFZhPAGml/UusIUUpoZDHfWewrIAiGfKLzClsQL+I/sZoIVUvjtl1X0x6ADoYo6xgvEI2FVMA7TQtxGaY8aly6nMmfkB6kZjAsyruAZooUsjFKvpoNkBsGyqcSqXL9MEYii2AVqg2QAwtSmajNBnAg917jOgFMU3QACQmykaDZoX4MuK+BQYAMxpymaCRgWYBzNAADBA6HOA0fgA82IGCABGCHEOMJofYH7MAAFAADQxQF6YAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANXZ2ACZ2YtmtmNml5eu229mF8zs/eby/qX/e97MrprZFTN7aqrCkRdyhLHIEEIgR1joMgN0VtLTe647Lemiux+TdLH5t8zsMUknJH29+Ztfmdm+YNUiZ2dFjjDOWZEhjHdW5Ajq0AC5+5uSPt1z9XFJ55rfz0l6Zun6l939c3f/UNJVSY8HqhUZI0cYiwwhBHKEhaHHAB1295uS1Fweaq5/UNLHS7e73lwHrEKOMBYZQgjkqEKhD4K2Fdf5yhuanTKzbTPbvn37duAykDlyhLHIEEIgRwUb2gDdMrMHJKm53Gmuvy7p4aXbPSTpxqo7cPcz7r7l7lsHDx4cWAYyR44wFhlCCOSoQkMboPOSTja/n5T02tL1J8zsHjM7KumYpLfGlYiCkSOMRYYQAjmq0F2bbmBmL0n6rqQDZnZd0s8k/VzSK2b2nKSPJD0rSe7+rpm9IumPkr6Q9N/u/q+JakdGyBHGIkMIgRxhwdxX7s6ctwiz25L+LumT2LX0cEB51SvdWfNX3b2YuVoz+0zSldh19ESOEpLpWCTll6NiMyQxFs1oVI6SaIAkycy23X0rdh1d5VavlGfNfeS4fNScnhyXL7eac6u3rxyXr8aa+SoMAABQHRogAABQnZQaoDOxC+gpt3qlPGvuI8flo+b05Lh8udWcW7195bh81dWczDFAAAAAc0lpBggAAGAW0RsgM3vazK6Y2VUzOx27njZmds3M/mBmvzez7ea6/WZ2wczeby7vj1zji2a2Y2aXl65rrdHMnm+e9ytm9lScqsPIIUdkKG05ZEgiR6kjR8Hqmz5D7h7tR9I+SX+W9KikuyW9I+mxmDWtqfWapAN7rvulpNPN76cl/SJyjd+R9C1JlzfVKOmx5vm+R9LRZj3si/08l5wjMpTuTy4ZIkdp/5CjvDIUewbocUlX3e0vk3cAAAHVSURBVP0Dd/+HpJclHY9cUx/HJZ1rfj8n6ZmItcjd35T06Z6r22o8Lulld//c3T+UdFW76yNHOeeIDKUh5wxJ5CgV5CiQOTIUuwF6UNLHS/++3lyXIpf0hpldMrNTzXWH3f2mJDWXh6JV166txpye+01yWRYylK6cloUcpSunZckxR0EztPG7wCZmK65L9WNpT7j7DTM7JOmCmb0Xu6CRcnruN8llWchQunJaFnKUrpyWpaQcDXreY88AXZf08NK/H5J0I1Ita7n7jeZyR9Kr2p1eu2VmD0hSc7kTr8JWbTVm89x3kMWykKGkZbMs5Chp2SxLpjkKmqHYDdDbko6Z2VEzu1vSCUnnI9d0BzO718zuW/wu6UlJl7Vb68nmZiclvRanwrXaajwv6YSZ3WNmRyUdk/RWhPpCSD5HZCh5yWdIIkcZIEfTCpuhBI5E/5GkP2n3qO0XYtfTUuOj2j3C/B1J7y7qlPQVSRclvd9c7o9c50uSbkr6p3Y74ufW1SjpheZ5vyLph7Gf55JzRIbS/0k9Q+Qojx9ylE+GOBM0AACoTuxdYAAAALOjAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANWhAQIAANX5/6JgUSBX+MjIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images('test/Kannada/character05/')\n",
    "print(\"Kannada language, 20 samples of the fifth character.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbQuVgXnYwqA"
   },
   "source": [
    "## Training the model on train and test set\n",
    "\n",
    "\n",
    " Please refer to 'Helper Functions for returning (key,value) pair arguments (kwargs)' of Helper Functions sections to know the functions used in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-0DWnSiWZFD"
   },
   "source": [
    "### Initialize the model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "QzeFJo0oWZef",
    "outputId": "3fa3d8fd-29b3-468b-97fe-ddf88627eee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-515106d62446>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Aditya Gannavarapu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-11-515106d62446>:19: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-515106d62446>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "model = OmniglotModel(args['classes'], **model_kwargs(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"png.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isbHa_05XWBh"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "2oum1XkrXWWK",
    "outputId": "eebe42b9-132c-49a4-8d00-99791a5c340b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "batch 0: train=0.050000 test=0.150000\n",
      "batch 10: train=0.150000 test=0.050000\n",
      "batch 20: train=0.250000 test=0.050000\n",
      "batch 30: train=0.300000 test=0.150000\n",
      "batch 40: train=0.300000 test=0.150000\n",
      "batch 50: train=0.350000 test=0.300000\n",
      "batch 60: train=0.250000 test=0.200000\n",
      "batch 70: train=0.400000 test=0.250000\n",
      "batch 80: train=0.400000 test=0.150000\n",
      "batch 90: train=0.450000 test=0.350000\n",
      "batch 100: train=0.300000 test=0.350000\n",
      "batch 110: train=0.550000 test=0.200000\n",
      "batch 120: train=0.400000 test=0.400000\n",
      "batch 130: train=0.450000 test=0.300000\n",
      "batch 140: train=0.450000 test=0.600000\n",
      "batch 150: train=0.500000 test=0.300000\n",
      "batch 160: train=0.500000 test=0.300000\n",
      "batch 170: train=0.350000 test=0.300000\n",
      "batch 180: train=0.200000 test=0.350000\n",
      "batch 190: train=0.400000 test=0.300000\n",
      "batch 200: train=0.450000 test=0.150000\n",
      "batch 210: train=0.600000 test=0.450000\n",
      "batch 220: train=0.650000 test=0.200000\n",
      "batch 230: train=0.550000 test=0.100000\n",
      "batch 240: train=0.550000 test=0.400000\n",
      "batch 250: train=0.300000 test=0.350000\n",
      "batch 260: train=0.600000 test=0.450000\n",
      "batch 270: train=0.350000 test=0.550000\n",
      "batch 280: train=0.500000 test=0.500000\n",
      "batch 290: train=0.250000 test=0.500000\n",
      "batch 300: train=0.250000 test=0.400000\n",
      "batch 310: train=0.550000 test=0.250000\n",
      "batch 320: train=0.400000 test=0.350000\n",
      "batch 330: train=0.500000 test=0.250000\n",
      "batch 340: train=0.250000 test=0.400000\n",
      "batch 350: train=0.300000 test=0.350000\n",
      "batch 360: train=0.450000 test=0.350000\n",
      "batch 370: train=0.350000 test=0.250000\n",
      "batch 380: train=0.400000 test=0.150000\n",
      "batch 390: train=0.350000 test=0.450000\n",
      "batch 400: train=0.450000 test=0.600000\n",
      "batch 410: train=0.300000 test=0.450000\n",
      "batch 420: train=0.400000 test=0.300000\n",
      "batch 430: train=0.400000 test=0.300000\n",
      "batch 440: train=0.550000 test=0.750000\n",
      "batch 450: train=0.450000 test=0.300000\n",
      "batch 460: train=0.650000 test=0.450000\n",
      "batch 470: train=0.400000 test=0.600000\n",
      "batch 480: train=0.600000 test=0.350000\n",
      "batch 490: train=0.550000 test=0.500000\n",
      "batch 500: train=0.450000 test=0.450000\n",
      "WARNING:tensorflow:From C:\\Users\\Aditya Gannavarapu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "batch 510: train=0.750000 test=0.350000\n",
      "batch 520: train=0.450000 test=0.800000\n",
      "batch 530: train=0.450000 test=0.550000\n",
      "batch 540: train=0.650000 test=0.500000\n",
      "batch 550: train=0.550000 test=0.500000\n",
      "batch 560: train=0.600000 test=0.300000\n",
      "batch 570: train=0.550000 test=0.250000\n",
      "batch 580: train=0.500000 test=0.400000\n",
      "batch 590: train=0.450000 test=0.300000\n",
      "batch 600: train=0.600000 test=0.350000\n",
      "batch 610: train=0.450000 test=0.300000\n",
      "batch 620: train=0.450000 test=0.350000\n",
      "batch 630: train=0.500000 test=0.350000\n",
      "batch 640: train=0.550000 test=0.600000\n",
      "batch 650: train=0.550000 test=0.450000\n",
      "batch 660: train=0.450000 test=0.500000\n",
      "batch 670: train=0.500000 test=0.500000\n",
      "batch 680: train=0.400000 test=0.450000\n",
      "batch 690: train=0.700000 test=0.400000\n",
      "batch 700: train=0.350000 test=0.450000\n",
      "batch 710: train=0.750000 test=0.250000\n",
      "batch 720: train=0.650000 test=0.450000\n",
      "batch 730: train=0.450000 test=0.450000\n",
      "batch 740: train=0.750000 test=0.350000\n",
      "batch 750: train=0.650000 test=0.500000\n",
      "batch 760: train=0.500000 test=0.350000\n",
      "batch 770: train=0.700000 test=0.500000\n",
      "batch 780: train=0.650000 test=0.300000\n",
      "batch 790: train=0.750000 test=0.400000\n",
      "batch 800: train=0.500000 test=0.450000\n",
      "batch 810: train=0.550000 test=0.250000\n",
      "batch 820: train=0.550000 test=0.550000\n",
      "batch 830: train=0.600000 test=0.400000\n",
      "batch 840: train=0.500000 test=0.550000\n",
      "batch 850: train=0.650000 test=0.650000\n",
      "batch 860: train=0.650000 test=0.400000\n",
      "batch 870: train=0.450000 test=0.500000\n",
      "batch 880: train=0.700000 test=0.500000\n",
      "batch 890: train=0.550000 test=0.550000\n",
      "batch 900: train=0.500000 test=0.450000\n",
      "batch 910: train=0.600000 test=0.500000\n",
      "batch 920: train=0.500000 test=0.400000\n",
      "batch 930: train=0.500000 test=0.350000\n",
      "batch 940: train=0.600000 test=0.500000\n",
      "batch 950: train=0.450000 test=0.500000\n",
      "batch 960: train=0.450000 test=0.350000\n",
      "batch 970: train=0.500000 test=0.550000\n",
      "batch 980: train=0.750000 test=0.450000\n",
      "batch 990: train=0.600000 test=0.450000\n",
      "batch 1000: train=0.650000 test=0.250000\n",
      "batch 1010: train=0.500000 test=0.450000\n",
      "batch 1020: train=0.550000 test=0.400000\n",
      "batch 1030: train=0.650000 test=0.650000\n",
      "batch 1040: train=0.650000 test=0.550000\n",
      "batch 1050: train=0.550000 test=0.600000\n",
      "batch 1060: train=0.500000 test=0.300000\n",
      "batch 1070: train=0.700000 test=0.450000\n",
      "batch 1080: train=0.400000 test=0.350000\n",
      "batch 1090: train=0.550000 test=0.650000\n",
      "batch 1100: train=0.400000 test=0.400000\n",
      "batch 1110: train=0.550000 test=0.600000\n",
      "batch 1120: train=0.600000 test=0.500000\n",
      "batch 1130: train=0.500000 test=0.550000\n",
      "batch 1140: train=0.550000 test=0.350000\n",
      "batch 1150: train=0.500000 test=0.600000\n",
      "batch 1160: train=0.600000 test=0.450000\n",
      "batch 1170: train=0.600000 test=0.700000\n",
      "batch 1180: train=0.450000 test=0.300000\n",
      "batch 1190: train=0.550000 test=0.400000\n",
      "batch 1200: train=0.650000 test=0.700000\n",
      "batch 1210: train=0.600000 test=0.650000\n",
      "batch 1220: train=0.500000 test=0.450000\n",
      "batch 1230: train=0.200000 test=0.600000\n",
      "batch 1240: train=0.450000 test=0.500000\n",
      "batch 1250: train=0.550000 test=0.600000\n",
      "batch 1260: train=0.500000 test=0.600000\n",
      "batch 1270: train=0.400000 test=0.400000\n",
      "batch 1280: train=0.450000 test=0.450000\n",
      "batch 1290: train=0.500000 test=0.400000\n",
      "batch 1300: train=0.400000 test=0.500000\n",
      "batch 1310: train=0.450000 test=0.350000\n",
      "batch 1320: train=0.600000 test=0.550000\n",
      "batch 1330: train=0.650000 test=0.250000\n",
      "batch 1340: train=0.350000 test=0.600000\n",
      "batch 1350: train=0.350000 test=0.650000\n",
      "batch 1360: train=0.500000 test=0.200000\n",
      "batch 1370: train=0.650000 test=0.350000\n",
      "batch 1380: train=0.650000 test=0.400000\n",
      "batch 1390: train=0.650000 test=0.550000\n",
      "batch 1400: train=0.650000 test=0.500000\n",
      "batch 1410: train=0.450000 test=0.400000\n",
      "batch 1420: train=0.600000 test=0.450000\n",
      "batch 1430: train=0.650000 test=0.450000\n",
      "batch 1440: train=0.550000 test=0.550000\n",
      "batch 1450: train=0.400000 test=0.450000\n",
      "batch 1460: train=0.550000 test=0.350000\n",
      "batch 1470: train=0.750000 test=0.700000\n",
      "batch 1480: train=0.500000 test=0.500000\n",
      "batch 1490: train=0.450000 test=0.400000\n",
      "batch 1500: train=0.650000 test=0.400000\n",
      "batch 1510: train=0.550000 test=0.500000\n",
      "batch 1520: train=0.550000 test=0.500000\n",
      "batch 1530: train=0.650000 test=0.500000\n",
      "batch 1540: train=0.550000 test=0.550000\n",
      "batch 1550: train=0.700000 test=0.500000\n",
      "batch 1560: train=0.500000 test=0.650000\n",
      "batch 1570: train=0.400000 test=0.350000\n",
      "batch 1580: train=0.550000 test=0.500000\n",
      "batch 1590: train=0.500000 test=0.550000\n",
      "batch 1600: train=0.600000 test=0.700000\n",
      "batch 1610: train=0.500000 test=0.350000\n",
      "batch 1620: train=0.600000 test=0.500000\n",
      "batch 1630: train=0.650000 test=0.600000\n",
      "batch 1640: train=0.150000 test=0.250000\n",
      "batch 1650: train=0.600000 test=0.650000\n",
      "batch 1660: train=0.650000 test=0.450000\n",
      "batch 1670: train=0.600000 test=0.300000\n",
      "batch 1680: train=0.600000 test=0.650000\n",
      "batch 1690: train=0.600000 test=0.550000\n",
      "batch 1700: train=0.650000 test=0.700000\n",
      "batch 1710: train=0.550000 test=0.550000\n",
      "batch 1720: train=0.650000 test=0.450000\n",
      "batch 1730: train=0.650000 test=0.550000\n",
      "batch 1740: train=0.700000 test=0.300000\n",
      "batch 1750: train=0.650000 test=0.350000\n",
      "batch 1760: train=0.600000 test=0.550000\n",
      "batch 1770: train=0.650000 test=0.600000\n",
      "batch 1780: train=0.700000 test=0.550000\n",
      "batch 1790: train=0.600000 test=0.700000\n",
      "batch 1800: train=0.500000 test=0.650000\n",
      "batch 1810: train=0.800000 test=0.400000\n",
      "batch 1820: train=0.700000 test=0.450000\n",
      "batch 1830: train=0.600000 test=0.500000\n",
      "batch 1840: train=0.600000 test=0.500000\n",
      "batch 1850: train=0.550000 test=0.450000\n",
      "batch 1860: train=0.450000 test=0.600000\n",
      "batch 1870: train=0.600000 test=0.700000\n",
      "batch 1880: train=0.350000 test=0.550000\n",
      "batch 1890: train=0.500000 test=0.400000\n",
      "batch 1900: train=0.550000 test=0.650000\n",
      "batch 1910: train=0.400000 test=0.750000\n",
      "batch 1920: train=0.600000 test=0.400000\n",
      "batch 1930: train=0.700000 test=0.450000\n",
      "batch 1940: train=0.600000 test=0.550000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1950: train=0.600000 test=0.700000\n",
      "batch 1960: train=0.600000 test=0.800000\n",
      "batch 1970: train=0.700000 test=0.400000\n",
      "batch 1980: train=0.650000 test=0.700000\n",
      "batch 1990: train=0.750000 test=0.550000\n",
      "batch 2000: train=0.550000 test=0.350000\n",
      "batch 2010: train=0.650000 test=0.500000\n",
      "batch 2020: train=0.550000 test=0.600000\n",
      "batch 2030: train=0.600000 test=0.300000\n",
      "batch 2040: train=0.650000 test=0.550000\n",
      "batch 2050: train=0.550000 test=0.500000\n",
      "batch 2060: train=0.600000 test=0.450000\n",
      "batch 2070: train=0.750000 test=0.750000\n",
      "batch 2080: train=0.650000 test=0.450000\n",
      "batch 2090: train=0.650000 test=0.450000\n",
      "batch 2100: train=0.500000 test=0.550000\n",
      "batch 2110: train=0.600000 test=0.450000\n",
      "batch 2120: train=0.700000 test=0.600000\n",
      "batch 2130: train=0.750000 test=0.600000\n",
      "batch 2140: train=0.600000 test=0.500000\n",
      "batch 2150: train=0.850000 test=0.450000\n",
      "batch 2160: train=0.700000 test=0.500000\n",
      "batch 2170: train=0.850000 test=0.600000\n",
      "batch 2180: train=0.650000 test=0.450000\n",
      "batch 2190: train=0.700000 test=0.350000\n",
      "batch 2200: train=0.750000 test=0.600000\n",
      "batch 2210: train=0.650000 test=0.500000\n",
      "batch 2220: train=0.550000 test=0.550000\n",
      "batch 2230: train=0.600000 test=0.350000\n",
      "batch 2240: train=0.600000 test=0.400000\n",
      "batch 2250: train=0.400000 test=0.600000\n",
      "batch 2260: train=0.650000 test=0.650000\n",
      "batch 2270: train=0.650000 test=0.450000\n",
      "batch 2280: train=0.600000 test=0.500000\n",
      "batch 2290: train=0.700000 test=0.600000\n",
      "batch 2300: train=0.750000 test=0.450000\n",
      "batch 2310: train=0.550000 test=0.550000\n",
      "batch 2320: train=0.700000 test=0.450000\n",
      "batch 2330: train=0.450000 test=0.650000\n",
      "batch 2340: train=0.650000 test=0.500000\n",
      "batch 2350: train=0.600000 test=0.650000\n",
      "batch 2360: train=0.750000 test=0.550000\n",
      "batch 2370: train=0.600000 test=0.350000\n",
      "batch 2380: train=0.750000 test=0.650000\n",
      "batch 2390: train=0.700000 test=0.400000\n",
      "batch 2400: train=0.600000 test=0.650000\n",
      "batch 2410: train=0.500000 test=0.550000\n",
      "batch 2420: train=0.750000 test=0.700000\n",
      "batch 2430: train=0.600000 test=0.400000\n",
      "batch 2440: train=0.650000 test=0.700000\n",
      "batch 2450: train=0.700000 test=0.400000\n",
      "batch 2460: train=0.650000 test=0.700000\n",
      "batch 2470: train=0.600000 test=0.450000\n",
      "batch 2480: train=0.550000 test=0.650000\n",
      "batch 2490: train=0.550000 test=0.450000\n",
      "batch 2500: train=0.450000 test=0.550000\n",
      "batch 2510: train=0.750000 test=0.600000\n",
      "batch 2520: train=0.650000 test=0.600000\n",
      "batch 2530: train=0.800000 test=0.400000\n",
      "batch 2540: train=0.750000 test=0.650000\n",
      "batch 2550: train=0.700000 test=0.650000\n",
      "batch 2560: train=0.800000 test=0.500000\n",
      "batch 2570: train=0.650000 test=0.650000\n",
      "batch 2580: train=0.800000 test=0.600000\n",
      "batch 2590: train=0.450000 test=0.600000\n",
      "batch 2600: train=0.400000 test=0.450000\n",
      "batch 2610: train=0.450000 test=0.550000\n",
      "batch 2620: train=0.550000 test=0.600000\n",
      "batch 2630: train=0.600000 test=0.300000\n",
      "batch 2640: train=0.650000 test=0.500000\n",
      "batch 2650: train=0.700000 test=0.650000\n",
      "batch 2660: train=0.750000 test=0.700000\n",
      "batch 2670: train=0.600000 test=0.700000\n",
      "batch 2680: train=0.700000 test=0.450000\n",
      "batch 2690: train=0.650000 test=0.650000\n",
      "batch 2700: train=0.350000 test=0.550000\n",
      "batch 2710: train=0.600000 test=0.700000\n",
      "batch 2720: train=0.650000 test=0.650000\n",
      "batch 2730: train=0.600000 test=0.550000\n",
      "batch 2740: train=0.900000 test=0.400000\n",
      "batch 2750: train=0.650000 test=0.500000\n",
      "batch 2760: train=0.650000 test=0.500000\n",
      "batch 2770: train=0.500000 test=0.750000\n",
      "batch 2780: train=0.600000 test=0.600000\n",
      "batch 2790: train=0.450000 test=0.550000\n",
      "batch 2800: train=0.550000 test=0.450000\n",
      "batch 2810: train=0.700000 test=0.550000\n",
      "batch 2820: train=0.750000 test=0.600000\n",
      "batch 2830: train=0.600000 test=0.650000\n",
      "batch 2840: train=0.600000 test=0.500000\n",
      "batch 2850: train=0.750000 test=0.650000\n",
      "batch 2860: train=0.750000 test=0.550000\n",
      "batch 2870: train=0.600000 test=0.600000\n",
      "batch 2880: train=0.650000 test=0.650000\n",
      "batch 2890: train=0.700000 test=0.650000\n",
      "batch 2900: train=0.750000 test=0.500000\n",
      "batch 2910: train=0.400000 test=0.550000\n",
      "batch 2920: train=0.650000 test=0.550000\n",
      "batch 2930: train=0.500000 test=0.550000\n",
      "batch 2940: train=1.000000 test=0.550000\n",
      "batch 2950: train=0.750000 test=0.450000\n",
      "batch 2960: train=0.850000 test=0.650000\n",
      "batch 2970: train=0.650000 test=0.400000\n",
      "batch 2980: train=0.500000 test=0.400000\n",
      "batch 2990: train=0.700000 test=0.400000\n",
      "batch 3000: train=0.600000 test=0.650000\n",
      "batch 3010: train=0.750000 test=0.500000\n",
      "batch 3020: train=0.650000 test=0.500000\n",
      "batch 3030: train=0.400000 test=0.550000\n",
      "batch 3040: train=0.500000 test=0.650000\n",
      "batch 3050: train=0.650000 test=0.800000\n",
      "batch 3060: train=0.850000 test=0.500000\n",
      "batch 3070: train=0.750000 test=0.600000\n",
      "batch 3080: train=0.700000 test=0.350000\n",
      "batch 3090: train=0.800000 test=0.350000\n",
      "batch 3100: train=0.500000 test=0.450000\n",
      "batch 3110: train=0.700000 test=0.450000\n",
      "batch 3120: train=0.700000 test=0.600000\n",
      "batch 3130: train=0.750000 test=0.650000\n",
      "batch 3140: train=0.600000 test=0.700000\n",
      "batch 3150: train=0.550000 test=0.450000\n",
      "batch 3160: train=0.400000 test=0.500000\n",
      "batch 3170: train=0.500000 test=0.500000\n",
      "batch 3180: train=0.550000 test=0.550000\n",
      "batch 3190: train=0.700000 test=0.700000\n",
      "batch 3200: train=0.750000 test=0.550000\n",
      "batch 3210: train=0.700000 test=0.500000\n",
      "batch 3220: train=0.650000 test=0.650000\n",
      "batch 3230: train=0.650000 test=0.550000\n",
      "batch 3240: train=0.700000 test=0.500000\n",
      "batch 3250: train=0.600000 test=0.550000\n",
      "batch 3260: train=0.650000 test=0.600000\n",
      "batch 3270: train=0.400000 test=0.400000\n",
      "batch 3280: train=0.700000 test=0.650000\n",
      "batch 3290: train=0.700000 test=0.600000\n",
      "batch 3300: train=0.850000 test=0.450000\n",
      "batch 3310: train=0.750000 test=0.500000\n",
      "batch 3320: train=0.550000 test=0.600000\n",
      "batch 3330: train=0.700000 test=0.500000\n",
      "batch 3340: train=0.650000 test=0.500000\n",
      "batch 3350: train=0.650000 test=0.650000\n",
      "batch 3360: train=0.500000 test=0.450000\n",
      "batch 3370: train=0.600000 test=0.700000\n",
      "batch 3380: train=0.750000 test=0.600000\n",
      "batch 3390: train=0.700000 test=0.600000\n",
      "batch 3400: train=0.700000 test=0.450000\n",
      "batch 3410: train=0.700000 test=0.650000\n",
      "batch 3420: train=0.550000 test=0.750000\n",
      "batch 3430: train=0.650000 test=0.500000\n",
      "batch 3440: train=0.550000 test=0.600000\n",
      "batch 3450: train=0.650000 test=0.600000\n",
      "batch 3460: train=0.500000 test=0.700000\n",
      "batch 3470: train=0.750000 test=0.450000\n",
      "batch 3480: train=0.750000 test=0.550000\n",
      "batch 3490: train=0.400000 test=0.550000\n",
      "batch 3500: train=0.650000 test=0.550000\n",
      "batch 3510: train=0.550000 test=0.550000\n",
      "batch 3520: train=0.700000 test=0.550000\n",
      "batch 3530: train=0.550000 test=0.700000\n",
      "batch 3540: train=0.650000 test=0.600000\n",
      "batch 3550: train=0.900000 test=0.600000\n",
      "batch 3560: train=0.650000 test=0.600000\n",
      "batch 3570: train=0.650000 test=0.600000\n",
      "batch 3580: train=0.700000 test=0.500000\n",
      "batch 3590: train=0.600000 test=0.700000\n",
      "batch 3600: train=0.450000 test=0.600000\n",
      "batch 3610: train=0.800000 test=0.400000\n",
      "batch 3620: train=0.600000 test=0.600000\n",
      "batch 3630: train=0.750000 test=0.600000\n",
      "batch 3640: train=0.850000 test=0.550000\n",
      "batch 3650: train=0.700000 test=0.400000\n",
      "batch 3660: train=0.650000 test=0.500000\n",
      "batch 3670: train=0.600000 test=0.600000\n",
      "batch 3680: train=0.800000 test=0.500000\n",
      "batch 3690: train=0.650000 test=0.500000\n",
      "batch 3700: train=0.700000 test=0.700000\n",
      "batch 3710: train=0.750000 test=0.650000\n",
      "batch 3720: train=0.750000 test=0.400000\n",
      "batch 3730: train=0.500000 test=0.750000\n",
      "batch 3740: train=0.750000 test=0.550000\n",
      "batch 3750: train=0.650000 test=0.550000\n",
      "batch 3760: train=0.600000 test=0.600000\n",
      "batch 3770: train=0.700000 test=0.550000\n",
      "batch 3780: train=0.850000 test=0.550000\n",
      "batch 3790: train=0.600000 test=0.550000\n",
      "batch 3800: train=0.800000 test=0.600000\n",
      "batch 3810: train=0.650000 test=0.550000\n",
      "batch 3820: train=0.650000 test=0.650000\n",
      "batch 3830: train=0.700000 test=0.500000\n",
      "batch 3840: train=0.750000 test=0.500000\n",
      "batch 3850: train=0.750000 test=0.400000\n",
      "batch 3860: train=0.550000 test=0.700000\n",
      "batch 3870: train=0.650000 test=0.450000\n",
      "batch 3880: train=0.500000 test=0.600000\n",
      "batch 3890: train=0.850000 test=0.550000\n",
      "batch 3900: train=0.850000 test=0.550000\n",
      "batch 3910: train=0.500000 test=0.400000\n",
      "batch 3920: train=0.650000 test=0.450000\n",
      "batch 3930: train=0.750000 test=0.550000\n",
      "batch 3940: train=0.850000 test=0.650000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3950: train=0.700000 test=0.600000\n",
      "batch 3960: train=0.600000 test=0.600000\n",
      "batch 3970: train=0.650000 test=0.600000\n",
      "batch 3980: train=0.650000 test=0.500000\n",
      "batch 3990: train=0.650000 test=0.750000\n",
      "batch 4000: train=0.600000 test=0.750000\n",
      "batch 4010: train=0.650000 test=0.550000\n",
      "batch 4020: train=0.700000 test=0.600000\n",
      "batch 4030: train=0.600000 test=0.600000\n",
      "batch 4040: train=0.750000 test=0.650000\n",
      "batch 4050: train=0.750000 test=0.800000\n",
      "batch 4060: train=0.550000 test=0.450000\n",
      "batch 4070: train=0.800000 test=0.450000\n",
      "batch 4080: train=0.550000 test=0.800000\n",
      "batch 4090: train=0.850000 test=0.500000\n",
      "batch 4100: train=0.750000 test=0.750000\n",
      "batch 4110: train=0.550000 test=0.500000\n",
      "batch 4120: train=0.600000 test=0.400000\n",
      "batch 4130: train=0.500000 test=0.550000\n",
      "batch 4140: train=0.850000 test=0.550000\n",
      "batch 4150: train=0.650000 test=0.350000\n",
      "batch 4160: train=0.550000 test=0.450000\n",
      "batch 4170: train=0.700000 test=0.500000\n",
      "batch 4180: train=0.700000 test=0.800000\n",
      "batch 4190: train=0.850000 test=0.350000\n",
      "batch 4200: train=0.700000 test=0.600000\n",
      "batch 4210: train=0.750000 test=0.650000\n",
      "batch 4220: train=0.650000 test=0.500000\n",
      "batch 4230: train=0.750000 test=0.650000\n",
      "batch 4240: train=0.700000 test=0.700000\n",
      "batch 4250: train=0.600000 test=0.600000\n",
      "batch 4260: train=0.850000 test=0.500000\n",
      "batch 4270: train=0.650000 test=0.600000\n",
      "batch 4280: train=0.700000 test=0.600000\n",
      "batch 4290: train=0.650000 test=0.800000\n",
      "batch 4300: train=0.650000 test=0.550000\n",
      "batch 4310: train=0.750000 test=0.550000\n",
      "batch 4320: train=0.650000 test=0.500000\n",
      "batch 4330: train=0.800000 test=0.450000\n",
      "batch 4340: train=0.700000 test=0.600000\n",
      "batch 4350: train=0.700000 test=0.450000\n",
      "batch 4360: train=0.600000 test=0.650000\n",
      "batch 4370: train=0.700000 test=0.700000\n",
      "batch 4380: train=0.800000 test=0.500000\n",
      "batch 4390: train=0.700000 test=0.500000\n",
      "batch 4400: train=0.700000 test=0.750000\n",
      "batch 4410: train=0.700000 test=0.600000\n",
      "batch 4420: train=0.800000 test=0.650000\n",
      "batch 4430: train=0.700000 test=0.500000\n",
      "batch 4440: train=0.700000 test=0.500000\n",
      "batch 4450: train=0.750000 test=0.450000\n",
      "batch 4460: train=0.850000 test=0.650000\n",
      "batch 4470: train=0.650000 test=0.900000\n",
      "batch 4480: train=0.800000 test=0.500000\n",
      "batch 4490: train=0.750000 test=0.600000\n",
      "batch 4500: train=0.750000 test=0.400000\n",
      "batch 4510: train=0.700000 test=0.750000\n",
      "batch 4520: train=0.750000 test=0.450000\n",
      "batch 4530: train=0.700000 test=0.650000\n",
      "batch 4540: train=0.650000 test=0.550000\n",
      "batch 4550: train=0.650000 test=0.600000\n",
      "batch 4560: train=0.600000 test=0.650000\n",
      "batch 4570: train=0.750000 test=0.400000\n",
      "batch 4580: train=0.800000 test=0.550000\n",
      "batch 4590: train=0.700000 test=0.700000\n",
      "batch 4600: train=0.650000 test=0.600000\n",
      "batch 4610: train=0.700000 test=0.600000\n",
      "batch 4620: train=0.500000 test=0.550000\n",
      "batch 4630: train=0.500000 test=0.450000\n",
      "batch 4640: train=0.750000 test=0.700000\n",
      "batch 4650: train=0.650000 test=0.750000\n",
      "batch 4660: train=0.700000 test=0.800000\n",
      "batch 4670: train=0.700000 test=0.600000\n",
      "batch 4680: train=0.600000 test=0.600000\n",
      "batch 4690: train=0.500000 test=0.550000\n",
      "batch 4700: train=0.850000 test=0.650000\n",
      "batch 4710: train=0.800000 test=0.650000\n",
      "batch 4720: train=0.550000 test=0.500000\n",
      "batch 4730: train=0.700000 test=0.750000\n",
      "batch 4740: train=0.600000 test=0.500000\n",
      "batch 4750: train=0.600000 test=0.600000\n",
      "batch 4760: train=0.750000 test=0.550000\n",
      "batch 4770: train=0.700000 test=0.650000\n",
      "batch 4780: train=0.750000 test=0.650000\n",
      "batch 4790: train=0.750000 test=0.800000\n",
      "batch 4800: train=0.550000 test=0.800000\n",
      "batch 4810: train=0.750000 test=0.500000\n",
      "batch 4820: train=0.700000 test=0.550000\n",
      "batch 4830: train=0.700000 test=0.700000\n",
      "batch 4840: train=0.750000 test=0.650000\n",
      "batch 4850: train=0.700000 test=0.700000\n",
      "batch 4860: train=0.450000 test=0.700000\n",
      "batch 4870: train=0.600000 test=0.650000\n",
      "batch 4880: train=0.600000 test=0.600000\n",
      "batch 4890: train=0.700000 test=0.900000\n",
      "batch 4900: train=0.750000 test=0.400000\n",
      "batch 4910: train=0.650000 test=0.700000\n",
      "batch 4920: train=0.650000 test=0.550000\n",
      "batch 4930: train=0.650000 test=0.500000\n",
      "batch 4940: train=0.600000 test=0.700000\n",
      "batch 4950: train=0.750000 test=0.800000\n",
      "batch 4960: train=0.650000 test=0.550000\n",
      "batch 4970: train=0.650000 test=0.550000\n",
      "batch 4980: train=0.750000 test=0.650000\n",
      "batch 4990: train=0.550000 test=0.800000\n",
      "batch 5000: train=0.800000 test=0.800000\n",
      "batch 5010: train=0.700000 test=0.600000\n",
      "batch 5020: train=0.650000 test=0.500000\n",
      "batch 5030: train=0.650000 test=0.700000\n",
      "batch 5040: train=0.600000 test=0.550000\n",
      "batch 5050: train=0.700000 test=0.550000\n",
      "batch 5060: train=0.800000 test=0.600000\n",
      "batch 5070: train=0.650000 test=0.750000\n",
      "batch 5080: train=0.700000 test=0.700000\n",
      "batch 5090: train=0.650000 test=0.800000\n",
      "batch 5100: train=0.700000 test=0.500000\n",
      "batch 5110: train=0.650000 test=0.650000\n",
      "batch 5120: train=0.750000 test=0.650000\n",
      "batch 5130: train=0.750000 test=0.600000\n",
      "batch 5140: train=0.750000 test=0.550000\n",
      "batch 5150: train=0.650000 test=0.550000\n",
      "batch 5160: train=0.650000 test=0.650000\n",
      "batch 5170: train=0.650000 test=0.600000\n",
      "batch 5180: train=0.800000 test=0.500000\n",
      "batch 5190: train=0.700000 test=0.600000\n",
      "batch 5200: train=0.550000 test=0.500000\n",
      "batch 5210: train=0.800000 test=0.700000\n",
      "batch 5220: train=0.700000 test=0.500000\n",
      "batch 5230: train=0.750000 test=0.550000\n",
      "batch 5240: train=0.950000 test=0.650000\n",
      "batch 5250: train=0.600000 test=0.600000\n",
      "batch 5260: train=0.650000 test=0.650000\n",
      "batch 5270: train=0.700000 test=0.550000\n",
      "batch 5280: train=0.700000 test=0.700000\n",
      "batch 5290: train=0.700000 test=0.600000\n",
      "batch 5300: train=0.600000 test=0.550000\n",
      "batch 5310: train=0.750000 test=0.750000\n",
      "batch 5320: train=0.750000 test=0.550000\n",
      "batch 5330: train=0.800000 test=0.650000\n",
      "batch 5340: train=0.500000 test=0.550000\n",
      "batch 5350: train=0.650000 test=0.450000\n",
      "batch 5360: train=0.700000 test=0.800000\n",
      "batch 5370: train=0.850000 test=0.700000\n",
      "batch 5380: train=0.600000 test=0.700000\n",
      "batch 5390: train=0.650000 test=0.600000\n",
      "batch 5400: train=0.700000 test=0.600000\n",
      "batch 5410: train=0.700000 test=0.650000\n",
      "batch 5420: train=0.850000 test=0.700000\n",
      "batch 5430: train=0.800000 test=0.600000\n",
      "batch 5440: train=0.900000 test=0.450000\n",
      "batch 5450: train=0.600000 test=0.450000\n",
      "batch 5460: train=0.700000 test=0.600000\n",
      "batch 5470: train=0.800000 test=0.650000\n",
      "batch 5480: train=0.750000 test=0.600000\n",
      "batch 5490: train=0.700000 test=0.400000\n",
      "batch 5500: train=0.800000 test=0.700000\n",
      "batch 5510: train=0.850000 test=0.700000\n",
      "batch 5520: train=0.700000 test=0.500000\n",
      "batch 5530: train=0.700000 test=0.650000\n",
      "batch 5540: train=0.850000 test=0.650000\n",
      "batch 5550: train=0.700000 test=0.500000\n",
      "batch 5560: train=0.750000 test=0.700000\n",
      "batch 5570: train=0.700000 test=0.750000\n",
      "batch 5580: train=0.700000 test=0.700000\n",
      "batch 5590: train=0.700000 test=0.700000\n",
      "batch 5600: train=0.800000 test=0.550000\n",
      "batch 5610: train=0.700000 test=0.650000\n",
      "batch 5620: train=0.750000 test=0.800000\n",
      "batch 5630: train=0.800000 test=0.550000\n",
      "batch 5640: train=0.700000 test=0.500000\n",
      "batch 5650: train=0.600000 test=0.500000\n",
      "batch 5660: train=0.700000 test=0.400000\n",
      "batch 5670: train=0.750000 test=0.450000\n",
      "batch 5680: train=0.800000 test=0.700000\n",
      "batch 5690: train=0.650000 test=0.550000\n",
      "batch 5700: train=0.650000 test=0.600000\n",
      "batch 5710: train=0.800000 test=0.550000\n",
      "batch 5720: train=0.600000 test=0.700000\n",
      "batch 5730: train=0.800000 test=0.500000\n",
      "batch 5740: train=0.700000 test=0.750000\n",
      "batch 5750: train=0.850000 test=0.750000\n",
      "batch 5760: train=0.850000 test=0.550000\n",
      "batch 5770: train=0.650000 test=0.650000\n",
      "batch 5780: train=0.800000 test=0.600000\n",
      "batch 5790: train=0.650000 test=0.750000\n",
      "batch 5800: train=0.650000 test=0.700000\n",
      "batch 5810: train=0.650000 test=0.600000\n",
      "batch 5820: train=0.600000 test=0.450000\n",
      "batch 5830: train=0.550000 test=0.750000\n",
      "batch 5840: train=0.700000 test=0.650000\n",
      "batch 5850: train=0.800000 test=0.600000\n",
      "batch 5860: train=0.700000 test=0.500000\n",
      "batch 5870: train=0.750000 test=0.600000\n",
      "batch 5880: train=0.650000 test=0.700000\n",
      "batch 5890: train=0.700000 test=0.650000\n",
      "batch 5900: train=0.850000 test=0.450000\n",
      "batch 5910: train=0.650000 test=0.500000\n",
      "batch 5920: train=0.650000 test=0.600000\n",
      "batch 5930: train=0.750000 test=0.600000\n",
      "batch 5940: train=0.650000 test=0.450000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5950: train=0.800000 test=0.600000\n",
      "batch 5960: train=0.600000 test=0.550000\n",
      "batch 5970: train=0.700000 test=0.550000\n",
      "batch 5980: train=0.650000 test=0.450000\n",
      "batch 5990: train=0.750000 test=0.400000\n",
      "batch 6000: train=0.750000 test=0.600000\n",
      "batch 6010: train=0.750000 test=0.800000\n",
      "batch 6020: train=0.700000 test=0.650000\n",
      "batch 6030: train=0.600000 test=0.650000\n",
      "batch 6040: train=0.850000 test=0.800000\n",
      "batch 6050: train=0.800000 test=0.650000\n",
      "batch 6060: train=0.600000 test=0.700000\n",
      "batch 6070: train=0.650000 test=0.400000\n",
      "batch 6080: train=0.700000 test=0.750000\n",
      "batch 6090: train=0.550000 test=0.600000\n",
      "batch 6100: train=0.800000 test=0.550000\n",
      "batch 6110: train=0.800000 test=0.650000\n",
      "batch 6120: train=0.700000 test=0.700000\n",
      "batch 6130: train=0.700000 test=0.800000\n",
      "batch 6140: train=0.750000 test=0.550000\n",
      "batch 6150: train=0.750000 test=0.450000\n",
      "batch 6160: train=0.750000 test=0.750000\n",
      "batch 6170: train=0.800000 test=0.400000\n",
      "batch 6180: train=0.600000 test=0.500000\n",
      "batch 6190: train=0.600000 test=0.800000\n",
      "batch 6200: train=0.650000 test=0.650000\n",
      "batch 6210: train=0.800000 test=0.650000\n",
      "batch 6220: train=0.600000 test=0.750000\n",
      "batch 6230: train=0.700000 test=0.800000\n",
      "batch 6240: train=0.700000 test=0.550000\n",
      "batch 6250: train=0.750000 test=0.550000\n",
      "batch 6260: train=0.550000 test=0.750000\n",
      "batch 6270: train=0.600000 test=0.800000\n",
      "batch 6280: train=0.850000 test=0.800000\n",
      "batch 6290: train=0.550000 test=0.500000\n",
      "batch 6300: train=0.850000 test=0.600000\n",
      "batch 6310: train=0.700000 test=0.500000\n",
      "batch 6320: train=0.550000 test=0.650000\n",
      "batch 6330: train=0.900000 test=0.750000\n",
      "batch 6340: train=0.700000 test=0.400000\n",
      "batch 6350: train=0.800000 test=0.550000\n",
      "batch 6360: train=0.650000 test=0.800000\n",
      "batch 6370: train=0.700000 test=0.450000\n",
      "batch 6380: train=0.650000 test=0.550000\n",
      "batch 6390: train=0.900000 test=0.750000\n",
      "batch 6400: train=0.600000 test=0.600000\n",
      "batch 6410: train=0.800000 test=0.550000\n",
      "batch 6420: train=0.800000 test=0.750000\n",
      "batch 6430: train=0.700000 test=0.600000\n",
      "batch 6440: train=0.750000 test=0.600000\n",
      "batch 6450: train=0.750000 test=0.600000\n",
      "batch 6460: train=0.800000 test=0.600000\n",
      "batch 6470: train=0.800000 test=0.800000\n",
      "batch 6480: train=0.750000 test=0.500000\n",
      "batch 6490: train=0.850000 test=0.850000\n",
      "batch 6500: train=0.750000 test=0.600000\n",
      "batch 6510: train=0.550000 test=0.700000\n",
      "batch 6520: train=0.800000 test=0.500000\n",
      "batch 6530: train=0.700000 test=0.650000\n",
      "batch 6540: train=0.750000 test=0.700000\n",
      "batch 6550: train=0.550000 test=0.650000\n",
      "batch 6560: train=0.700000 test=0.550000\n",
      "batch 6570: train=0.850000 test=0.650000\n",
      "batch 6580: train=0.800000 test=0.600000\n",
      "batch 6590: train=0.700000 test=0.650000\n",
      "batch 6600: train=0.700000 test=0.450000\n",
      "batch 6610: train=0.600000 test=0.750000\n",
      "batch 6620: train=0.550000 test=0.650000\n",
      "batch 6630: train=0.900000 test=0.600000\n",
      "batch 6640: train=0.700000 test=0.600000\n",
      "batch 6650: train=0.750000 test=0.700000\n",
      "batch 6660: train=0.900000 test=0.550000\n",
      "batch 6670: train=0.600000 test=0.650000\n",
      "batch 6680: train=0.550000 test=0.650000\n",
      "batch 6690: train=0.850000 test=0.550000\n",
      "batch 6700: train=0.550000 test=0.600000\n",
      "batch 6710: train=0.800000 test=0.800000\n",
      "batch 6720: train=0.750000 test=0.700000\n",
      "batch 6730: train=0.750000 test=0.600000\n",
      "batch 6740: train=0.800000 test=0.650000\n",
      "batch 6750: train=0.700000 test=0.550000\n",
      "batch 6760: train=0.800000 test=0.700000\n",
      "batch 6770: train=0.800000 test=0.650000\n",
      "batch 6780: train=0.850000 test=0.600000\n",
      "batch 6790: train=0.700000 test=0.600000\n",
      "batch 6800: train=0.750000 test=0.400000\n",
      "batch 6810: train=0.750000 test=0.650000\n",
      "batch 6820: train=0.600000 test=0.650000\n",
      "batch 6830: train=0.800000 test=0.700000\n",
      "batch 6840: train=0.650000 test=0.700000\n",
      "batch 6850: train=0.750000 test=0.850000\n",
      "batch 6860: train=0.700000 test=0.600000\n",
      "batch 6870: train=0.800000 test=0.500000\n",
      "batch 6880: train=0.800000 test=0.450000\n",
      "batch 6890: train=0.700000 test=0.650000\n",
      "batch 6900: train=0.850000 test=0.750000\n",
      "batch 6910: train=0.700000 test=0.600000\n",
      "batch 6920: train=0.650000 test=0.600000\n",
      "batch 6930: train=0.750000 test=0.750000\n",
      "batch 6940: train=0.650000 test=0.800000\n",
      "batch 6950: train=0.750000 test=0.600000\n",
      "batch 6960: train=0.850000 test=0.700000\n",
      "batch 6970: train=0.850000 test=0.600000\n",
      "batch 6980: train=0.750000 test=0.800000\n",
      "batch 6990: train=0.650000 test=0.650000\n",
      "batch 7000: train=0.650000 test=0.600000\n",
      "batch 7010: train=0.900000 test=0.650000\n",
      "batch 7020: train=0.700000 test=0.350000\n",
      "batch 7030: train=0.750000 test=0.750000\n",
      "batch 7040: train=0.550000 test=0.700000\n",
      "batch 7050: train=0.800000 test=0.650000\n",
      "batch 7060: train=0.900000 test=0.700000\n",
      "batch 7070: train=0.700000 test=0.600000\n",
      "batch 7080: train=0.650000 test=0.550000\n",
      "batch 7090: train=0.500000 test=0.700000\n",
      "batch 7100: train=0.700000 test=0.650000\n",
      "batch 7110: train=0.600000 test=0.700000\n",
      "batch 7120: train=0.600000 test=0.700000\n",
      "batch 7130: train=0.600000 test=0.700000\n",
      "batch 7140: train=0.750000 test=0.500000\n",
      "batch 7150: train=0.800000 test=0.700000\n",
      "batch 7160: train=0.700000 test=0.650000\n",
      "batch 7170: train=0.850000 test=0.550000\n",
      "batch 7180: train=0.750000 test=0.600000\n",
      "batch 7190: train=0.950000 test=0.600000\n",
      "batch 7200: train=0.750000 test=0.850000\n",
      "batch 7210: train=0.700000 test=0.750000\n",
      "batch 7220: train=0.800000 test=0.600000\n",
      "batch 7230: train=0.650000 test=0.650000\n",
      "batch 7240: train=0.750000 test=0.400000\n",
      "batch 7250: train=0.800000 test=0.600000\n",
      "batch 7260: train=0.950000 test=0.700000\n",
      "batch 7270: train=0.700000 test=0.600000\n",
      "batch 7280: train=0.800000 test=0.700000\n",
      "batch 7290: train=0.700000 test=0.650000\n",
      "batch 7300: train=0.700000 test=0.650000\n",
      "batch 7310: train=0.650000 test=0.400000\n",
      "batch 7320: train=0.650000 test=0.500000\n",
      "batch 7330: train=0.550000 test=0.750000\n",
      "batch 7340: train=0.750000 test=0.550000\n",
      "batch 7350: train=0.800000 test=0.600000\n",
      "batch 7360: train=0.750000 test=0.650000\n",
      "batch 7370: train=0.750000 test=0.500000\n",
      "batch 7380: train=0.550000 test=0.700000\n",
      "batch 7390: train=0.650000 test=0.750000\n",
      "batch 7400: train=0.650000 test=0.700000\n",
      "batch 7410: train=0.650000 test=0.850000\n",
      "batch 7420: train=0.750000 test=0.800000\n",
      "batch 7430: train=0.600000 test=0.600000\n",
      "batch 7440: train=0.800000 test=0.700000\n",
      "batch 7450: train=0.700000 test=0.550000\n",
      "batch 7460: train=0.800000 test=0.750000\n",
      "batch 7470: train=0.950000 test=0.750000\n",
      "batch 7480: train=0.650000 test=0.700000\n",
      "batch 7490: train=0.500000 test=0.450000\n",
      "batch 7500: train=0.850000 test=0.750000\n",
      "batch 7510: train=0.800000 test=0.650000\n",
      "batch 7520: train=0.750000 test=0.750000\n",
      "batch 7530: train=0.850000 test=0.700000\n",
      "batch 7540: train=0.750000 test=0.500000\n",
      "batch 7550: train=0.750000 test=0.450000\n",
      "batch 7560: train=0.750000 test=0.700000\n",
      "batch 7570: train=0.700000 test=0.700000\n",
      "batch 7580: train=0.700000 test=0.800000\n",
      "batch 7590: train=0.550000 test=0.400000\n",
      "batch 7600: train=0.800000 test=0.650000\n",
      "batch 7610: train=0.700000 test=0.600000\n",
      "batch 7620: train=0.800000 test=0.750000\n",
      "batch 7630: train=0.800000 test=0.650000\n",
      "batch 7640: train=0.800000 test=0.700000\n",
      "batch 7650: train=0.850000 test=0.700000\n",
      "batch 7660: train=0.800000 test=0.700000\n",
      "batch 7670: train=0.750000 test=0.700000\n",
      "batch 7680: train=0.850000 test=0.650000\n",
      "batch 7690: train=0.700000 test=0.650000\n",
      "batch 7700: train=0.800000 test=0.800000\n",
      "batch 7710: train=0.550000 test=0.700000\n",
      "batch 7720: train=0.650000 test=0.700000\n",
      "batch 7730: train=0.800000 test=0.450000\n",
      "batch 7740: train=0.900000 test=0.750000\n",
      "batch 7750: train=0.950000 test=0.650000\n",
      "batch 7760: train=0.850000 test=0.650000\n",
      "batch 7770: train=0.800000 test=0.700000\n",
      "batch 7780: train=0.650000 test=0.650000\n",
      "batch 7790: train=0.900000 test=0.500000\n",
      "batch 7800: train=0.750000 test=0.700000\n",
      "batch 7810: train=0.500000 test=0.800000\n",
      "batch 7820: train=0.800000 test=0.650000\n",
      "batch 7830: train=0.600000 test=0.750000\n",
      "batch 7840: train=0.750000 test=0.650000\n",
      "batch 7850: train=0.750000 test=0.650000\n",
      "batch 7860: train=0.700000 test=0.650000\n",
      "batch 7870: train=0.650000 test=0.800000\n",
      "batch 7880: train=0.750000 test=0.650000\n",
      "batch 7890: train=0.850000 test=0.700000\n",
      "batch 7900: train=0.800000 test=0.750000\n",
      "batch 7910: train=0.900000 test=0.750000\n",
      "batch 7920: train=0.700000 test=0.550000\n",
      "batch 7930: train=0.750000 test=0.600000\n",
      "batch 7940: train=0.850000 test=0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7950: train=0.700000 test=0.800000\n",
      "batch 7960: train=0.650000 test=0.700000\n",
      "batch 7970: train=1.000000 test=0.700000\n",
      "batch 7980: train=0.750000 test=0.650000\n",
      "batch 7990: train=0.750000 test=0.600000\n",
      "batch 8000: train=0.800000 test=0.750000\n",
      "batch 8010: train=0.650000 test=0.700000\n",
      "batch 8020: train=0.700000 test=0.800000\n",
      "batch 8030: train=0.550000 test=0.750000\n",
      "batch 8040: train=0.550000 test=0.750000\n",
      "batch 8050: train=0.700000 test=0.700000\n",
      "batch 8060: train=0.650000 test=0.650000\n",
      "batch 8070: train=0.650000 test=0.700000\n",
      "batch 8080: train=0.800000 test=0.700000\n",
      "batch 8090: train=0.950000 test=0.650000\n",
      "batch 8100: train=0.750000 test=0.700000\n",
      "batch 8110: train=0.750000 test=0.750000\n",
      "batch 8120: train=0.650000 test=0.700000\n",
      "batch 8130: train=0.800000 test=0.550000\n",
      "batch 8140: train=0.850000 test=0.650000\n",
      "batch 8150: train=0.750000 test=0.750000\n",
      "batch 8160: train=0.700000 test=0.600000\n",
      "batch 8170: train=0.750000 test=0.500000\n",
      "batch 8180: train=0.650000 test=0.800000\n",
      "batch 8190: train=0.750000 test=0.750000\n",
      "batch 8200: train=0.850000 test=0.800000\n",
      "batch 8210: train=0.700000 test=0.750000\n",
      "batch 8220: train=0.600000 test=0.650000\n",
      "batch 8230: train=0.700000 test=0.850000\n",
      "batch 8240: train=0.800000 test=0.550000\n",
      "batch 8250: train=0.850000 test=0.800000\n",
      "batch 8260: train=0.700000 test=0.550000\n",
      "batch 8270: train=0.800000 test=0.800000\n",
      "batch 8280: train=0.600000 test=0.800000\n",
      "batch 8290: train=0.750000 test=0.650000\n",
      "batch 8300: train=0.750000 test=0.400000\n",
      "batch 8310: train=0.700000 test=0.700000\n",
      "batch 8320: train=0.800000 test=0.600000\n",
      "batch 8330: train=0.850000 test=0.600000\n",
      "batch 8340: train=0.700000 test=0.800000\n",
      "batch 8350: train=0.950000 test=0.750000\n",
      "batch 8360: train=0.900000 test=0.700000\n",
      "batch 8370: train=1.000000 test=0.750000\n",
      "batch 8380: train=0.800000 test=0.700000\n",
      "batch 8390: train=0.800000 test=0.650000\n",
      "batch 8400: train=0.900000 test=0.650000\n",
      "batch 8410: train=0.850000 test=0.650000\n",
      "batch 8420: train=0.850000 test=0.700000\n",
      "batch 8430: train=0.800000 test=0.850000\n",
      "batch 8440: train=0.800000 test=0.550000\n",
      "batch 8450: train=0.800000 test=0.700000\n",
      "batch 8460: train=0.800000 test=0.750000\n",
      "batch 8470: train=0.850000 test=0.750000\n",
      "batch 8480: train=0.800000 test=0.900000\n",
      "batch 8490: train=0.850000 test=0.800000\n",
      "batch 8500: train=0.850000 test=0.700000\n",
      "batch 8510: train=0.850000 test=0.550000\n",
      "batch 8520: train=0.750000 test=0.550000\n",
      "batch 8530: train=0.800000 test=0.850000\n",
      "batch 8540: train=0.800000 test=0.600000\n",
      "batch 8550: train=0.600000 test=0.550000\n",
      "batch 8560: train=0.650000 test=0.850000\n",
      "batch 8570: train=0.700000 test=0.850000\n",
      "batch 8580: train=0.900000 test=0.650000\n",
      "batch 8590: train=0.750000 test=0.750000\n",
      "batch 8600: train=0.800000 test=0.700000\n",
      "batch 8610: train=0.750000 test=0.700000\n",
      "batch 8620: train=0.750000 test=0.550000\n",
      "batch 8630: train=0.750000 test=0.600000\n",
      "batch 8640: train=0.750000 test=0.750000\n",
      "batch 8650: train=0.800000 test=0.700000\n",
      "batch 8660: train=0.800000 test=0.550000\n",
      "batch 8670: train=0.700000 test=0.800000\n",
      "batch 8680: train=0.800000 test=0.550000\n",
      "batch 8690: train=0.700000 test=0.650000\n",
      "batch 8700: train=0.700000 test=0.750000\n",
      "batch 8710: train=0.750000 test=0.700000\n",
      "batch 8720: train=0.750000 test=0.600000\n",
      "batch 8730: train=0.950000 test=0.800000\n",
      "batch 8740: train=0.850000 test=0.700000\n",
      "batch 8750: train=0.800000 test=0.850000\n",
      "batch 8760: train=0.800000 test=0.800000\n",
      "batch 8770: train=0.900000 test=0.650000\n",
      "batch 8780: train=0.800000 test=0.750000\n",
      "batch 8790: train=0.850000 test=0.800000\n",
      "batch 8800: train=0.800000 test=0.800000\n",
      "batch 8810: train=0.800000 test=0.500000\n",
      "batch 8820: train=0.900000 test=0.600000\n",
      "batch 8830: train=0.850000 test=0.500000\n",
      "batch 8840: train=0.900000 test=0.700000\n",
      "batch 8850: train=0.800000 test=0.750000\n",
      "batch 8860: train=0.800000 test=0.450000\n",
      "batch 8870: train=0.750000 test=0.650000\n",
      "batch 8880: train=0.900000 test=0.750000\n",
      "batch 8890: train=0.800000 test=0.700000\n",
      "batch 8900: train=0.800000 test=0.750000\n",
      "batch 8910: train=0.800000 test=0.800000\n",
      "batch 8920: train=0.500000 test=0.600000\n",
      "batch 8930: train=0.650000 test=0.750000\n",
      "batch 8940: train=0.700000 test=0.600000\n",
      "batch 8950: train=0.950000 test=0.550000\n",
      "batch 8960: train=0.550000 test=0.750000\n",
      "batch 8970: train=0.800000 test=0.700000\n",
      "batch 8980: train=0.600000 test=0.750000\n",
      "batch 8990: train=0.700000 test=0.900000\n",
      "batch 9000: train=0.850000 test=0.750000\n",
      "batch 9010: train=0.850000 test=0.900000\n",
      "batch 9020: train=0.650000 test=0.900000\n",
      "batch 9030: train=0.750000 test=0.650000\n",
      "batch 9040: train=0.750000 test=0.800000\n",
      "batch 9050: train=0.750000 test=0.900000\n",
      "batch 9060: train=0.850000 test=0.600000\n",
      "batch 9070: train=0.750000 test=1.000000\n",
      "batch 9080: train=0.750000 test=0.700000\n",
      "batch 9090: train=0.900000 test=0.550000\n",
      "batch 9100: train=0.800000 test=0.550000\n",
      "batch 9110: train=0.750000 test=0.750000\n",
      "batch 9120: train=0.850000 test=0.600000\n",
      "batch 9130: train=0.900000 test=0.750000\n",
      "batch 9140: train=0.750000 test=0.500000\n",
      "batch 9150: train=0.950000 test=0.700000\n",
      "batch 9160: train=0.900000 test=0.850000\n",
      "batch 9170: train=0.800000 test=0.700000\n",
      "batch 9180: train=0.700000 test=0.700000\n",
      "batch 9190: train=0.800000 test=0.400000\n",
      "batch 9200: train=0.800000 test=0.700000\n",
      "batch 9210: train=0.800000 test=0.700000\n",
      "batch 9220: train=0.750000 test=0.600000\n",
      "batch 9230: train=0.850000 test=0.750000\n",
      "batch 9240: train=0.800000 test=0.850000\n",
      "batch 9250: train=0.900000 test=0.700000\n",
      "batch 9260: train=0.700000 test=0.800000\n",
      "batch 9270: train=0.600000 test=0.700000\n",
      "batch 9280: train=0.750000 test=0.600000\n",
      "batch 9290: train=0.800000 test=0.950000\n",
      "batch 9300: train=0.900000 test=0.600000\n",
      "batch 9310: train=0.750000 test=0.650000\n",
      "batch 9320: train=0.700000 test=0.800000\n",
      "batch 9330: train=0.800000 test=0.800000\n",
      "batch 9340: train=0.850000 test=0.800000\n",
      "batch 9350: train=0.650000 test=0.600000\n",
      "batch 9360: train=0.600000 test=0.550000\n",
      "batch 9370: train=0.900000 test=0.750000\n",
      "batch 9380: train=0.750000 test=0.750000\n",
      "batch 9390: train=0.750000 test=0.800000\n",
      "batch 9400: train=0.900000 test=0.700000\n",
      "batch 9410: train=0.600000 test=0.700000\n",
      "batch 9420: train=0.700000 test=0.700000\n",
      "batch 9430: train=0.700000 test=0.900000\n",
      "batch 9440: train=0.700000 test=0.700000\n",
      "batch 9450: train=0.900000 test=0.850000\n",
      "batch 9460: train=0.800000 test=0.750000\n",
      "batch 9470: train=0.700000 test=0.750000\n",
      "batch 9480: train=0.900000 test=0.800000\n",
      "batch 9490: train=0.700000 test=0.550000\n",
      "batch 9500: train=0.800000 test=0.600000\n",
      "batch 9510: train=0.800000 test=0.750000\n",
      "batch 9520: train=0.900000 test=0.750000\n",
      "batch 9530: train=0.850000 test=0.750000\n",
      "batch 9540: train=0.750000 test=0.900000\n",
      "batch 9550: train=0.750000 test=0.650000\n",
      "batch 9560: train=0.700000 test=0.600000\n",
      "batch 9570: train=0.550000 test=0.700000\n",
      "batch 9580: train=0.650000 test=0.650000\n",
      "batch 9590: train=0.800000 test=0.700000\n",
      "batch 9600: train=0.850000 test=0.650000\n",
      "batch 9610: train=0.700000 test=0.700000\n",
      "batch 9620: train=0.750000 test=0.500000\n",
      "batch 9630: train=0.800000 test=0.650000\n",
      "batch 9640: train=0.800000 test=0.600000\n",
      "batch 9650: train=0.800000 test=0.800000\n",
      "batch 9660: train=0.850000 test=0.800000\n",
      "batch 9670: train=0.800000 test=0.700000\n",
      "batch 9680: train=0.800000 test=0.750000\n",
      "batch 9690: train=0.800000 test=0.800000\n",
      "batch 9700: train=0.850000 test=0.650000\n",
      "batch 9710: train=0.750000 test=0.700000\n",
      "batch 9720: train=0.800000 test=0.550000\n",
      "batch 9730: train=0.650000 test=0.600000\n",
      "batch 9740: train=0.800000 test=0.650000\n",
      "batch 9750: train=0.750000 test=0.700000\n",
      "batch 9760: train=0.800000 test=0.800000\n",
      "batch 9770: train=0.850000 test=0.800000\n",
      "batch 9780: train=0.750000 test=0.700000\n",
      "batch 9790: train=0.750000 test=0.450000\n",
      "batch 9800: train=0.800000 test=0.800000\n",
      "batch 9810: train=0.850000 test=0.700000\n",
      "batch 9820: train=0.750000 test=0.650000\n",
      "batch 9830: train=0.750000 test=0.700000\n",
      "batch 9840: train=0.900000 test=0.700000\n",
      "batch 9850: train=0.950000 test=0.500000\n",
      "batch 9860: train=0.700000 test=0.800000\n",
      "batch 9870: train=0.900000 test=0.550000\n",
      "batch 9880: train=0.800000 test=0.700000\n",
      "batch 9890: train=0.800000 test=0.800000\n",
      "batch 9900: train=0.600000 test=0.800000\n",
      "batch 9910: train=0.800000 test=0.650000\n",
      "batch 9920: train=0.800000 test=0.800000\n",
      "batch 9930: train=0.850000 test=0.600000\n",
      "batch 9940: train=0.900000 test=0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9950: train=0.850000 test=0.800000\n",
      "batch 9960: train=0.800000 test=0.750000\n",
      "batch 9970: train=0.750000 test=0.650000\n",
      "batch 9980: train=0.850000 test=0.850000\n",
      "batch 9990: train=0.800000 test=0.550000\n",
      "batch 10000: train=0.850000 test=0.800000\n",
      "batch 10010: train=0.900000 test=0.850000\n",
      "batch 10020: train=0.800000 test=0.750000\n",
      "batch 10030: train=0.950000 test=0.650000\n",
      "batch 10040: train=0.850000 test=0.800000\n",
      "batch 10050: train=0.800000 test=0.450000\n",
      "batch 10060: train=0.800000 test=0.750000\n",
      "batch 10070: train=0.950000 test=0.750000\n",
      "batch 10080: train=0.750000 test=0.450000\n",
      "batch 10090: train=0.850000 test=0.900000\n",
      "batch 10100: train=0.700000 test=0.750000\n",
      "batch 10110: train=0.850000 test=0.600000\n",
      "batch 10120: train=0.750000 test=0.600000\n",
      "batch 10130: train=0.750000 test=0.800000\n",
      "batch 10140: train=0.850000 test=0.600000\n",
      "batch 10150: train=0.950000 test=0.600000\n",
      "batch 10160: train=0.850000 test=0.800000\n",
      "batch 10170: train=0.800000 test=0.800000\n",
      "batch 10180: train=0.900000 test=0.450000\n",
      "batch 10190: train=0.750000 test=0.500000\n",
      "batch 10200: train=0.750000 test=0.650000\n",
      "batch 10210: train=0.800000 test=0.800000\n",
      "batch 10220: train=0.850000 test=0.800000\n",
      "batch 10230: train=0.800000 test=0.700000\n",
      "batch 10240: train=0.950000 test=0.600000\n",
      "batch 10250: train=0.700000 test=0.750000\n",
      "batch 10260: train=0.800000 test=0.650000\n",
      "batch 10270: train=0.850000 test=0.800000\n",
      "batch 10280: train=0.800000 test=0.950000\n",
      "batch 10290: train=0.800000 test=0.800000\n",
      "batch 10300: train=0.800000 test=0.500000\n",
      "batch 10310: train=0.750000 test=0.850000\n",
      "batch 10320: train=0.750000 test=0.800000\n",
      "batch 10330: train=0.900000 test=0.800000\n",
      "batch 10340: train=0.900000 test=0.850000\n",
      "batch 10350: train=0.800000 test=0.800000\n",
      "batch 10360: train=0.750000 test=0.650000\n",
      "batch 10370: train=0.650000 test=0.700000\n",
      "batch 10380: train=0.800000 test=0.750000\n",
      "batch 10390: train=0.800000 test=0.650000\n",
      "batch 10400: train=0.900000 test=0.650000\n",
      "batch 10410: train=0.850000 test=0.650000\n",
      "batch 10420: train=0.900000 test=0.700000\n",
      "batch 10430: train=0.700000 test=0.700000\n",
      "batch 10440: train=0.850000 test=0.700000\n",
      "batch 10450: train=0.700000 test=0.700000\n",
      "batch 10460: train=0.800000 test=0.900000\n",
      "batch 10470: train=0.700000 test=0.750000\n",
      "batch 10480: train=0.950000 test=0.850000\n",
      "batch 10490: train=0.750000 test=0.700000\n",
      "batch 10500: train=0.700000 test=0.700000\n",
      "batch 10510: train=0.800000 test=0.700000\n",
      "batch 10520: train=0.700000 test=0.850000\n",
      "batch 10530: train=0.800000 test=0.750000\n",
      "batch 10540: train=0.850000 test=0.600000\n",
      "batch 10550: train=0.800000 test=0.800000\n",
      "batch 10560: train=0.800000 test=0.550000\n",
      "batch 10570: train=0.700000 test=0.800000\n",
      "batch 10580: train=0.700000 test=0.600000\n",
      "batch 10590: train=0.800000 test=0.550000\n",
      "batch 10600: train=0.700000 test=0.850000\n",
      "batch 10610: train=0.900000 test=0.600000\n",
      "batch 10620: train=0.750000 test=0.700000\n",
      "batch 10630: train=0.800000 test=0.750000\n",
      "batch 10640: train=0.900000 test=0.700000\n",
      "batch 10650: train=0.950000 test=0.750000\n",
      "batch 10660: train=0.900000 test=0.750000\n",
      "batch 10670: train=0.700000 test=0.550000\n",
      "batch 10680: train=0.900000 test=0.800000\n",
      "batch 10690: train=0.900000 test=0.550000\n",
      "batch 10700: train=0.900000 test=0.800000\n",
      "batch 10710: train=0.850000 test=0.650000\n",
      "batch 10720: train=0.700000 test=0.750000\n",
      "batch 10730: train=0.800000 test=0.500000\n",
      "batch 10740: train=0.850000 test=0.800000\n",
      "batch 10750: train=0.750000 test=0.600000\n",
      "batch 10760: train=0.850000 test=0.800000\n",
      "batch 10770: train=0.800000 test=0.900000\n",
      "batch 10780: train=0.900000 test=0.700000\n",
      "batch 10790: train=0.900000 test=0.800000\n",
      "batch 10800: train=0.800000 test=0.850000\n",
      "batch 10810: train=0.600000 test=0.700000\n",
      "batch 10820: train=0.800000 test=0.550000\n",
      "batch 10830: train=0.850000 test=0.850000\n",
      "batch 10840: train=0.750000 test=0.600000\n",
      "batch 10850: train=0.900000 test=0.800000\n",
      "batch 10860: train=0.900000 test=0.800000\n",
      "batch 10870: train=0.850000 test=0.600000\n",
      "batch 10880: train=0.900000 test=0.850000\n",
      "batch 10890: train=0.900000 test=0.700000\n",
      "batch 10900: train=0.800000 test=0.750000\n",
      "batch 10910: train=0.900000 test=0.800000\n",
      "batch 10920: train=0.950000 test=0.650000\n",
      "batch 10930: train=0.850000 test=0.700000\n",
      "batch 10940: train=0.800000 test=0.650000\n",
      "batch 10950: train=0.850000 test=0.850000\n",
      "batch 10960: train=0.900000 test=0.800000\n",
      "batch 10970: train=0.850000 test=0.750000\n",
      "batch 10980: train=0.750000 test=0.700000\n",
      "batch 10990: train=0.950000 test=0.800000\n",
      "batch 11000: train=0.750000 test=0.750000\n",
      "batch 11010: train=0.800000 test=0.700000\n",
      "batch 11020: train=0.600000 test=0.750000\n",
      "batch 11030: train=0.700000 test=0.550000\n",
      "batch 11040: train=0.850000 test=0.850000\n",
      "batch 11050: train=0.600000 test=0.700000\n",
      "batch 11060: train=0.850000 test=0.550000\n",
      "batch 11070: train=0.700000 test=0.750000\n",
      "batch 11080: train=0.800000 test=0.800000\n",
      "batch 11090: train=0.800000 test=0.800000\n",
      "batch 11100: train=0.700000 test=0.800000\n",
      "batch 11110: train=0.800000 test=0.800000\n",
      "batch 11120: train=0.700000 test=0.600000\n",
      "batch 11130: train=0.750000 test=0.700000\n",
      "batch 11140: train=0.850000 test=0.500000\n",
      "batch 11150: train=0.700000 test=0.650000\n",
      "batch 11160: train=0.900000 test=0.700000\n",
      "batch 11170: train=0.700000 test=0.800000\n",
      "batch 11180: train=0.800000 test=0.800000\n",
      "batch 11190: train=0.800000 test=0.650000\n",
      "batch 11200: train=0.900000 test=0.700000\n",
      "batch 11210: train=0.800000 test=0.800000\n",
      "batch 11220: train=0.700000 test=0.600000\n",
      "batch 11230: train=0.750000 test=0.700000\n",
      "batch 11240: train=0.850000 test=0.700000\n",
      "batch 11250: train=0.750000 test=0.700000\n",
      "batch 11260: train=0.700000 test=0.600000\n",
      "batch 11270: train=0.750000 test=0.900000\n",
      "batch 11280: train=0.800000 test=0.800000\n",
      "batch 11290: train=0.800000 test=0.650000\n",
      "batch 11300: train=0.800000 test=0.700000\n",
      "batch 11310: train=0.700000 test=0.600000\n",
      "batch 11320: train=0.800000 test=0.550000\n",
      "batch 11330: train=0.650000 test=0.650000\n",
      "batch 11340: train=0.850000 test=0.750000\n",
      "batch 11350: train=0.750000 test=0.750000\n",
      "batch 11360: train=0.850000 test=0.650000\n",
      "batch 11370: train=0.800000 test=0.800000\n",
      "batch 11380: train=0.900000 test=0.700000\n",
      "batch 11390: train=0.650000 test=0.750000\n",
      "batch 11400: train=0.800000 test=0.700000\n",
      "batch 11410: train=0.800000 test=0.600000\n",
      "batch 11420: train=0.900000 test=0.650000\n",
      "batch 11430: train=0.700000 test=0.800000\n",
      "batch 11440: train=0.750000 test=0.700000\n",
      "batch 11450: train=0.850000 test=0.750000\n",
      "batch 11460: train=0.850000 test=0.600000\n",
      "batch 11470: train=0.850000 test=0.650000\n",
      "batch 11480: train=0.750000 test=0.700000\n",
      "batch 11490: train=0.750000 test=0.650000\n",
      "batch 11500: train=0.800000 test=0.600000\n",
      "batch 11510: train=0.900000 test=0.700000\n",
      "batch 11520: train=0.900000 test=0.800000\n",
      "batch 11530: train=0.900000 test=0.650000\n",
      "batch 11540: train=0.650000 test=0.700000\n",
      "batch 11550: train=0.650000 test=0.500000\n",
      "batch 11560: train=0.750000 test=0.750000\n",
      "batch 11570: train=0.850000 test=0.750000\n",
      "batch 11580: train=0.800000 test=0.750000\n",
      "batch 11590: train=0.850000 test=0.750000\n",
      "batch 11600: train=0.700000 test=0.750000\n",
      "batch 11610: train=0.750000 test=0.850000\n",
      "batch 11620: train=0.800000 test=0.700000\n",
      "batch 11630: train=0.850000 test=0.800000\n",
      "batch 11640: train=0.800000 test=0.750000\n",
      "batch 11650: train=0.750000 test=0.650000\n",
      "batch 11660: train=0.900000 test=0.650000\n",
      "batch 11670: train=0.850000 test=0.600000\n",
      "batch 11680: train=0.950000 test=0.800000\n",
      "batch 11690: train=0.800000 test=0.750000\n",
      "batch 11700: train=0.950000 test=0.600000\n",
      "batch 11710: train=0.700000 test=0.650000\n",
      "batch 11720: train=0.650000 test=0.600000\n",
      "batch 11730: train=0.700000 test=0.800000\n",
      "batch 11740: train=0.800000 test=0.700000\n",
      "batch 11750: train=0.900000 test=0.700000\n",
      "batch 11760: train=0.800000 test=0.850000\n",
      "batch 11770: train=0.950000 test=0.700000\n",
      "batch 11780: train=0.900000 test=0.700000\n",
      "batch 11790: train=0.850000 test=0.700000\n",
      "batch 11800: train=0.900000 test=0.800000\n",
      "batch 11810: train=0.850000 test=0.400000\n",
      "batch 11820: train=0.900000 test=0.700000\n",
      "batch 11830: train=0.700000 test=0.600000\n",
      "batch 11840: train=0.750000 test=0.800000\n",
      "batch 11850: train=0.900000 test=0.650000\n",
      "batch 11860: train=0.750000 test=0.700000\n",
      "batch 11870: train=0.800000 test=0.850000\n",
      "batch 11880: train=0.750000 test=0.800000\n",
      "batch 11890: train=0.800000 test=0.700000\n",
      "batch 11900: train=0.800000 test=0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 11910: train=0.800000 test=0.750000\n",
      "batch 11920: train=1.000000 test=0.750000\n",
      "batch 11930: train=0.750000 test=0.850000\n",
      "batch 11940: train=0.750000 test=0.850000\n",
      "batch 11950: train=0.900000 test=0.650000\n",
      "batch 11960: train=0.800000 test=0.800000\n",
      "batch 11970: train=0.750000 test=0.850000\n",
      "batch 11980: train=0.800000 test=0.750000\n",
      "batch 11990: train=0.750000 test=0.800000\n",
      "batch 12000: train=0.900000 test=0.700000\n",
      "batch 12010: train=0.700000 test=0.800000\n",
      "batch 12020: train=0.850000 test=0.650000\n",
      "batch 12030: train=0.800000 test=0.900000\n",
      "batch 12040: train=0.750000 test=0.750000\n",
      "batch 12050: train=0.750000 test=0.600000\n",
      "batch 12060: train=0.800000 test=0.700000\n",
      "batch 12070: train=0.900000 test=0.600000\n",
      "batch 12080: train=0.800000 test=0.650000\n",
      "batch 12090: train=0.900000 test=0.550000\n",
      "batch 12100: train=0.900000 test=0.650000\n",
      "batch 12110: train=0.850000 test=0.700000\n",
      "batch 12120: train=0.800000 test=0.650000\n",
      "batch 12130: train=0.950000 test=0.650000\n",
      "batch 12140: train=0.850000 test=0.850000\n",
      "batch 12150: train=0.850000 test=0.550000\n",
      "batch 12160: train=0.900000 test=0.650000\n",
      "batch 12170: train=0.850000 test=0.550000\n",
      "batch 12180: train=0.950000 test=0.950000\n",
      "batch 12190: train=0.900000 test=0.850000\n",
      "batch 12200: train=0.800000 test=0.850000\n",
      "batch 12210: train=0.900000 test=0.700000\n",
      "batch 12220: train=0.850000 test=0.600000\n",
      "batch 12230: train=0.800000 test=0.750000\n",
      "batch 12240: train=0.800000 test=0.750000\n",
      "batch 12250: train=0.800000 test=0.750000\n",
      "batch 12260: train=0.800000 test=0.700000\n",
      "batch 12270: train=0.950000 test=0.600000\n",
      "batch 12280: train=0.850000 test=0.650000\n",
      "batch 12290: train=0.750000 test=0.550000\n",
      "batch 12300: train=0.800000 test=0.800000\n",
      "batch 12310: train=0.900000 test=0.700000\n",
      "batch 12320: train=0.750000 test=0.400000\n",
      "batch 12330: train=0.900000 test=0.750000\n",
      "batch 12340: train=0.900000 test=0.550000\n",
      "batch 12350: train=0.800000 test=0.800000\n",
      "batch 12360: train=0.750000 test=0.850000\n",
      "batch 12370: train=0.950000 test=0.750000\n",
      "batch 12380: train=0.900000 test=0.750000\n",
      "batch 12390: train=0.750000 test=0.650000\n",
      "batch 12400: train=0.800000 test=0.550000\n",
      "batch 12410: train=0.800000 test=0.700000\n",
      "batch 12420: train=0.750000 test=0.750000\n",
      "batch 12430: train=0.800000 test=0.500000\n",
      "batch 12440: train=0.950000 test=0.800000\n",
      "batch 12450: train=0.900000 test=0.600000\n",
      "batch 12460: train=0.800000 test=0.600000\n",
      "batch 12470: train=0.750000 test=0.650000\n",
      "batch 12480: train=0.900000 test=0.700000\n",
      "batch 12490: train=0.600000 test=0.650000\n",
      "batch 12500: train=0.750000 test=0.600000\n",
      "batch 12510: train=0.800000 test=0.600000\n",
      "batch 12520: train=0.850000 test=0.700000\n",
      "batch 12530: train=0.900000 test=0.800000\n",
      "batch 12540: train=0.750000 test=0.750000\n",
      "batch 12550: train=0.850000 test=0.800000\n",
      "batch 12560: train=0.700000 test=0.850000\n",
      "batch 12570: train=0.850000 test=0.850000\n",
      "batch 12580: train=0.800000 test=0.600000\n",
      "batch 12590: train=0.600000 test=0.750000\n",
      "batch 12600: train=0.900000 test=0.900000\n",
      "batch 12610: train=0.800000 test=0.650000\n",
      "batch 12620: train=0.700000 test=0.700000\n",
      "batch 12630: train=0.800000 test=0.650000\n",
      "batch 12640: train=0.850000 test=0.550000\n",
      "batch 12650: train=0.850000 test=0.900000\n",
      "batch 12660: train=0.700000 test=0.700000\n",
      "batch 12670: train=0.900000 test=0.800000\n",
      "batch 12680: train=0.800000 test=0.650000\n",
      "batch 12690: train=0.800000 test=0.750000\n",
      "batch 12700: train=0.850000 test=0.600000\n",
      "batch 12710: train=0.900000 test=0.850000\n",
      "batch 12720: train=0.900000 test=0.850000\n",
      "batch 12730: train=0.800000 test=0.700000\n",
      "batch 12740: train=0.850000 test=0.600000\n",
      "batch 12750: train=0.800000 test=0.750000\n",
      "batch 12760: train=0.900000 test=0.650000\n",
      "batch 12770: train=0.850000 test=0.850000\n",
      "batch 12780: train=0.800000 test=0.650000\n",
      "batch 12790: train=0.750000 test=0.750000\n",
      "batch 12800: train=0.800000 test=0.950000\n",
      "batch 12810: train=0.800000 test=0.850000\n",
      "batch 12820: train=0.800000 test=0.650000\n",
      "batch 12830: train=0.800000 test=0.600000\n",
      "batch 12840: train=0.750000 test=0.600000\n",
      "batch 12850: train=0.750000 test=0.750000\n",
      "batch 12860: train=0.650000 test=0.750000\n",
      "batch 12870: train=0.850000 test=0.750000\n",
      "batch 12880: train=0.800000 test=0.700000\n",
      "batch 12890: train=0.800000 test=0.600000\n",
      "batch 12900: train=0.800000 test=0.700000\n",
      "batch 12910: train=0.750000 test=0.700000\n",
      "batch 12920: train=0.950000 test=0.650000\n",
      "batch 12930: train=0.800000 test=0.750000\n",
      "batch 12940: train=0.850000 test=0.600000\n",
      "batch 12950: train=0.700000 test=0.700000\n",
      "batch 12960: train=0.800000 test=0.800000\n",
      "batch 12970: train=0.950000 test=0.750000\n",
      "batch 12980: train=0.750000 test=0.600000\n",
      "batch 12990: train=0.800000 test=0.850000\n",
      "batch 13000: train=0.750000 test=0.650000\n",
      "batch 13010: train=0.750000 test=0.800000\n",
      "batch 13020: train=0.850000 test=0.800000\n",
      "batch 13030: train=0.650000 test=0.550000\n",
      "batch 13040: train=0.850000 test=0.550000\n",
      "batch 13050: train=0.750000 test=0.800000\n",
      "batch 13060: train=0.650000 test=0.700000\n",
      "batch 13070: train=0.850000 test=0.600000\n",
      "batch 13080: train=0.800000 test=0.650000\n",
      "batch 13090: train=0.850000 test=0.750000\n",
      "batch 13100: train=0.850000 test=0.850000\n",
      "batch 13110: train=0.900000 test=0.650000\n",
      "batch 13120: train=0.700000 test=0.600000\n",
      "batch 13130: train=0.750000 test=0.850000\n",
      "batch 13140: train=0.700000 test=0.800000\n",
      "batch 13150: train=0.750000 test=0.700000\n",
      "batch 13160: train=0.750000 test=0.700000\n",
      "batch 13170: train=0.750000 test=0.900000\n",
      "batch 13180: train=0.850000 test=0.800000\n",
      "batch 13190: train=1.000000 test=0.700000\n",
      "batch 13200: train=0.800000 test=0.750000\n",
      "batch 13210: train=0.700000 test=0.700000\n",
      "batch 13220: train=0.750000 test=0.650000\n",
      "batch 13230: train=0.850000 test=0.750000\n",
      "batch 13240: train=0.950000 test=0.600000\n",
      "batch 13250: train=0.700000 test=0.850000\n",
      "batch 13260: train=0.800000 test=0.600000\n",
      "batch 13270: train=0.700000 test=0.650000\n",
      "batch 13280: train=0.850000 test=0.700000\n",
      "batch 13290: train=0.900000 test=0.600000\n",
      "batch 13300: train=0.900000 test=0.700000\n",
      "batch 13310: train=0.850000 test=0.800000\n",
      "batch 13320: train=0.900000 test=0.600000\n",
      "batch 13330: train=0.950000 test=0.650000\n",
      "batch 13340: train=0.850000 test=0.800000\n",
      "batch 13350: train=0.850000 test=0.850000\n",
      "batch 13360: train=0.950000 test=0.850000\n",
      "batch 13370: train=0.800000 test=0.700000\n",
      "batch 13380: train=0.850000 test=0.700000\n",
      "batch 13390: train=0.850000 test=0.750000\n",
      "batch 13400: train=0.950000 test=0.700000\n",
      "batch 13410: train=0.900000 test=0.700000\n",
      "batch 13420: train=0.800000 test=0.700000\n",
      "batch 13430: train=0.650000 test=0.800000\n",
      "batch 13440: train=0.750000 test=0.850000\n",
      "batch 13450: train=0.800000 test=0.750000\n",
      "batch 13460: train=0.900000 test=0.700000\n",
      "batch 13470: train=0.800000 test=0.700000\n",
      "batch 13480: train=0.950000 test=0.950000\n",
      "batch 13490: train=0.850000 test=0.650000\n",
      "batch 13500: train=0.850000 test=0.850000\n",
      "batch 13510: train=0.850000 test=0.700000\n",
      "batch 13520: train=0.800000 test=0.650000\n",
      "batch 13530: train=0.750000 test=0.850000\n",
      "batch 13540: train=0.850000 test=0.650000\n",
      "batch 13550: train=0.650000 test=0.900000\n",
      "batch 13560: train=0.850000 test=0.550000\n",
      "batch 13570: train=0.900000 test=0.900000\n",
      "batch 13580: train=0.950000 test=0.600000\n",
      "batch 13590: train=0.800000 test=0.850000\n",
      "batch 13600: train=0.800000 test=0.800000\n",
      "batch 13610: train=0.950000 test=0.600000\n",
      "batch 13620: train=0.750000 test=0.600000\n",
      "batch 13630: train=0.850000 test=0.700000\n",
      "batch 13640: train=0.850000 test=0.850000\n",
      "batch 13650: train=0.900000 test=0.850000\n",
      "batch 13660: train=0.850000 test=0.700000\n",
      "batch 13670: train=0.900000 test=0.800000\n",
      "batch 13680: train=0.800000 test=0.700000\n",
      "batch 13690: train=0.850000 test=0.900000\n",
      "batch 13700: train=0.900000 test=0.800000\n",
      "batch 13710: train=0.800000 test=0.800000\n",
      "batch 13720: train=0.900000 test=0.800000\n",
      "batch 13730: train=0.900000 test=0.800000\n",
      "batch 13740: train=0.800000 test=0.550000\n",
      "batch 13750: train=0.900000 test=0.600000\n",
      "batch 13760: train=0.800000 test=0.700000\n",
      "batch 13770: train=0.800000 test=0.850000\n",
      "batch 13780: train=0.750000 test=0.800000\n",
      "batch 13790: train=0.850000 test=0.850000\n",
      "batch 13800: train=0.900000 test=0.750000\n",
      "batch 13810: train=0.850000 test=0.700000\n",
      "batch 13820: train=1.000000 test=0.700000\n",
      "batch 13830: train=0.900000 test=0.700000\n",
      "batch 13840: train=0.850000 test=0.750000\n",
      "batch 13850: train=0.900000 test=0.550000\n",
      "batch 13860: train=0.900000 test=0.700000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 13870: train=0.850000 test=0.800000\n",
      "batch 13880: train=0.750000 test=0.850000\n",
      "batch 13890: train=0.850000 test=0.750000\n",
      "batch 13900: train=0.650000 test=0.800000\n",
      "batch 13910: train=0.850000 test=0.850000\n",
      "batch 13920: train=0.700000 test=0.800000\n",
      "batch 13930: train=0.800000 test=0.650000\n",
      "batch 13940: train=0.800000 test=0.750000\n",
      "batch 13950: train=0.950000 test=0.900000\n",
      "batch 13960: train=0.700000 test=0.750000\n",
      "batch 13970: train=0.800000 test=0.750000\n",
      "batch 13980: train=0.700000 test=0.650000\n",
      "batch 13990: train=0.950000 test=0.750000\n",
      "batch 14000: train=0.800000 test=0.650000\n",
      "batch 14010: train=0.850000 test=0.750000\n",
      "batch 14020: train=0.950000 test=0.900000\n",
      "batch 14030: train=0.800000 test=0.700000\n",
      "batch 14040: train=0.650000 test=0.800000\n",
      "batch 14050: train=0.900000 test=0.550000\n",
      "batch 14060: train=0.850000 test=0.900000\n",
      "batch 14070: train=0.850000 test=0.550000\n",
      "batch 14080: train=0.850000 test=0.700000\n",
      "batch 14090: train=0.800000 test=0.700000\n",
      "batch 14100: train=0.800000 test=0.750000\n",
      "batch 14110: train=0.850000 test=0.700000\n",
      "batch 14120: train=0.900000 test=0.850000\n",
      "batch 14130: train=0.800000 test=0.800000\n",
      "batch 14140: train=0.900000 test=0.750000\n",
      "batch 14150: train=0.850000 test=0.550000\n",
      "batch 14160: train=0.700000 test=0.850000\n",
      "batch 14170: train=0.600000 test=0.600000\n",
      "batch 14180: train=0.850000 test=0.750000\n",
      "batch 14190: train=0.900000 test=0.900000\n",
      "batch 14200: train=0.850000 test=0.850000\n",
      "batch 14210: train=0.850000 test=0.750000\n",
      "batch 14220: train=0.900000 test=0.850000\n",
      "batch 14230: train=0.750000 test=0.800000\n",
      "batch 14240: train=0.850000 test=0.700000\n",
      "batch 14250: train=0.750000 test=0.800000\n",
      "batch 14260: train=0.850000 test=0.700000\n",
      "batch 14270: train=0.850000 test=0.900000\n",
      "batch 14280: train=0.850000 test=0.600000\n",
      "batch 14290: train=0.800000 test=0.550000\n",
      "batch 14300: train=0.950000 test=0.800000\n",
      "batch 14310: train=0.900000 test=0.750000\n",
      "batch 14320: train=0.950000 test=0.700000\n",
      "batch 14330: train=0.800000 test=0.850000\n",
      "batch 14340: train=0.950000 test=0.700000\n",
      "batch 14350: train=0.800000 test=0.800000\n",
      "batch 14360: train=0.850000 test=0.900000\n",
      "batch 14370: train=0.700000 test=0.600000\n",
      "batch 14380: train=0.800000 test=0.800000\n",
      "batch 14390: train=0.750000 test=0.700000\n",
      "batch 14400: train=0.950000 test=0.850000\n",
      "batch 14410: train=0.850000 test=0.800000\n",
      "batch 14420: train=0.900000 test=0.800000\n",
      "batch 14430: train=0.750000 test=0.650000\n",
      "batch 14440: train=0.950000 test=0.800000\n",
      "batch 14450: train=0.700000 test=0.600000\n",
      "batch 14460: train=0.800000 test=0.750000\n",
      "batch 14470: train=0.750000 test=0.750000\n",
      "batch 14480: train=0.800000 test=0.750000\n",
      "batch 14490: train=0.900000 test=0.700000\n",
      "batch 14500: train=0.900000 test=0.850000\n",
      "batch 14510: train=0.800000 test=0.700000\n",
      "batch 14520: train=0.950000 test=0.800000\n",
      "batch 14530: train=0.750000 test=0.700000\n",
      "batch 14540: train=0.750000 test=0.800000\n",
      "batch 14550: train=0.950000 test=0.700000\n",
      "batch 14560: train=0.850000 test=0.750000\n",
      "batch 14570: train=0.650000 test=0.750000\n",
      "batch 14580: train=0.950000 test=0.750000\n",
      "batch 14590: train=0.900000 test=0.600000\n",
      "batch 14600: train=0.850000 test=0.750000\n",
      "batch 14610: train=0.850000 test=0.700000\n",
      "batch 14620: train=0.750000 test=0.750000\n",
      "batch 14630: train=0.800000 test=0.550000\n",
      "batch 14640: train=0.850000 test=0.750000\n",
      "batch 14650: train=0.950000 test=0.750000\n",
      "batch 14660: train=0.800000 test=0.750000\n",
      "batch 14670: train=0.650000 test=0.650000\n",
      "batch 14680: train=0.850000 test=0.800000\n",
      "batch 14690: train=0.950000 test=0.650000\n",
      "batch 14700: train=0.800000 test=0.600000\n",
      "batch 14710: train=0.850000 test=0.750000\n",
      "batch 14720: train=0.900000 test=0.600000\n",
      "batch 14730: train=0.800000 test=0.850000\n",
      "batch 14740: train=0.900000 test=0.650000\n",
      "batch 14750: train=0.950000 test=0.850000\n",
      "batch 14760: train=0.700000 test=0.800000\n",
      "batch 14770: train=0.900000 test=0.700000\n",
      "batch 14780: train=0.850000 test=0.900000\n",
      "batch 14790: train=0.800000 test=0.800000\n",
      "batch 14800: train=0.700000 test=0.850000\n",
      "batch 14810: train=0.800000 test=0.700000\n",
      "batch 14820: train=0.750000 test=0.850000\n",
      "batch 14830: train=0.800000 test=0.750000\n",
      "batch 14840: train=0.850000 test=0.750000\n",
      "batch 14850: train=0.700000 test=0.800000\n",
      "batch 14860: train=0.800000 test=0.800000\n",
      "batch 14870: train=1.000000 test=0.800000\n",
      "batch 14880: train=0.800000 test=0.900000\n",
      "batch 14890: train=0.850000 test=0.700000\n",
      "batch 14900: train=0.900000 test=0.700000\n",
      "batch 14910: train=0.850000 test=0.800000\n",
      "batch 14920: train=0.950000 test=0.700000\n",
      "batch 14930: train=0.950000 test=0.700000\n",
      "batch 14940: train=1.000000 test=0.950000\n",
      "batch 14950: train=0.600000 test=0.850000\n",
      "batch 14960: train=0.900000 test=0.900000\n",
      "batch 14970: train=0.900000 test=0.800000\n",
      "batch 14980: train=0.800000 test=0.800000\n",
      "batch 14990: train=0.900000 test=0.800000\n",
      "batch 15000: train=0.950000 test=0.700000\n",
      "batch 15010: train=0.850000 test=0.750000\n",
      "batch 15020: train=0.850000 test=0.750000\n",
      "batch 15030: train=0.750000 test=0.950000\n",
      "batch 15040: train=0.900000 test=0.600000\n",
      "batch 15050: train=0.900000 test=0.800000\n",
      "batch 15060: train=0.800000 test=0.650000\n",
      "batch 15070: train=0.900000 test=0.700000\n",
      "batch 15080: train=0.800000 test=0.650000\n",
      "batch 15090: train=0.950000 test=0.750000\n",
      "batch 15100: train=0.850000 test=0.700000\n",
      "batch 15110: train=0.750000 test=0.700000\n",
      "batch 15120: train=0.950000 test=0.850000\n",
      "batch 15130: train=0.850000 test=0.700000\n",
      "batch 15140: train=0.850000 test=0.600000\n",
      "batch 15150: train=0.750000 test=0.700000\n",
      "batch 15160: train=0.850000 test=0.700000\n",
      "batch 15170: train=0.850000 test=0.850000\n",
      "batch 15180: train=0.850000 test=0.650000\n",
      "batch 15190: train=0.750000 test=0.650000\n",
      "batch 15200: train=0.950000 test=0.600000\n",
      "batch 15210: train=0.900000 test=0.800000\n",
      "batch 15220: train=0.700000 test=0.950000\n",
      "batch 15230: train=0.950000 test=0.650000\n",
      "batch 15240: train=0.850000 test=0.700000\n",
      "batch 15250: train=0.800000 test=0.600000\n",
      "batch 15260: train=0.900000 test=0.700000\n",
      "batch 15270: train=0.800000 test=0.750000\n",
      "batch 15280: train=0.850000 test=0.800000\n",
      "batch 15290: train=0.750000 test=0.900000\n",
      "batch 15300: train=0.850000 test=0.750000\n",
      "batch 15310: train=0.850000 test=0.850000\n",
      "batch 15320: train=0.800000 test=0.850000\n",
      "batch 15330: train=0.800000 test=0.750000\n",
      "batch 15340: train=0.750000 test=0.700000\n",
      "batch 15350: train=0.800000 test=0.800000\n",
      "batch 15360: train=0.850000 test=0.650000\n",
      "batch 15370: train=0.800000 test=0.750000\n",
      "batch 15380: train=0.850000 test=0.900000\n",
      "batch 15390: train=0.900000 test=0.800000\n",
      "batch 15400: train=0.950000 test=0.750000\n",
      "batch 15410: train=0.900000 test=0.900000\n",
      "batch 15420: train=0.900000 test=0.750000\n",
      "batch 15430: train=0.750000 test=0.900000\n",
      "batch 15440: train=0.700000 test=0.650000\n",
      "batch 15450: train=0.800000 test=0.800000\n",
      "batch 15460: train=0.900000 test=0.650000\n",
      "batch 15470: train=0.800000 test=0.450000\n",
      "batch 15480: train=0.900000 test=0.750000\n",
      "batch 15490: train=0.650000 test=0.800000\n",
      "batch 15500: train=0.850000 test=0.700000\n",
      "batch 15510: train=0.700000 test=0.850000\n",
      "batch 15520: train=0.900000 test=0.850000\n",
      "batch 15530: train=0.850000 test=0.550000\n",
      "batch 15540: train=0.800000 test=0.800000\n",
      "batch 15550: train=0.850000 test=0.700000\n",
      "batch 15560: train=0.800000 test=0.700000\n",
      "batch 15570: train=0.750000 test=0.950000\n",
      "batch 15580: train=0.900000 test=0.800000\n",
      "batch 15590: train=0.900000 test=0.700000\n",
      "batch 15600: train=0.800000 test=0.750000\n",
      "batch 15610: train=0.850000 test=0.800000\n",
      "batch 15620: train=0.900000 test=0.800000\n",
      "batch 15630: train=0.750000 test=0.550000\n",
      "batch 15640: train=0.800000 test=0.600000\n",
      "batch 15650: train=0.750000 test=0.700000\n",
      "batch 15660: train=0.800000 test=0.700000\n",
      "batch 15670: train=0.950000 test=0.850000\n",
      "batch 15680: train=0.850000 test=0.750000\n",
      "batch 15690: train=0.800000 test=0.600000\n",
      "batch 15700: train=0.750000 test=0.800000\n",
      "batch 15710: train=0.850000 test=0.900000\n",
      "batch 15720: train=0.750000 test=0.900000\n",
      "batch 15730: train=0.800000 test=0.800000\n",
      "batch 15740: train=0.750000 test=0.700000\n",
      "batch 15750: train=0.800000 test=0.750000\n",
      "batch 15760: train=0.800000 test=0.750000\n",
      "batch 15770: train=0.800000 test=0.750000\n",
      "batch 15780: train=0.800000 test=0.900000\n",
      "batch 15790: train=0.800000 test=0.800000\n",
      "batch 15800: train=0.800000 test=0.800000\n",
      "batch 15810: train=0.850000 test=0.800000\n",
      "batch 15820: train=0.700000 test=0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 15830: train=0.750000 test=0.750000\n",
      "batch 15840: train=0.950000 test=0.550000\n",
      "batch 15850: train=0.800000 test=0.600000\n",
      "batch 15860: train=0.800000 test=0.800000\n",
      "batch 15870: train=0.750000 test=0.650000\n",
      "batch 15880: train=1.000000 test=0.800000\n",
      "batch 15890: train=0.850000 test=0.750000\n",
      "batch 15900: train=0.750000 test=0.750000\n",
      "batch 15910: train=0.900000 test=0.800000\n",
      "batch 15920: train=0.800000 test=0.700000\n",
      "batch 15930: train=0.900000 test=0.750000\n",
      "batch 15940: train=0.650000 test=0.850000\n",
      "batch 15950: train=0.850000 test=0.750000\n",
      "batch 15960: train=0.750000 test=0.800000\n",
      "batch 15970: train=0.900000 test=0.650000\n",
      "batch 15980: train=0.800000 test=0.750000\n",
      "batch 15990: train=0.850000 test=0.800000\n",
      "batch 16000: train=0.950000 test=0.800000\n",
      "batch 16010: train=0.850000 test=0.700000\n",
      "batch 16020: train=1.000000 test=0.850000\n",
      "batch 16030: train=0.800000 test=0.750000\n",
      "batch 16040: train=0.850000 test=0.600000\n",
      "batch 16050: train=0.750000 test=0.400000\n",
      "batch 16060: train=0.850000 test=0.800000\n",
      "batch 16070: train=0.750000 test=0.650000\n",
      "batch 16080: train=0.850000 test=0.800000\n",
      "batch 16090: train=0.850000 test=0.700000\n",
      "batch 16100: train=0.900000 test=0.800000\n",
      "batch 16110: train=0.900000 test=0.700000\n",
      "batch 16120: train=0.850000 test=0.750000\n",
      "batch 16130: train=0.800000 test=0.750000\n",
      "batch 16140: train=0.900000 test=0.700000\n",
      "batch 16150: train=0.750000 test=1.000000\n",
      "batch 16160: train=0.700000 test=0.800000\n",
      "batch 16170: train=0.850000 test=0.900000\n",
      "batch 16180: train=0.800000 test=0.950000\n",
      "batch 16190: train=0.900000 test=0.700000\n",
      "batch 16200: train=0.700000 test=0.750000\n",
      "batch 16210: train=0.800000 test=0.850000\n",
      "batch 16220: train=0.750000 test=0.700000\n",
      "batch 16230: train=0.950000 test=0.650000\n",
      "batch 16240: train=0.900000 test=0.750000\n",
      "batch 16250: train=0.700000 test=0.750000\n",
      "batch 16260: train=0.900000 test=0.700000\n",
      "batch 16270: train=0.800000 test=0.800000\n",
      "batch 16280: train=0.950000 test=0.750000\n",
      "batch 16290: train=0.850000 test=0.650000\n",
      "batch 16300: train=0.600000 test=0.750000\n",
      "batch 16310: train=0.950000 test=0.750000\n",
      "batch 16320: train=0.800000 test=0.800000\n",
      "batch 16330: train=0.950000 test=0.800000\n",
      "batch 16340: train=0.900000 test=0.750000\n",
      "batch 16350: train=0.850000 test=0.650000\n",
      "batch 16360: train=0.900000 test=0.800000\n",
      "batch 16370: train=0.850000 test=0.750000\n",
      "batch 16380: train=0.950000 test=0.850000\n",
      "batch 16390: train=0.850000 test=0.700000\n",
      "batch 16400: train=0.750000 test=0.900000\n",
      "batch 16410: train=0.850000 test=0.750000\n",
      "batch 16420: train=0.900000 test=0.700000\n",
      "batch 16430: train=0.800000 test=0.800000\n",
      "batch 16440: train=0.750000 test=0.750000\n",
      "batch 16450: train=0.900000 test=0.550000\n",
      "batch 16460: train=0.850000 test=0.650000\n",
      "batch 16470: train=0.950000 test=0.700000\n",
      "batch 16480: train=0.950000 test=0.750000\n",
      "batch 16490: train=0.750000 test=0.850000\n",
      "batch 16500: train=0.850000 test=0.800000\n",
      "batch 16510: train=0.800000 test=0.950000\n",
      "batch 16520: train=0.850000 test=0.900000\n",
      "batch 16530: train=1.000000 test=0.650000\n",
      "batch 16540: train=0.800000 test=0.850000\n",
      "batch 16550: train=0.950000 test=0.850000\n",
      "batch 16560: train=0.700000 test=0.900000\n",
      "batch 16570: train=0.850000 test=0.800000\n",
      "batch 16580: train=0.650000 test=1.000000\n",
      "batch 16590: train=0.800000 test=0.800000\n",
      "batch 16600: train=0.900000 test=0.700000\n",
      "batch 16610: train=0.900000 test=0.700000\n",
      "batch 16620: train=0.950000 test=0.900000\n",
      "batch 16630: train=0.800000 test=0.900000\n",
      "batch 16640: train=0.900000 test=0.650000\n",
      "batch 16650: train=0.800000 test=0.900000\n",
      "batch 16660: train=0.800000 test=0.700000\n",
      "batch 16670: train=0.850000 test=0.700000\n",
      "batch 16680: train=0.950000 test=0.750000\n",
      "batch 16690: train=0.900000 test=0.700000\n",
      "batch 16700: train=0.800000 test=0.650000\n",
      "batch 16710: train=0.900000 test=0.750000\n",
      "batch 16720: train=0.650000 test=0.450000\n",
      "batch 16730: train=0.900000 test=0.700000\n",
      "batch 16740: train=0.900000 test=0.600000\n",
      "batch 16750: train=0.850000 test=0.950000\n",
      "batch 16760: train=0.850000 test=0.700000\n",
      "batch 16770: train=0.900000 test=0.800000\n",
      "batch 16780: train=0.850000 test=0.650000\n",
      "batch 16790: train=0.850000 test=0.700000\n",
      "batch 16800: train=0.850000 test=0.600000\n",
      "batch 16810: train=0.750000 test=0.850000\n",
      "batch 16820: train=0.800000 test=0.750000\n",
      "batch 16830: train=0.900000 test=0.750000\n",
      "batch 16840: train=0.650000 test=0.700000\n",
      "batch 16850: train=0.900000 test=0.800000\n",
      "batch 16860: train=0.800000 test=0.850000\n",
      "batch 16870: train=0.950000 test=0.850000\n",
      "batch 16880: train=0.750000 test=0.900000\n",
      "batch 16890: train=0.950000 test=0.900000\n",
      "batch 16900: train=0.800000 test=0.700000\n",
      "batch 16910: train=0.750000 test=0.650000\n",
      "batch 16920: train=0.900000 test=0.800000\n",
      "batch 16930: train=0.900000 test=0.750000\n",
      "batch 16940: train=0.850000 test=0.750000\n",
      "batch 16950: train=0.850000 test=0.650000\n",
      "batch 16960: train=0.850000 test=0.750000\n",
      "batch 16970: train=0.800000 test=0.800000\n",
      "batch 16980: train=0.800000 test=0.850000\n",
      "batch 16990: train=0.700000 test=0.800000\n",
      "batch 17000: train=0.850000 test=0.700000\n",
      "batch 17010: train=0.800000 test=0.700000\n",
      "batch 17020: train=0.700000 test=0.750000\n",
      "batch 17030: train=0.700000 test=0.850000\n",
      "batch 17040: train=0.900000 test=0.750000\n",
      "batch 17050: train=0.850000 test=0.800000\n",
      "batch 17060: train=0.900000 test=0.800000\n",
      "batch 17070: train=0.900000 test=0.750000\n",
      "batch 17080: train=0.850000 test=0.700000\n",
      "batch 17090: train=0.900000 test=0.800000\n",
      "batch 17100: train=0.850000 test=0.650000\n",
      "batch 17110: train=0.750000 test=0.800000\n",
      "batch 17120: train=0.900000 test=0.850000\n",
      "batch 17130: train=0.800000 test=0.650000\n",
      "batch 17140: train=0.950000 test=0.800000\n",
      "batch 17150: train=0.900000 test=0.800000\n",
      "batch 17160: train=0.800000 test=0.850000\n",
      "batch 17170: train=0.800000 test=0.700000\n",
      "batch 17180: train=0.750000 test=0.850000\n",
      "batch 17190: train=0.950000 test=0.750000\n",
      "batch 17200: train=1.000000 test=0.650000\n",
      "batch 17210: train=0.800000 test=0.750000\n",
      "batch 17220: train=1.000000 test=0.700000\n",
      "batch 17230: train=0.900000 test=0.750000\n",
      "batch 17240: train=0.700000 test=0.700000\n",
      "batch 17250: train=0.900000 test=0.750000\n",
      "batch 17260: train=0.850000 test=0.750000\n",
      "batch 17270: train=0.750000 test=0.850000\n",
      "batch 17280: train=0.900000 test=0.800000\n",
      "batch 17290: train=0.900000 test=0.900000\n",
      "batch 17300: train=0.750000 test=0.900000\n",
      "batch 17310: train=0.900000 test=0.850000\n",
      "batch 17320: train=0.850000 test=0.750000\n",
      "batch 17330: train=0.800000 test=1.000000\n",
      "batch 17340: train=0.800000 test=0.750000\n",
      "batch 17350: train=0.850000 test=0.850000\n",
      "batch 17360: train=0.800000 test=0.700000\n",
      "batch 17370: train=0.800000 test=0.750000\n",
      "batch 17380: train=0.900000 test=0.650000\n",
      "batch 17390: train=0.900000 test=0.750000\n",
      "batch 17400: train=0.850000 test=0.850000\n",
      "batch 17410: train=0.850000 test=0.750000\n",
      "batch 17420: train=0.900000 test=0.750000\n",
      "batch 17430: train=0.900000 test=0.850000\n",
      "batch 17440: train=0.900000 test=0.750000\n",
      "batch 17450: train=0.900000 test=0.800000\n",
      "batch 17460: train=0.950000 test=0.800000\n",
      "batch 17470: train=0.800000 test=0.700000\n",
      "batch 17480: train=0.900000 test=0.850000\n",
      "batch 17490: train=0.900000 test=0.900000\n",
      "batch 17500: train=0.750000 test=0.650000\n",
      "batch 17510: train=0.950000 test=0.800000\n",
      "batch 17520: train=0.750000 test=0.950000\n",
      "batch 17530: train=0.950000 test=0.800000\n",
      "batch 17540: train=0.900000 test=0.750000\n",
      "batch 17550: train=0.850000 test=0.800000\n",
      "batch 17560: train=0.750000 test=0.800000\n",
      "batch 17570: train=0.800000 test=0.650000\n",
      "batch 17580: train=0.950000 test=0.800000\n",
      "batch 17590: train=0.750000 test=0.650000\n",
      "batch 17600: train=0.850000 test=0.850000\n",
      "batch 17610: train=0.950000 test=0.700000\n",
      "batch 17620: train=0.800000 test=0.650000\n",
      "batch 17630: train=0.850000 test=0.750000\n",
      "batch 17640: train=0.900000 test=0.650000\n",
      "batch 17650: train=0.850000 test=0.650000\n",
      "batch 17660: train=0.900000 test=0.700000\n",
      "batch 17670: train=0.950000 test=0.850000\n",
      "batch 17680: train=0.950000 test=0.800000\n",
      "batch 17690: train=0.700000 test=0.700000\n",
      "batch 17700: train=0.750000 test=0.750000\n",
      "batch 17710: train=0.700000 test=0.600000\n",
      "batch 17720: train=0.850000 test=0.600000\n",
      "batch 17730: train=0.850000 test=0.800000\n",
      "batch 17740: train=0.800000 test=0.850000\n",
      "batch 17750: train=0.750000 test=0.800000\n",
      "batch 17760: train=1.000000 test=0.850000\n",
      "batch 17770: train=0.850000 test=0.800000\n",
      "batch 17780: train=0.850000 test=0.650000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 17790: train=0.850000 test=0.750000\n",
      "batch 17800: train=0.800000 test=0.800000\n",
      "batch 17810: train=0.850000 test=0.850000\n",
      "batch 17820: train=0.950000 test=0.800000\n",
      "batch 17830: train=0.800000 test=0.800000\n",
      "batch 17840: train=0.950000 test=0.650000\n",
      "batch 17850: train=0.850000 test=0.600000\n",
      "batch 17860: train=0.900000 test=0.850000\n",
      "batch 17870: train=0.900000 test=0.800000\n",
      "batch 17880: train=0.900000 test=0.900000\n",
      "batch 17890: train=0.950000 test=0.750000\n",
      "batch 17900: train=0.800000 test=0.800000\n",
      "batch 17910: train=0.900000 test=0.900000\n",
      "batch 17920: train=0.850000 test=0.750000\n",
      "batch 17930: train=0.950000 test=0.700000\n",
      "batch 17940: train=1.000000 test=0.700000\n",
      "batch 17950: train=0.750000 test=0.700000\n",
      "batch 17960: train=1.000000 test=0.750000\n",
      "batch 17970: train=0.900000 test=0.850000\n",
      "batch 17980: train=0.900000 test=0.800000\n",
      "batch 17990: train=0.700000 test=0.850000\n",
      "batch 18000: train=0.950000 test=0.800000\n",
      "batch 18010: train=0.800000 test=0.850000\n",
      "batch 18020: train=0.950000 test=0.900000\n",
      "batch 18030: train=0.800000 test=0.700000\n",
      "batch 18040: train=0.750000 test=0.750000\n",
      "batch 18050: train=0.950000 test=0.800000\n",
      "batch 18060: train=0.750000 test=0.750000\n",
      "batch 18070: train=0.900000 test=0.750000\n",
      "batch 18080: train=0.800000 test=0.650000\n",
      "batch 18090: train=0.900000 test=0.950000\n",
      "batch 18100: train=0.900000 test=0.800000\n",
      "batch 18110: train=0.900000 test=0.700000\n",
      "batch 18120: train=0.850000 test=0.650000\n",
      "batch 18130: train=0.700000 test=0.600000\n",
      "batch 18140: train=0.950000 test=0.850000\n",
      "batch 18150: train=0.900000 test=0.900000\n",
      "batch 18160: train=0.850000 test=0.900000\n",
      "batch 18170: train=0.850000 test=0.750000\n",
      "batch 18180: train=0.800000 test=0.750000\n",
      "batch 18190: train=0.900000 test=0.650000\n",
      "batch 18200: train=0.950000 test=0.700000\n",
      "batch 18210: train=1.000000 test=0.750000\n",
      "batch 18220: train=0.850000 test=0.850000\n",
      "batch 18230: train=0.800000 test=0.800000\n",
      "batch 18240: train=0.850000 test=0.750000\n",
      "batch 18250: train=0.900000 test=0.850000\n",
      "batch 18260: train=0.950000 test=0.800000\n",
      "batch 18270: train=0.850000 test=0.900000\n",
      "batch 18280: train=0.900000 test=0.750000\n",
      "batch 18290: train=0.700000 test=0.700000\n",
      "batch 18300: train=0.950000 test=0.850000\n",
      "batch 18310: train=0.750000 test=0.700000\n",
      "batch 18320: train=0.950000 test=0.850000\n",
      "batch 18330: train=0.850000 test=1.000000\n",
      "batch 18340: train=0.850000 test=0.800000\n",
      "batch 18350: train=0.800000 test=0.750000\n",
      "batch 18360: train=0.850000 test=0.750000\n",
      "batch 18370: train=0.900000 test=0.700000\n",
      "batch 18380: train=0.650000 test=0.700000\n",
      "batch 18390: train=0.800000 test=0.600000\n",
      "batch 18400: train=0.800000 test=0.700000\n",
      "batch 18410: train=0.650000 test=0.850000\n",
      "batch 18420: train=0.750000 test=0.750000\n",
      "batch 18430: train=0.800000 test=0.600000\n",
      "batch 18440: train=0.700000 test=0.850000\n",
      "batch 18450: train=0.950000 test=0.800000\n",
      "batch 18460: train=1.000000 test=0.800000\n",
      "batch 18470: train=0.750000 test=0.700000\n",
      "batch 18480: train=0.900000 test=0.800000\n",
      "batch 18490: train=0.850000 test=0.900000\n",
      "batch 18500: train=0.900000 test=0.750000\n",
      "batch 18510: train=0.950000 test=0.600000\n",
      "batch 18520: train=0.950000 test=0.900000\n",
      "batch 18530: train=0.800000 test=0.850000\n",
      "batch 18540: train=0.800000 test=0.850000\n",
      "batch 18550: train=0.900000 test=1.000000\n",
      "batch 18560: train=0.950000 test=0.800000\n",
      "batch 18570: train=0.900000 test=0.700000\n",
      "batch 18580: train=0.850000 test=0.800000\n",
      "batch 18590: train=0.900000 test=0.900000\n",
      "batch 18600: train=1.000000 test=0.650000\n",
      "batch 18610: train=0.800000 test=0.800000\n",
      "batch 18620: train=0.950000 test=0.700000\n",
      "batch 18630: train=0.800000 test=0.750000\n",
      "batch 18640: train=0.750000 test=0.700000\n",
      "batch 18650: train=0.800000 test=0.700000\n",
      "batch 18660: train=0.900000 test=0.800000\n",
      "batch 18670: train=0.900000 test=0.850000\n",
      "batch 18680: train=0.850000 test=0.850000\n",
      "batch 18690: train=0.750000 test=0.600000\n",
      "batch 18700: train=0.700000 test=0.750000\n",
      "batch 18710: train=0.850000 test=0.600000\n",
      "batch 18720: train=0.850000 test=0.900000\n",
      "batch 18730: train=0.850000 test=0.850000\n",
      "batch 18740: train=0.850000 test=0.800000\n",
      "batch 18750: train=1.000000 test=0.850000\n",
      "batch 18760: train=0.900000 test=0.850000\n",
      "batch 18770: train=1.000000 test=0.850000\n",
      "batch 18780: train=0.950000 test=0.750000\n",
      "batch 18790: train=0.950000 test=0.800000\n",
      "batch 18800: train=0.800000 test=0.800000\n",
      "batch 18810: train=0.900000 test=0.650000\n",
      "batch 18820: train=0.850000 test=0.750000\n",
      "batch 18830: train=0.900000 test=0.800000\n",
      "batch 18840: train=0.950000 test=0.850000\n",
      "batch 18850: train=0.900000 test=0.850000\n",
      "batch 18860: train=0.900000 test=0.850000\n",
      "batch 18870: train=0.700000 test=0.850000\n",
      "batch 18880: train=0.900000 test=0.700000\n",
      "batch 18890: train=0.850000 test=0.850000\n",
      "batch 18900: train=0.950000 test=0.800000\n",
      "batch 18910: train=1.000000 test=0.850000\n",
      "batch 18920: train=0.900000 test=0.750000\n",
      "batch 18930: train=0.900000 test=0.800000\n",
      "batch 18940: train=0.900000 test=0.650000\n",
      "batch 18950: train=0.850000 test=0.800000\n",
      "batch 18960: train=0.950000 test=0.900000\n",
      "batch 18970: train=0.950000 test=0.850000\n",
      "batch 18980: train=0.850000 test=0.650000\n",
      "batch 18990: train=1.000000 test=0.800000\n",
      "batch 19000: train=0.800000 test=0.800000\n",
      "batch 19010: train=0.750000 test=0.750000\n",
      "batch 19020: train=0.900000 test=0.750000\n",
      "batch 19030: train=0.850000 test=0.800000\n",
      "batch 19040: train=0.950000 test=0.850000\n",
      "batch 19050: train=0.950000 test=0.950000\n",
      "batch 19060: train=0.750000 test=0.700000\n",
      "batch 19070: train=0.950000 test=0.750000\n",
      "batch 19080: train=0.950000 test=0.950000\n",
      "batch 19090: train=0.900000 test=0.850000\n",
      "batch 19100: train=1.000000 test=0.850000\n",
      "batch 19110: train=0.850000 test=0.550000\n",
      "batch 19120: train=0.900000 test=1.000000\n",
      "batch 19130: train=0.900000 test=0.850000\n",
      "batch 19140: train=0.950000 test=0.650000\n",
      "batch 19150: train=0.950000 test=0.750000\n",
      "batch 19160: train=0.850000 test=0.800000\n",
      "batch 19170: train=0.900000 test=0.700000\n",
      "batch 19180: train=0.900000 test=0.650000\n",
      "batch 19190: train=0.850000 test=0.800000\n",
      "batch 19200: train=1.000000 test=0.650000\n",
      "batch 19210: train=0.850000 test=0.900000\n",
      "batch 19220: train=0.800000 test=0.850000\n",
      "batch 19230: train=0.950000 test=0.600000\n",
      "batch 19240: train=0.800000 test=0.700000\n",
      "batch 19250: train=0.900000 test=0.600000\n",
      "batch 19260: train=0.900000 test=0.950000\n",
      "batch 19270: train=0.900000 test=0.700000\n",
      "batch 19280: train=0.900000 test=0.850000\n",
      "batch 19290: train=0.900000 test=0.700000\n",
      "batch 19300: train=0.950000 test=0.850000\n",
      "batch 19310: train=0.900000 test=0.650000\n",
      "batch 19320: train=0.900000 test=0.950000\n",
      "batch 19330: train=0.850000 test=0.750000\n",
      "batch 19340: train=0.850000 test=0.900000\n",
      "batch 19350: train=0.800000 test=0.800000\n",
      "batch 19360: train=0.900000 test=0.900000\n",
      "batch 19370: train=0.950000 test=0.750000\n",
      "batch 19380: train=0.850000 test=0.600000\n",
      "batch 19390: train=0.950000 test=0.850000\n",
      "batch 19400: train=0.900000 test=0.700000\n",
      "batch 19410: train=0.850000 test=0.900000\n",
      "batch 19420: train=0.850000 test=0.900000\n",
      "batch 19430: train=0.800000 test=0.900000\n",
      "batch 19440: train=0.850000 test=0.650000\n",
      "batch 19450: train=0.950000 test=0.800000\n",
      "batch 19460: train=0.850000 test=0.750000\n",
      "batch 19470: train=0.850000 test=0.700000\n",
      "batch 19480: train=0.900000 test=0.950000\n",
      "batch 19490: train=0.900000 test=0.700000\n",
      "batch 19500: train=0.850000 test=0.750000\n",
      "batch 19510: train=0.950000 test=0.750000\n",
      "batch 19520: train=1.000000 test=0.700000\n",
      "batch 19530: train=0.750000 test=0.800000\n",
      "batch 19540: train=0.950000 test=0.550000\n",
      "batch 19550: train=0.850000 test=0.700000\n",
      "batch 19560: train=0.900000 test=0.750000\n",
      "batch 19570: train=0.850000 test=0.750000\n",
      "batch 19580: train=0.900000 test=0.850000\n",
      "batch 19590: train=0.900000 test=0.750000\n",
      "batch 19600: train=0.900000 test=0.850000\n",
      "batch 19610: train=0.950000 test=0.800000\n",
      "batch 19620: train=0.900000 test=0.800000\n",
      "batch 19630: train=0.800000 test=0.650000\n",
      "batch 19640: train=0.900000 test=0.750000\n",
      "batch 19650: train=0.750000 test=0.700000\n",
      "batch 19660: train=0.900000 test=0.950000\n",
      "batch 19670: train=0.850000 test=0.900000\n",
      "batch 19680: train=0.750000 test=0.750000\n",
      "batch 19690: train=0.750000 test=0.700000\n",
      "batch 19700: train=0.750000 test=0.750000\n",
      "batch 19710: train=0.900000 test=0.700000\n",
      "batch 19720: train=0.900000 test=0.750000\n",
      "batch 19730: train=0.900000 test=0.850000\n",
      "batch 19740: train=0.950000 test=0.650000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 19750: train=0.800000 test=0.700000\n",
      "batch 19760: train=1.000000 test=0.600000\n",
      "batch 19770: train=0.850000 test=0.850000\n",
      "batch 19780: train=0.800000 test=0.850000\n",
      "batch 19790: train=0.800000 test=0.700000\n",
      "batch 19800: train=0.850000 test=0.900000\n",
      "batch 19810: train=0.950000 test=0.700000\n",
      "batch 19820: train=0.850000 test=0.750000\n",
      "batch 19830: train=0.900000 test=0.950000\n",
      "batch 19840: train=0.950000 test=0.900000\n",
      "batch 19850: train=0.900000 test=0.950000\n",
      "batch 19860: train=0.900000 test=0.850000\n",
      "batch 19870: train=0.700000 test=0.650000\n",
      "batch 19880: train=0.900000 test=0.700000\n",
      "batch 19890: train=0.900000 test=0.700000\n",
      "batch 19900: train=0.900000 test=0.750000\n",
      "batch 19910: train=0.850000 test=0.750000\n",
      "batch 19920: train=0.900000 test=0.800000\n",
      "batch 19930: train=0.900000 test=0.800000\n",
      "batch 19940: train=0.850000 test=0.800000\n",
      "batch 19950: train=0.900000 test=0.750000\n",
      "batch 19960: train=0.850000 test=0.850000\n",
      "batch 19970: train=0.900000 test=0.550000\n",
      "batch 19980: train=0.800000 test=0.700000\n",
      "batch 19990: train=0.600000 test=0.750000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if not args['pretrained']:\n",
    "        print('Training...')\n",
    "        train(sess, model, train_set, test_set, save_dir='checkpoints',**train_kwargs(args))\n",
    "    else:\n",
    "        print('Restoring from checkpoint...')\n",
    "        tf.train.Saver().restore(sess, tf.train.latest_checkpoint(args['checkpoint']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OO4t6t3_YXy2"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training,close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNpMNQmIYX_p"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the train and test accuracies after 20,000 epochs\n",
    "    \n",
    "#### Train Accuracy\n",
    "<img src=\"train.png\" style=\"display:inline;\"></img>\n",
    "#### Test Accuracy\n",
    "<img src=\"test.png\" style=\"display:inline;\"></img>\n",
    "#### Train vs Test\n",
    "<img src=\"train_test.png\" style=\"display:inline;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training accuracy for 100 random one-shot 20-way tasks generated from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiKPgR_u4Ly7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt-19999\n",
      "In Evaluate\n",
      "Train accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "args['eval_samples'] = 100\n",
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    print('Train accuracy: ' + str(evaluate(sess, model, train_set, **eval_kwargs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing accuracy for 100 random one-shot 20-way tasks generated from testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt-19999\n",
      "In Evaluate\n",
      "Test accuracy: 0.7925\n"
     ]
    }
   ],
   "source": [
    "args['eval_samples'] = 100\n",
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    print('Test accuracy: ' + str(evaluate(sess, model, test_set, **eval_kwargs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The n-shot, k-way task\n",
    "\n",
    "The ability of an algorithm to perform few-shot learning is typically measured by its performance on n-shot, k-way tasks. These are run as follows:\n",
    "\n",
    ">A model is given a query sample belonging to a new, previously unseen class\n",
    "\n",
    ">It is also given a support set, S, consisting of n examples each from k different unseen classes\n",
    "\n",
    ">The algorithm then has to determine which of the support set classes the query sample belongs to\n",
    "\n",
    "We are using one-shot 20-way tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generate n-shot, k-way tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fucntion which uses the make_one_shot fucntion of FOML to return predictions and samples of the given dataset.\n",
    "\n",
    "def create_one_shot(sess,\n",
    "             model,\n",
    "             dataset,\n",
    "             num_classes,\n",
    "             num_shots,\n",
    "             eval_inner_batch_size,\n",
    "             eval_inner_iters,\n",
    "             replacement,\n",
    "             num_samples,\n",
    "             transductive,\n",
    "             model_fn,\n",
    "             weight_decay_rate):\n",
    "    \"\"\"\n",
    "    Create a few-shot tasks on a dataset.\n",
    "    \"\"\"\n",
    "    foml = model_fn(sess,\n",
    "                         transductive=transductive,\n",
    "                         pre_step_op=weight_decay(weight_decay_rate))\n",
    "    pred_sample = foml.make_one_shot(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes=num_classes, num_shots=num_shots,\n",
    "                                          inner_batch_size=eval_inner_batch_size,\n",
    "                                          inner_iters=eval_inner_iters, replacement=replacement)\n",
    "    return pred_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call that function to generate one-shot 20-way tasks and assign them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt-19999\n"
     ]
    }
   ],
   "source": [
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    prediction_samples = create_one_shot(sess,model,test_set, **eval_kwargs)\n",
    "prediction = []\n",
    "samples = []\n",
    "for pred, sample in prediction_samples:\n",
    "    prediction.append(pred)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting one-shot 20-way tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions to concatenate support set into one plot and to plot the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(samples):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc = len(samples)\n",
    "    h , w = samples[0][0].shape\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros(((n-1)*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i in range(len(samples)):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = samples[i][0]\n",
    "        y += 1\n",
    "        if y >= n and x <= (n-1):\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "def plot_oneshot_task(prediction , samples):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    p = samples[prediction][0]\n",
    "    ax1.set_title('Sample')\n",
    "    ax2.set_title('Support Set')\n",
    "    ax1.matshow(p, cmap='gray')\n",
    "    img = concat_images(samples)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a random prediction and plot the sample and support set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAC+CAYAAAALItWnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADxZJREFUeJzt3X/MJVddx/HPR0CwIiIWi7Wlq0aDwd950P6BsSYqQYP+oRiNVdGIxoT4T21AoUoJppoQg4p/qbHamloj/orSBNDUGA3IVqHF2JgGlrSs1F1kF9jUxpavf8w8ZDo7M3fmPnPmnDvzfiVPsvvce2fOzH3u537vmXPPcUQIALC8z8ndAADYKgIYADIhgAEgEwIYADIhgAEgEwIYADIhgDOy/Ubbd+ZuB4A8NhvAtl9q+59tX7T9P7b/yfZLcrcLwHY8PXcDcrD9HEl/I+nnJP2ppM+V9G2SHs/ZLgDbstUK+KslKSLuiognI+KxiHhnRNxv+ytt/73tj9s+b/uPbT/3+IG2z9i+2fb9ti/Z/n3bV9m+x/anbL/b9hfV9z1lO2z/jO2ztv/L9k19jbJ9fV2VX7D9Ads3JD8TALLZagD/p6Qnbf+h7ZcfB2bNkm6TdLWkr5F0raQ3th7/A5K+S1WQv0LSPZJ+SdKVqs7pz7fu/x2SvkrSd0t6ne3vbDfI9pdJ+ltJb5b0PEm/IOnttp+//2ECKNkmAzgiPinppZJC0u9KOmf7r21fFREPRcS7IuLxiDgn6TckfXtrE78dEY9GxEcl/aOk90bEv0XE45L+QtI3te5/a0RciogHJP2BpB/paNaNkt4REe+IiM9ExLsknZb0PXMdN4CybDKAJSki/iMiXhUR10j6WlUV71ttf4ntP7H9UduflHSnqsq26dHGvx/r+P+zW/d/uPHvj9T7artO0ivr7ocLti+oepP40skHB+AgbDaAmyLiQUm3qwri21RVxl8fEc9RVZn6hLu4tvHvF0o623GfhyXdERHPbfx8fkT82gn3DaBQmwxg2y+yfZPta+r/X6uqW+A9kr5A0qclXaj7ZW+eYZe32L7C9osl/aSkuzvuc6ekV9h+me2n2X6W7RuO2whgfTYZwJI+JelbJb3X9iVVwftBSTdJulXSN0u6qOqi2J/PsL9/kPSQpL+T9JaIeGf7DhHxsKTvV3Ux75yqivhmbfc5AlbPTMieju1Tkj4s6RkR8UTe1gAoDdUVAGRCAANAJnRBAEAmVMAAkAkBDACZEMAAkAkBDACZEMBAoQ5l0YD6G5uP7LjPNbbfXk/xetH2A7ZfNXL799r+6VkaW5hNTsgOlO5QFg2wPTZD7pD0AVWTTj0u6eskvSBVuw4FFTBQpt5FA6TL1xNsTP7/9Pr/99q+zfa/1BXnX9l+Xuu+nQsF2H6m7bfWt52t//3M+rYbbD9i+7W2PybpLlXzYV9t+9P1T9dsfy+RdHs9LesT9fSt9zT22bkYge1fVfXG87Z622+b8yTnRgADZRpaNGCsH5f0U6qmP31C0m+1bu9bKOD1kq6X9I2SvkHSt0h6Q+NxL1C1aMB19T5eLulsRDy7/uma7e89kn7H9g/bfmHzhqHFCCLi9arm3H5Nve3X7HEeikUAAwUaWjRgwmbuiIgPRsQlSbdI+iHbT2vc3rdQwI9KelNE/He9KMGtkn6s8bjPSPqVetGCx0a25ZWqgvQWSR+2/f5Gf/ZmFyMggIFC9S0aMGET7YUAnqGnLi7Qt1DA1fX/u26TpHMR8b8T2qGI+EREvC4iXizpKknvl/SXtq0NL0Yw2IF+5ZVXxqlTpxZqCrbmzJkzOn/+/Eknu9+EiHjQ9u2Sfrb+1SVJVzTu0nVBq70QwP9JOt/4/bWSHmzcftx1cFZVKP57x21SVZVr4P+DIuK87bdI+glVXQ7HixG8uu8hU7Z/SAYD+NSpUzp9+vRSbcHGHB0d5W5CsWy/SNL3Sro7Ih5pLRogVRXka+v+1IuSfrFjMzfa/iNJZyS9SdKfRcSTVdEpqVoo4NWSvlzVQgE31r+/S9IbbL9PVfj9sqoFA/o8KumLbX9hRFzsOZ5fVzUS4kFJn6dqdMdDEfHx+mLi+2y/TNK7VVXq19e3P1Jv/ysG9n+w6IIAyjS0aIDqftK7Jd0v6T5VQ9ba7lC11NbHJD1Ll6/W3bdQwJtV9cHeL+kBSf9a/65TvaTXXZI+VHchdI2CuELVgrUXJH1IVYX9ffXjdy1G8JuSftD2J2y3LyQetMHZ0I6OjoIKGKkcHR3p9OnTdEEkYPteSXdGxO913HZKLBRQBCpgAMiEAAaATPgqMrBCEXHDwG1nJNH1UwAqYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEz4IgY2ialWkdLYqVYJYGwSU60ipbFTrdIFAQCZEMAAkAldEAA+q7FaxijH84kfP25ofnFcjgAGJrLdGzQnDaK+AFwi2Jr7HrM/24PnYq3mPGa6IAAgEypgoCAR0VkFD3UNzFGN7VO5H7d1arfFVKVU2SmOkwoYaEkdKFNExGU/bSdt7yH03+Z+TqZ2z4xFAAMHph3EcwTCXKGSIijbF/qOq+4lqu92G+Z+k6ILAphRXxdCim3Msa852trczj4B1W7D2G00u0BOGoxjz8Pc3SEEMLCHpfolx4y4OKlcXQ997d81EqRd/aesglOfGwIYKEw7UIaqy7kuwOUI4Sn7HKp0j0O4pK6YsQhgoGDNCi9V/2quC1z7dD3s+kRwkgDN8SZEAAOFa/ezpth+M7yWHBUxtb9339vH2rUNLsIBG9V+8c8ZyO0AK3FIWupqPcfxMwwNOGBzh/DQflIM+xq6EDfltjn6gIfGWqdCBTyjnN/jx7qM+ZvJ2X/btk9bpj6m7+LkIaMCBoBMqICBDlM+0i4xF0KXUj5ZpfgySN+Fx+Yxp7xYuFR/MAEM9NgVLClfpLuGaDX7QXNOVjPXN+n2uT31OW//PsU5JoCBllIqS6m/Le3+07lCYky/bN+bwxr6ZIfGGKd4oyOAFzD0EQtomhKkJV2EW4Nd1XaKECaAgYJMfXGnqj7Hbm8thUSuNzwCGFiBZhDP0Q1xvK2+24Yet3ZzVsEEMLAic4bgVgJ1irmrYMYBA8AErIgBACtAF8SMpkycPWVw/Rwfefg4CZSHChgAMiGAASATAhgAMiGAASCTg7wIx0WpypQLdnwd+rDlnHAH6VABAweCeR/WhwAGDkDz68EplgZCHgfZBQFs0RLTPi69KvDWEcAAnmLo2kLpKydPtWup+12POek5IICBgi3Z1TAljFJNUL609qeKKdNSznHsxQTw0n1aufvQpjx5c9w39/GW7KRrrqVcsqZPqn2N3W6qCcpz6Hv++kYOMRsaMJPmi6/507xtl9R9s+12laK09pxE17E0f5fq/BdTAQO59L34+kYbDN1/KWvriy3R0CRac51zKmBs3pgr/2Mr40Pv6ula6LNUc7SvvY2lh/kRwACQSTFdEGv+GHXS+YDnMOVry2t+Lk6qlItPJTxHpVfIJ9XXrcSKGMBM9lmAcsmFKXOMrOj6CN7+aJ5j5EdqXf3qqY+PAAb21BdUKaUOhLFX+0sclXFSzeNZ6nklgLF5c8yzsHRFuMTH/7UF7BhdF1tTDjMkgAH1f9Pr+N99j2mPFV3CEnNC5JZ7wqG+czz3c1zMRTggt6kfQUsIiBR2vfHkvgA5p6XHb7cVE8BMsl4pYeL0EtqQW1dF3HyxrvV89H3sHnpzSmHp0SZ9+2lejEtx3MUEMFCypa6K59QVtru+opvSUiE89K3H9v3mRgADeIqSqvylughyHSsBDEDS5d0O+8yVm0qzCl7TxUcCGMBTlFD5NrWr4NLadxLFB/AaTvaUrwEvibmDgbwYBwygeGsoxLoQwACQCQEMAJkQwACQSfEX4daKC2AAqIABIBMCGAAyIYCBHegWQioEMDCA8C3DWp8HAhhA0dYavtIBjIJYcm7aKU/0kt/MSbV6Masil6GkSW9KU9LMbClQAQNAJsVXwEAuS3z07avwmvvumglsC59e1l79SgQw0Kn54s+51Pyutem65vCdGlhDYZ67e2TN4SsRwMAoKZfFGdr20FSmXUsHTX2zGKrAx1Tnzd/NeX5KufCWeh7iYgJ4a1/NneO41npuchi62Nu1ZthcL8Z9Kuyux6T6W+hbE24Lf3vtT0EpukSKCWCgJEMvsrkroeMX+K7FMHdt4yT7R2VX3/rcbzwEMKDxIZQqrAjB/MZ+wpmzu4UABjBJyu6H3Gu/Lb1PAhgYIVe/5xJfRNrnuFIEVc5hZ2PPwdx/BwQwMEHK0RDt/Rxr7i/lm0CuZd9zV705HWQAL3nFdy1t2NofdgpLVcF9Id8c6zu3sX8fax39MHakAxfhgMyWqILHvNDnaMe+byopxgLnLhKa56LrWFJ0kTAXBABkQgUMTJDzSwjtvuCUXwbpO87cVWpqzX7wJY6fAAYmSh1CU/pj5wzhfdsxl6UucI6xVDuKD+BSnpC2EtpVQhuQx5q+DtyuOrf0d00fMIAi5BoGlxMBDByoNVaKazymIQQwgKJsKYQJYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIpPjJeACsW9dUmFtBBQxs2NDEN7kmxTmeFW0Lk/JQAQMbl3sKyC1VvG1UwMCG7ZoCMmcVumQVnKvipgIGCrPFpYBKlWIhziYqYKBAEfHZn7Yt9I2OlfJcLHGeqYCBHVJfpW9uv2/b7X7aFItyNreZuvLb15JdElL6pZ8IYGBAVxAtHU7tEEgZCmPeDHJYql1LHz8BDOzQfiE2K8au2+fYR5fUoxVyBW7fJ4xSqvGU+6UPGAAyIYCBgvR1LZTUHTC39sXGUkaBcBEOKMASL8Sx+1jzCIhml0Oz22HJ8G2/EaTu9iGAgQFDF7zmHIUg7Q7XkqrglMFUynGmHgEhEcDATkuOdihdM5Byf4V5KSmPkwAGOuxb+WwhkKQ0I0FKxDhgIIO1Bsq+usbHtvtsOWfTMQoCgKRxE/J0hezYPmxcjgoYwGVdCu3b9nn8IVXEQ21lFASALKaEzyEFbikIYACSCNAc6AMGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwYB4xNuu+++87b/kjudmC1rhtzJwIYmxQRz8/dBoAuCADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIhAAGgEwIYADIxENLUds+J4kp+5DKdcxKhi0bDGAAQDp0QQBAJgQwAGRCAANAJgQwAGRCAANAJv8PPg5ebw1dryQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = random.randint(0,len(prediction)-1)\n",
    "plot_oneshot_task(prediction[x],samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation \n",
    "\n",
    "Given this sample from unseen data and support set to the model, it determines the class of the query sample from the support set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis  (20-dimensional data)\n",
    "\n",
    "Please refer to 'Helper function to plot images in a row' of Helper Functions sections to know the functions used in this section.\n",
    "\n",
    "256 points plotted over 20-dimensions\n",
    "\n",
    "\n",
    "- Total Variance for Fig 1 (Dense/Kernel): 42.7%\n",
    "- Total Variance for Fig 2 (Dense/Kernel/Adam): 99.2%\n",
    "- Total Variance for Fig 3 (Dense/Kernel/Adam_1): 98.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2520x2520 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_A = cv2.imread(\"B_A.png\")\n",
    "img_B = cv2.imread(\"B_B.png\")\n",
    "img_C = cv2.imread(\"B_C.png\")\n",
    "\n",
    "titles = [\"Dense/kernel\", \n",
    "          \"Dense/kernel/Adam\", \n",
    "          \"Dense/kernel/Adam_1\"]\n",
    "images = [img_A,img_B,img_C]\n",
    "\n",
    "grid_display(images, titles, 3, (35,35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did the model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-of-the-art solution\n",
    "\n",
    "The state-of-the-art solution 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks' on Few-Shot learning presented us with an accuracy of 98.7%. Taking into consideration the number of epochs trained and the GPU availability, I trained for 20,000 epochs and achieved an accuracy of 87.15%. If the model had trained for more epochs, I guess we might nearly achieve the state-of-the-art accuracy presented in ICML 2017. By Chelsea Finn • Pieter Abbeel • Sergey Levine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial training algorithm (Scaled updates)\n",
    "\n",
    "Intially in the train_step function of class FOML, model state variables are updated from the last backup and this session's state variables, pushing the difference into an updates array and then average of these updates are scaled with meta step size and adding then to the old varibales of the model. \n",
    "\n",
    "But training on this way of train_step implementation, the accuracy was not improving over a lot of epochs. Please see the train vs test accuracy of scaled updates type of training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in meta_iter:\n",
    "    #declare mini_batches\n",
    "    for batch in mini_batches:\n",
    "        inputs, labels = zip(*batch)\n",
    "        last_backup = self._model_state.export_variables()\n",
    "        self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "    updates.append(subtract(self._model_state.export_variables(), last_backup))\n",
    "    self._model_state.import_variables(old_vars)\n",
    "update = average(updates)\n",
    "self._model_state.import_variables(add(old_vars, scale(update, meta_step_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs Test Accuracy (Scaled Updates)\n",
    "\n",
    "<img src=\"train_test_su.png\">\n",
    "\n",
    "\n",
    "As you can see over 4k epoch (12k - 16k), accuracy was not improving, so I had to stop training and tweak the train_step implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaked Training algorithm (Interpolating updates)\n",
    "\n",
    "As present, the model was trained on this tweaked train_step function implementation. We achieved an accuracy of 87.15% over 20,000 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "* Training the present model on more epochs,on a better GPU to nearly achieve the state-of-the-art solution accuracy\n",
    "* Refering to a paper [How to Train your MAML](https://openreview.net/forum?id=HJGven05Y7) by Antreas Antoniou, Harrison Edwards, Amos Storkey, we get more insights on the problems of vanilla MAML.\n",
    "* This paper proposes several improvements for the MAML algorithm that improve its stability and performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MOkkRtKgP8vn",
    "SDbXkSguzcRl",
    "YGoqzrxCRT08",
    "KvSfs6DtQ8AQ",
    "VXLnl9VOTGSs",
    "X0v7CU4MPl81",
    "T1EdHROGSN3n",
    "Di9-cWimTB9g",
    "tTaGIO69VNnn",
    "k3AjdjwwTrKM",
    "WXl3jnkRFBb2",
    "9jK0PRHaNHWB"
   ],
   "name": "One-Shot Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

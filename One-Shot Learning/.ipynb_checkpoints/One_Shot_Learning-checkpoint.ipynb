{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rDHfg_djSVA"
   },
   "source": [
    "# One-Shot Learning of Omniglot Dataset using First-Order MAML \n",
    "\n",
    "This notebook explores one-shot learning of Omniglot Dataset using MAML (Model-Agnostic Meta-Learning). \n",
    "\n",
    "## Overview\n",
    "\n",
    "Artificial intelligence is trying to learn how to learn from the way humans learn. We can quickly and easily recognize a new object from just seeing one or few pictures of it, or even from only reading about it without having ever seen it before. We can learn quickly a new skill as well as master many different tasks. This seems easy for the human intelligence but for machines, it is quite a challenge to overcome. Can machine learning do better?\n",
    "\n",
    "## One-Shot Learning\n",
    "\n",
    "Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of **one-shot learning** - if you take a human who’s never seen a bird before, and show them a single picture of a bird, they will probably be able to distinguish birds from other animals with astoundingly high precision. We want machines to also learn and perform tasks in **One Shot Learning**\n",
    "\n",
    "## Meta Learning\n",
    "\n",
    "Best put, **Learning to Learn** is Meta Learning. Deep learning has a great success in mastering one task using a large dataset. We really want to achieve Few-Shot Meta Learning, an algorithm that trains a neural network to learn many different tasks using only a small data per task. In meta-learning, there is a meta-learner and a learner. The meta-learner (or the agent) trains the learner (or the model) on a training set that contains a large number of different tasks.\n",
    "\n",
    "## MAML(Model-Agnostic Meta Learning)\n",
    "\n",
    "Model-Agnostic ~  Model Independent \n",
    "\n",
    "MAML, short for Model-Agnostic Meta-Learning is a fairly general optimization algorithm, compatible with any model that learns through gradient descent. MAML does not learn on batches of samples like most deep learning algorithms but batches of tasks AKA meta-batches. For each task in a meta-batch we first initialise a new “fast model” using the weights of the base meta-learner. We then compute the gradient and hence a parameter update from samples drawn from that task and update the weights of the fast model i.e. perform typical mini-batch stochastic gradient descent on the weights of the \"fast model\".The brilliance of this approach is that it can not only work for supervised regression and classification problems but also for reinforcement learning using any differentiable model!\n",
    "\n",
    ">**First-Order MAML**\n",
    "\n",
    "> To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as First-Order MAML (FOML).\n",
    "\n",
    "For this one-shot learning problem, we are going to use First-Order MAML.\n",
    "\n",
    "## Omniglot Dataset\n",
    "\n",
    "Omniglot dataset is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fRd4Sc6cT0_KFm6IhB3Bqw.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rbld2gy8Th-j"
   },
   "source": [
    "# Let's get started!\n",
    "\n",
    "Let us structure the whole code , so that it would be easier to understand. This is how we are going to break it down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUlvBYKg65Xt"
   },
   "source": [
    "## Mount Goolge Drive to Colab\n",
    "\n",
    "If using Google Colab, mount the drive and go to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aEH4iUUajPIY",
    "outputId": "81267d74-373c-4ea9-b33c-181db2304e57"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZRimCF2yjCEP",
    "outputId": "e5c45175-d0b1-4df4-aa8e-d22bff82909f"
   },
   "outputs": [],
   "source": [
    "cd '/content/drive/My Drive/One-Shot Learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOkkRtKgP8vn"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kynkNg31yi9C"
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDbXkSguzcRl"
   },
   "source": [
    "## Preparing our Dataset\n",
    "\n",
    "[Omniglot](https://https://github.com/brendenlake/omniglot). \n",
    "\n",
    "Download and Extract: \n",
    "\n",
    "Background set of [30 alphabets](https://https://github.com/brendenlake/omniglot/blob/master/python/images_background.zip) for training and save it in 'train' folder.\n",
    "\n",
    "Evaluation on set of [20 alphabets](https://https://github.com/brendenlake/omniglot/blob/master/python/images_evaluation.zip) for testing and save it in 'test' folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWNw3ilT0j47"
   },
   "outputs": [],
   "source": [
    "train_dir = 'train' #Directory for training samples \n",
    "test_dir = 'test' #Directory for testing samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jjFyd-hQu87"
   },
   "source": [
    "## Initialize required arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLDcJ3Bgyjg-"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "\"shots\":1, #Number of shots for Meta-Learning (1-shot)\n",
    "\"classes\":20, #Number of classes in the sample (20-way)\n",
    "\"learning_rate\":0.0005,# Learning Rate\n",
    "    \n",
    "#Meta-Training arguments\n",
    "\"train_shots\":10, # Shots in a training batch\n",
    "\"meta_batch\":5, # Batch size for Meta-training\n",
    "\"meta_iters\":20000, # Iteration for Meta-training\n",
    "\"meta_step\":1.0,# Meta-Training step size\n",
    "\"meta_step_final\":0.0, # Meta-Training step size at the end\n",
    "    \n",
    "#Inner Loop Training arguments\n",
    "\"inner_batch\":20, # Inner Batch size for train step\n",
    "\"inner_iters\":10, # Inner Iteration for train step\n",
    "    \n",
    "#Evaluation arguments\n",
    "\"eval_batch\":10, #Inner Batch size for evaluation\n",
    "\"eval_interval\":10, #Train steps for evaluation\n",
    "\"eval_iters\":50, #Inner Iterations for evaluation\n",
    "\"eval_samples\":10000,# Evaluation Samples\n",
    "    \n",
    "#FOML\n",
    "\"foml_tail\":None, # Declare tail-shots for FOML\n",
    "\"pretrained\":False, # Flag for checking if the model is pretrained or not\n",
    "\"replacement\":False, # Replacement of Sample data\n",
    "\"seed\":0, # Random seed\n",
    "\n",
    "\"transductive\":False, # Flag to evaluate all the samples at once\n",
    "\"weight_decay\":1 # Rate of decay of weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzyHiQvzTBHo"
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "* Helper Function to plot images in a row\n",
    "*   Helper Functions to read and augment our dataset\n",
    " > Character class\n",
    " \n",
    " > Functions to read and augment the dataset\n",
    "*   Helper Functions for variable manipulation\n",
    "* Helper Functions for returning (key,value) pair arguments (kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot images in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_display(list_of_images, list_of_titles=[], no_of_columns=2, figsize=(10,10)):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    column = 0\n",
    "    for i in range(len(list_of_images)):\n",
    "        column += 1\n",
    "        #  check for end of column and create a new figure\n",
    "        if column == no_of_columns+1:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            column = 1\n",
    "        fig.add_subplot(1, no_of_columns, column)\n",
    "        plt.imshow(list_of_images[i])\n",
    "        plt.axis('off')\n",
    "        if len(list_of_titles) >= len(list_of_images):\n",
    "            plt.title(list_of_titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGoqzrxCRT08"
   },
   "source": [
    "### Helper functions to  read and augment our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvSfs6DtQ8AQ"
   },
   "source": [
    "#### Character Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VXAe49Z3aeA"
   },
   "outputs": [],
   "source": [
    "#We will declare a class Character for every character in training and testing samples making it easy to read and visualize.\n",
    "\n",
    "#A single character class.\n",
    "\n",
    "class Character:\n",
    "    def __init__(self, dir_path, rotation=0):\n",
    "        self.dir_path = dir_path\n",
    "        self.rotation = rotation\n",
    "        self._cache = {}\n",
    "\n",
    "    def sample(self, num_images):\n",
    "      \n",
    "        \"\"\"\n",
    "        Sample images (as numpy arrays) from the class.\n",
    "        A sequence of 28x28 numpy arrays.\n",
    "        \"\"\"\n",
    "        \n",
    "        names = [f for f in os.listdir(self.dir_path) if f.endswith('.png')]\n",
    "        random.shuffle(names)\n",
    "        images = []\n",
    "        for name in names[:num_images]:\n",
    "            images.append(self._read_image(os.path.join(self.dir_path, name)))\n",
    "        return images\n",
    "\n",
    "    def _read_image(self, path):\n",
    "      \n",
    "        \"\"\"\n",
    "        Read images from the given path.\n",
    "        28x28 numpy arrays. Each pixel ranges from 0 to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        if path in self._cache:\n",
    "            return self._cache[path]\n",
    "        with open(path, 'rb') as in_file:\n",
    "            img = Image.open(in_file).resize((28, 28)).rotate(self.rotation)\n",
    "            self._cache[path] = np.array(img).astype('float32')\n",
    "            return self._cache[path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXLnl9VOTGSs"
   },
   "source": [
    "#### Functions to read and augment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPizJubXR9Y7"
   },
   "outputs": [],
   "source": [
    "# Iterate over the characters in a data directory and read them..\n",
    "\n",
    "def read_dataset(data_dir):\n",
    "    for alphabet_name in sorted(os.listdir(data_dir)):\n",
    "        alphabet_dir = os.path.join(data_dir, alphabet_name)\n",
    "        if not os.path.isdir(alphabet_dir):\n",
    "            continue\n",
    "        for char_name in sorted(os.listdir(alphabet_dir)):\n",
    "            if not char_name.startswith('character'):\n",
    "                continue\n",
    "            yield Character(os.path.join(alphabet_dir, char_name), 0)\n",
    "\n",
    "#Augment the dataset by adding 90 degree rotations.\n",
    "\n",
    "def augment_dataset(dataset):\n",
    "    for character in dataset:\n",
    "        for rotation in [0, 90, 180, 270]:\n",
    "            yield Character(character.dir_path, rotation=rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0v7CU4MPl81"
   },
   "source": [
    "### Helper functions for variable manipulation\n",
    "\n",
    "We create a class called VariableState to manage the state and to save and restore session variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpmWKe8WTBmX"
   },
   "outputs": [],
   "source": [
    "def interpolate(old_vars, new_vars, epsilon):\n",
    "    \"\"\"\n",
    "    Interpolate between two sequences of variables.\n",
    "    \"\"\"\n",
    "    return add(old_vars, scale(subtract(new_vars, old_vars), epsilon))\n",
    "\n",
    "def average(var_seqs):\n",
    "    \"\"\"\n",
    "    Average a sequence of variable sequences.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for variables in zip(*var_seqs):\n",
    "        res.append(np.mean(variables, axis=0))\n",
    "    return res\n",
    "\n",
    "def subtract(var_seq_1, var_seq_2):\n",
    "    \"\"\"\n",
    "    Subtract one variable sequence from another.\n",
    "    \"\"\"\n",
    "    return [v1 - v2 for v1, v2 in zip(var_seq_1, var_seq_2)]\n",
    "\n",
    "def add(var_seq_1, var_seq_2):\n",
    "    \"\"\"\n",
    "    Add two variable sequences.\n",
    "    \"\"\"\n",
    "    return [v1 + v2 for v1, v2 in zip(var_seq_1, var_seq_2)]\n",
    "\n",
    "def scale(var_seq, scale):\n",
    "    \"\"\"\n",
    "    Scale a variable sequence.\n",
    "    \"\"\"\n",
    "    return [v * scale for v in var_seq]\n",
    "  \n",
    "def weight_decay(rate, variables=None):\n",
    "    \"\"\"\n",
    "    Create an Op that performs weight decay.\n",
    "    \"\"\"\n",
    "    if variables is None:\n",
    "        variables = tf.trainable_variables()\n",
    "    ops = [tf.assign(var, var * rate) for var in variables]\n",
    "    return tf.group(*ops)\n",
    "\n",
    "class VariableState:\n",
    "    \"\"\"\n",
    "    Manage the state of a set of variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, session, variables):\n",
    "        self._session = session\n",
    "        self._variables = variables\n",
    "        self._placeholders = [tf.placeholder(v.dtype.base_dtype, shape=v.get_shape()) for v in variables]\n",
    "        assigns = [tf.assign(v, p) for v, p in zip(self._variables, self._placeholders)]\n",
    "        self._assign_op = tf.group(*assigns)\n",
    "\n",
    "    def export_variables(self):\n",
    "        \"\"\"\n",
    "        Save the current variables.\n",
    "        \"\"\"\n",
    "        return self._session.run(self._variables)\n",
    "\n",
    "    def import_variables(self, values):\n",
    "        \"\"\"\n",
    "        Restore the variables.\n",
    "        \"\"\"\n",
    "        self._session.run(self._assign_op, feed_dict=dict(zip(self._placeholders, values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iz4t92puCarE"
   },
   "source": [
    "### Helper Functions for returning (key,value) pair arguments  (kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KNO4X1hCc08"
   },
   "outputs": [],
   "source": [
    "#Declare helper functions to build kwargs\n",
    "\n",
    "def model_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build the kwargs for model constructors.\n",
    "    \"\"\"\n",
    "    res = {'learning_rate': args['learning_rate']}\n",
    "    return res\n",
    "  \n",
    "def train_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build kwargs for the train() function.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'num_classes': args['classes'],\n",
    "        'num_shots': args['shots'],\n",
    "        'train_shots': (args['train_shots'] or None),\n",
    "        'inner_batch_size': args['inner_batch'],\n",
    "        'inner_iters': args['inner_iters'],\n",
    "        'replacement': args['replacement'],\n",
    "        'meta_step_size': args['meta_step'],\n",
    "        'meta_step_size_final': args['meta_step_final'],\n",
    "        'meta_batch_size': args['meta_batch'],\n",
    "        'meta_iters': args['meta_iters'],\n",
    "        'eval_inner_batch_size': args['eval_batch'],\n",
    "        'eval_inner_iters': args['eval_iters'],\n",
    "        'eval_interval': args['eval_interval'],\n",
    "        'weight_decay_rate': args['weight_decay'],\n",
    "        'transductive': args['transductive'],\n",
    "        'model_fn': _args_model(args)\n",
    "    }\n",
    "\n",
    "  \n",
    "def evaluate_kwargs(args):\n",
    "    \"\"\"\n",
    "    Build kwargs for the evaluate() function.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'num_classes': args['classes'],\n",
    "        'num_shots': args['shots'],\n",
    "        'eval_inner_batch_size': args['eval_batch'],\n",
    "        'eval_inner_iters': args['eval_iters'],\n",
    "        'replacement': args['replacement'],\n",
    "        'weight_decay_rate': args['weight_decay'],\n",
    "        'num_samples': args['eval_samples'],\n",
    "        'transductive': args['transductive'],\n",
    "        'model_fn': _args_model(args)\n",
    "    }\n",
    "\n",
    "def _args_model(args):\n",
    "    \"\"\"\n",
    "    This return the FOML class (First-Order MAML)\n",
    "    \"\"\"\n",
    "    return partial(FOML, tail_shots=args['foml_tail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1EdHROGSN3n"
   },
   "source": [
    "## Defining our Model\n",
    "\n",
    "We create a class OmniglotModel to define our model. Our model is a 4-layer deep convolution network with ReLu as activation function, a full-connected layer at the end and a sparse softmax entropy function to calculate loss. We calculate predictions using argmax of the generated probalistic distribution of output of the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKaJv8WNSNWy"
   },
   "outputs": [],
   "source": [
    "# We are going to use Adam optimizer as our default optimzer.\n",
    "DEFAULT_OPTIMIZER = partial(tf.train.AdamOptimizer, beta1=0)\n",
    "\n",
    "#A model for Omniglot classification.\n",
    "class OmniglotModel:\n",
    "    def __init__(self, num_classes, optimizer=DEFAULT_OPTIMIZER, **optim_kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create input and label placeholders.Structure the model.\n",
    "        model.summary gives you an overview of the model\n",
    "        \"\"\"\n",
    "        self.input_ph = tf.placeholder(tf.float32, shape=(None, 28, 28))\n",
    "        out = tf.reshape(self.input_ph, (-1, 28, 28, 1))\n",
    "        for _ in range(4):\n",
    "            out = tf.layers.conv2d(out, 64, 3, strides=2, padding='same')\n",
    "            out = tf.layers.batch_normalization(out, training=True)\n",
    "            out = tf.nn.relu(out)\n",
    "        out = tf.reshape(out, (-1, int(np.prod(out.get_shape()[1:]))))\n",
    "        self.logits = tf.layers.dense(out, num_classes)\n",
    "        self.summary.append(self.logits)\n",
    "        self.label_ph = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_ph,\n",
    "                                                                   logits=self.logits)\n",
    "        self.predictions = tf.argmax(self.logits, axis=-1)\n",
    "        self.minimize_op = optimizer(**optim_kwargs).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Di9-cWimTB9g"
   },
   "source": [
    "## First-Order MAML implementation\n",
    "\n",
    "We'll approch this implementation in two steps\n",
    "\n",
    "*   Creating helper functions\n",
    "\n",
    "*   Declaring a class FOML.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTaGIO69VNnn"
   },
   "source": [
    "### Creating helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PN5AmBkTCR7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    _mini_batches():\n",
    "    \n",
    "    This fucntion generates mini-batches from data.\n",
    "\n",
    "    Returns:\n",
    "      An iterable of sequences of (input, label) pairs,\n",
    "        where each sequence is a mini-batch.\n",
    "    \"\"\"\n",
    "def _mini_batches(samples, batch_size, num_batches, replacement):\n",
    "    samples = list(samples)\n",
    "    if replacement:\n",
    "        for _ in range(num_batches):\n",
    "            yield random.sample(samples, batch_size)\n",
    "        return\n",
    "    cur_batch = []\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        random.shuffle(samples)\n",
    "        for sample in samples:\n",
    "            cur_batch.append(sample)\n",
    "            if len(cur_batch) < batch_size:\n",
    "                continue\n",
    "            yield cur_batch\n",
    "            cur_batch = []\n",
    "            batch_count += 1\n",
    "            if batch_count == num_batches:\n",
    "                return\n",
    "\n",
    "\"\"\"\n",
    "    _sample_mini_dataset_():\n",
    "    \n",
    "    This function creates a few shot task from the dataset, given number of shots and classes. .\n",
    "\n",
    "    Returns:\n",
    "      An iterable of (input, label) pairs.\n",
    "    \"\"\"\n",
    "\n",
    "def _sample_mini_dataset(dataset, num_classes, num_shots):\n",
    "   \n",
    "    shuffled = list(dataset)\n",
    "    random.shuffle(shuffled)\n",
    "    for class_idx, class_obj in enumerate(shuffled[:num_classes]):\n",
    "        for sample in class_obj.sample(num_shots):\n",
    "            yield (sample, class_idx)\n",
    "            \n",
    "\"\"\"\n",
    "    _split_train_test():\n",
    "    \n",
    "    This fucntion splits a few-shot task into a train and a test set, given an iterable of (input, label) pairs.\n",
    "    test_shots: the number of examples per class in the test set.\n",
    "\n",
    "    Returns:\n",
    "      A tuple (train, test), where train and test are\n",
    "        sequences of (input, label) pairs.\n",
    "    \"\"\"\n",
    "def _split_train_test(samples, test_shots=1):\n",
    "    train_set = list(samples)\n",
    "    test_set = []\n",
    "    labels = set(item[1] for item in train_set)\n",
    "    for _ in range(test_shots):\n",
    "        for label in labels:\n",
    "            for i, item in enumerate(train_set):\n",
    "                if item[1] == label:\n",
    "                    del train_set[i]\n",
    "                    test_set.append(item)\n",
    "                    break\n",
    "    if len(test_set) < len(labels) * test_shots:\n",
    "        raise IndexError('not enough examples of each class for test set')\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3AjdjwwTrKM"
   },
   "source": [
    "### Declaring a class FOML (FOML implementation) \n",
    "\n",
    "Let's break this down. (Please check comments in the code for respective steps)\n",
    "\n",
    "Please refer to 'Helper functions for variable manipulation' in Helper Functions section to know about the functions used in this section.\n",
    "\n",
    "\n",
    "\n",
    "*   Step 1 - Initialize a session of FOML\n",
    "*   Step 2 -  In the train_step, we initialize old variables from model state and new variables array to save them.\n",
    "* Step 3 - Looping through our meta bacth size, we create a mini dataset from the dataset according to number of classes and shots.\n",
    "* Step 4 - Now we create mini-batches from this mini dataset created.\n",
    "* Step 5 - For every batch in mini batches, take input, labels from batch and run the session on our minimize_op from model (Adam Optimizer).\n",
    "* Step 6 - Once it is done, we append the model's state varibales into the new_vars array.\n",
    "* Step 7 - Restore the state variables for next iteration.\n",
    "* Step 8 -  Once all the training set is looped through the meta batch size, we take an average of all the new variables saved until now.\n",
    "* Step 9 -  We perform update of state variables by interpolating the old and new variables with meta step size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7RpNO1E7YRdQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A basic implementation of \"first-order MAML\" (FOML).\n",
    "\n",
    "    FOML uses the gradient from the last mini-batch as the update\n",
    "    direction.\n",
    "\n",
    "    There are two ways to sample batches for FOML.\n",
    "    By default, FOML samples batches such that final mini-batch may overlap with\n",
    "    the previous mini-batches.\n",
    "    Alternatively, if tail_shots is specified, then a\n",
    "    separate mini-batch is used for the final step.\n",
    "    This final mini-batch is guaranteed not to overlap\n",
    "    with the training mini-batches.\n",
    "    \n",
    "    \n",
    "    It can operate in two evaluation modes: normal\n",
    "    and transductive. In transductive mode, information is\n",
    "    allowed to leak between test samples via BatchNorm.\n",
    "    Typically, MAML is used in a transductive manner.\n",
    "    \"\"\"\n",
    "\n",
    "class FOML():\n",
    "\n",
    "    def __init__(self, session, variables=None, transductive=False, pre_step_op=None, tail_shots=None):\n",
    "      \n",
    "        #STEP-1\n",
    "        \"\"\"\n",
    "        Create a first-order MAML session.\n",
    "\n",
    "        Args:\n",
    "          tail_shots: if specified, this is the number of\n",
    "            examples per class to reserve for the final\n",
    "            mini-batch.\n",
    "          pre_step_op : if specified, this is the pre-step operation we do,which is generally the weight decay.\n",
    "          transductive : this is to specify to evaluate all samples at once.\n",
    "        \"\"\"\n",
    "        self.session = session\n",
    "        self._model_state = VariableState(self.session, variables or tf.trainable_variables())\n",
    "        self._full_state = VariableState(self.session, tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "        self._transductive = transductive\n",
    "        self._pre_step_op = pre_step_op\n",
    "        self.tail_shots = tail_shots\n",
    "        \n",
    "    #This is the training function for each step which takes the dataset create mini batches and train them.\n",
    "    #Also updates the weights to model state.\n",
    "\n",
    "    def train_step(self,\n",
    "                   dataset,\n",
    "                   input_ph,\n",
    "                   label_ph,\n",
    "                   minimize_op,\n",
    "                   num_classes,\n",
    "                   num_shots,\n",
    "                   inner_batch_size,\n",
    "                   inner_iters,\n",
    "                   replacement,\n",
    "                   meta_step_size,\n",
    "                   meta_batch_size):\n",
    "        #STEP-2\n",
    "        old_vars = self._model_state.export_variables()\n",
    "        new_vars = []\n",
    "        for _ in range(meta_batch_size):\n",
    "            #STEP-3\n",
    "            mini_dataset = _sample_mini_dataset(dataset, num_classes, num_shots)\n",
    "            #STEP-4\n",
    "            mini_batches = _mini_batches(mini_dataset, inner_batch_size, inner_iters,\n",
    "                                              replacement)\n",
    "            #STEP-5\n",
    "            for batch in mini_batches:\n",
    "                inputs, labels = zip(*batch)\n",
    "                if self._pre_step_op:\n",
    "                    self.session.run(self._pre_step_op)\n",
    "                self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "            #STEP-6\n",
    "            new_vars.append(self._model_state.export_variables())\n",
    "            #STEP-7\n",
    "            self._model_state.import_variables(old_vars)\n",
    "        #STEP-8\n",
    "        new_vars = average(new_vars)\n",
    "        #STEP-9\n",
    "        self._model_state.import_variables(interpolate(old_vars,new_vars,meta_step_size))\n",
    "        \n",
    "    \n",
    "    def _mini_batches(self, mini_dataset, inner_batch_size, inner_iters, replacement):\n",
    "        \"\"\"\n",
    "        Generate inner-loop mini-batches for the task.\n",
    "        \"\"\"\n",
    "        if self.tail_shots is None:\n",
    "            #This call the external _mini_batch function if tail_shots is none.\n",
    "            for value in _mini_batches(mini_dataset, inner_batch_size, inner_iters, replacement):\n",
    "                yield value\n",
    "            return\n",
    "        train, tail = _split_train_test(mini_dataset, test_shots=self.tail_shots)\n",
    "        for batch in _mini_batches(train, inner_batch_size, inner_iters - 1, replacement):\n",
    "            yield batch\n",
    "        yield tail\n",
    "        \n",
    "        \n",
    "    def make_one_shot(self,\n",
    "                 dataset,\n",
    "                 input_ph,\n",
    "                 label_ph,\n",
    "                 minimize_op,\n",
    "                 predictions,\n",
    "                 num_classes,\n",
    "                 num_shots,\n",
    "                 inner_batch_size,\n",
    "                 inner_iters,\n",
    "                 replacement):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate One-Shot tasks from the given dataset.\n",
    "        \n",
    "         Args:\n",
    "          dataset: a sequence of data classes, where each data\n",
    "            class has a sample(n) method.\n",
    "          input_ph: placeholder for a batch of samples.\n",
    "          label_ph: placeholder for a batch of labels.\n",
    "          minimize_op: TensorFlow Op to minimize a loss on the\n",
    "            batch specified by input_ph and label_ph.\n",
    "          predictions: a Tensor of integer label predictions.\n",
    "          num_classes: number of data classes to sample.\n",
    "          num_shots: number of examples per data class.\n",
    "          inner_batch_size: batch size for every inner-loop\n",
    "            training iteration.\n",
    "          inner_iters: number of inner-loop iterations.\n",
    "          replacement: sample with replacement.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        A tuple of (predictions,samples) generated from the dataset. Few-Shot tasks comprise of samples and support set.\n",
    "        \"\"\"\n",
    "        train_set, test_set = _split_train_test(\n",
    "            _sample_mini_dataset(dataset, num_classes, num_shots+1))\n",
    "        for batch in _mini_batches(train_set, inner_batch_size, inner_iters, replacement):\n",
    "            inputs, labels = zip(*batch)\n",
    "            if self._pre_step_op:\n",
    "                self.session.run(self._pre_step_op)\n",
    "            self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "        test_preds = self._test_predictions(train_set, test_set, input_ph, predictions)\n",
    "        pred_sample = zip(test_preds, test_set)\n",
    "        return pred_sample\n",
    "        \n",
    "    def evaluate(self,\n",
    "                 dataset,\n",
    "                 input_ph,\n",
    "                 label_ph,\n",
    "                 minimize_op,\n",
    "                 predictions,\n",
    "                 num_classes,\n",
    "                 num_shots,\n",
    "                 inner_batch_size,\n",
    "                 inner_iters,\n",
    "                 replacement):\n",
    "        \"\"\"\n",
    "        Run a single evaluation of the model.\n",
    "\n",
    "        Samples a few-shot learning task and measures\n",
    "        performance.\n",
    "\n",
    "        Args:\n",
    "          dataset: a sequence of data classes, where each data\n",
    "            class has a sample(n) method.\n",
    "          input_ph: placeholder for a batch of samples.\n",
    "          label_ph: placeholder for a batch of labels.\n",
    "          minimize_op: TensorFlow Op to minimize a loss on the\n",
    "            batch specified by input_ph and label_ph.\n",
    "          predictions: a Tensor of integer label predictions.\n",
    "          num_classes: number of data classes to sample.\n",
    "          num_shots: number of examples per data class.\n",
    "          inner_batch_size: batch size for every inner-loop\n",
    "            training iteration.\n",
    "          inner_iters: number of inner-loop iterations.\n",
    "          replacement: sample with replacement.\n",
    "\n",
    "        Returns:\n",
    "          The number of correctly predicted samples.\n",
    "            This always ranges from 0 to num_classes.\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        old_vars = self._full_state.export_variables()\n",
    "        pred_sample = self.make_one_shot(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes, num_shots,\n",
    "                                          inner_batch_size,\n",
    "                                          inner_iters, replacement=replacement)\n",
    "        num_correct = sum([pred == sample[1] for pred,sample in pred_sample])\n",
    "        self._full_state.import_variables(old_vars)\n",
    "        return num_correct\n",
    "    \n",
    "    #This functions return the predictions on the train and test set.\n",
    "    def _test_predictions(self, train_set, test_set, input_ph, predictions):\n",
    "        if self._transductive:\n",
    "            inputs, _ = zip(*test_set)\n",
    "            return self.session.run(predictions, feed_dict={input_ph: inputs})\n",
    "        res = []\n",
    "        for test_sample in test_set:\n",
    "            inputs, _ = zip(*train_set)\n",
    "            inputs += (test_sample[0],)\n",
    "            res.append(self.session.run(predictions, feed_dict={input_ph: inputs})[-1])\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXl3jnkRFBb2"
   },
   "source": [
    "## Train function\n",
    "\n",
    "Let's break this down. (Please check comments in the code for respective steps)\n",
    "\n",
    "\n",
    "*   Step 1 - Create a directory to save checkpoints of our model, if it doesn't exist\n",
    "*   Step 2 - Create a saver instance to save checkpoints.\n",
    "* Step 3 - Load the FOML class.\n",
    "* Step 4 - Create a accuracy placeholder and a summary for this placeholder.\n",
    "* Step 5 - Create a variable to merge summary and save graph of sessions in seperate file writers for train and test sets.\n",
    "* Step 6 - Initialze the session with global variables.\n",
    "* Step 7 - Loop through the meta iterations for training and create a curated meta step size from the fraction of training done to reduce the meta step size as we proceed through training.\n",
    "* Step 8 - Call the train_step function of our FOML class. This trains the model over several steps through the number of meta iterations.\n",
    "* Step 9 - For every number of evaluation interations, evaluate the model on the training set to get correct predictions.\n",
    "* Step 10 - Loop through the dataset and writer of train and test sets, call the evaluvate function of the class and save the predictions in a variable.\n",
    "* Step 11 - Feed the accuracy placeholder with the ratio of correct predictions to number of classes . Save the summary using merged and the accuracy placeholder.\n",
    "* Step 12 - Add this summary to the writer and append the accuracy into the accuracies array.\n",
    "* Step 13 - Print the batch number with train accuracy  and test accuracy.\n",
    "* Step 14 - For every 100 iterations, save a checkpoint of training for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Z3PX8SkFCOm"
   },
   "outputs": [],
   "source": [
    "def train(sess,\n",
    "          model,\n",
    "          train_set,\n",
    "          test_set,\n",
    "          save_dir,\n",
    "          num_classes,\n",
    "          num_shots,\n",
    "          inner_batch_size,\n",
    "          inner_iters,\n",
    "          replacement,\n",
    "          meta_step_size,\n",
    "          meta_step_size_final,\n",
    "          meta_batch_size,\n",
    "          meta_iters,\n",
    "          eval_inner_batch_size,\n",
    "          eval_inner_iters,\n",
    "          eval_interval,\n",
    "          weight_decay_rate,\n",
    "          train_shots,\n",
    "          transductive,\n",
    "          model_fn,\n",
    "          time_deadline=None,\n",
    "          log_fn=print):\n",
    "    \"\"\"\n",
    "    Train a model on a dataset.\n",
    "    \"\"\"\n",
    "    #STEP-1\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    #STEP-2\n",
    "    saver = tf.train.Saver()\n",
    "    #STEP-3\n",
    "    foml = model_fn(sess, transductive = transductive, pre_step_op = weight_decay(weight_decay_rate))\n",
    "    #STEP-4\n",
    "    accuracy_ph = tf.placeholder(tf.float32, shape=())\n",
    "    tf.summary.scalar('accuracy', accuracy_ph)\n",
    "    #STEP-5\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(save_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(save_dir, 'test'), sess.graph)\n",
    "    #STEP-6\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #STEP-7\n",
    "    for i in range(meta_iters):\n",
    "        frac_done = i / meta_iters\n",
    "        cur_meta_step_size = frac_done * meta_step_size_final + (1 - frac_done) * meta_step_size\n",
    "        #STEP-8\n",
    "        foml.train_step(train_set, model.input_ph, model.label_ph, model.minimize_op,\n",
    "                           num_classes=num_classes, num_shots=(train_shots or num_shots),\n",
    "                           inner_batch_size=inner_batch_size, inner_iters=inner_iters,\n",
    "                           replacement=replacement,\n",
    "                           meta_step_size=cur_meta_step_size, meta_batch_size=meta_batch_size)\n",
    "        #STEP-9\n",
    "        if i % eval_interval == 0:\n",
    "            accuracies = []\n",
    "            #STEP-10\n",
    "            for dataset, writer in [(train_set, train_writer), (test_set, test_writer)]:\n",
    "                correct = foml.evaluate(dataset, model.input_ph, model.label_ph,\n",
    "                                           model.minimize_op, model.predictions,\n",
    "                                           num_classes=num_classes, num_shots=num_shots,\n",
    "                                           inner_batch_size=eval_inner_batch_size,\n",
    "                                           inner_iters=eval_inner_iters, replacement=replacement)\n",
    "                #STEP-11\n",
    "                summary = sess.run(merged, feed_dict={accuracy_ph: correct/num_classes})\n",
    "                #STEP-12\n",
    "                writer.add_summary(summary, i)\n",
    "                writer.flush()\n",
    "                accuracies.append(correct / num_classes)\n",
    "            #STEP-13\n",
    "            log_fn('batch %d: train=%f test=%f' % (i, accuracies[0], accuracies[1]))\n",
    "        #STEP-14\n",
    "        if i % 100 == 0 or i == meta_iters-1:\n",
    "            saver.save(sess, os.path.join(save_dir, 'model.ckpt'), global_step=i)\n",
    "        if time_deadline is not None and time.time() > time_deadline:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jK0PRHaNHWB"
   },
   "source": [
    "## Evaluate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EiYF-NaNHuD"
   },
   "outputs": [],
   "source": [
    "def evaluate(sess,\n",
    "             model,\n",
    "             dataset,\n",
    "             num_classes,\n",
    "             num_shots,\n",
    "             eval_inner_batch_size,\n",
    "             eval_inner_iters,\n",
    "             replacement,\n",
    "             num_samples,\n",
    "             transductive,\n",
    "             model_fn,\n",
    "             weight_decay_rate):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset.\n",
    "    \"\"\"\n",
    "    foml = model_fn(sess,\n",
    "                         transductive=transductive,\n",
    "                         pre_step_op=weight_decay(weight_decay_rate))\n",
    "    print(\"In Evaluate\")\n",
    "    total_correct = 0\n",
    "    for _ in range(num_samples):\n",
    "        total_correct += foml.evaluate(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes=num_classes, num_shots=num_shots,\n",
    "                                          inner_batch_size=eval_inner_batch_size,\n",
    "                                          inner_iters=eval_inner_iters, replacement=replacement)\n",
    "    return total_correct / (num_samples * num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73ZL9rxwSDrr"
   },
   "source": [
    "## Read the train and test data into lists\n",
    "\n",
    "Please refer to 'Helper Functions to read and augment our dataset' of Helper Functions sections to know the functions used in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EndfIfx3SKl5"
   },
   "outputs": [],
   "source": [
    "#Reading the dataset and converting into train and test lists.\n",
    "train_set = read_dataset(train_dir)\n",
    "test_set = list(read_dataset(test_dir))\n",
    "train_set = list(augment_dataset(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to show samples of characters.\n",
    "def plot_images(path):\n",
    "    \"\"\"\n",
    "        Plot all 20 samples of a particular character of a language\n",
    "    \"\"\"\n",
    "    f, axarr = plt.subplots(5,4, figsize=(10,10))\n",
    "    images_list = []\n",
    "    for image in os.listdir(path):\n",
    "        image_path = os.path.join(path, image)\n",
    "        img = cv2.imread(image_path)\n",
    "        images_list.append(img)\n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            axarr[i,j].imshow(images_list.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images('train/Arcadian/character03/')\n",
    "print(\"Arcadian language, 20 samples of the third character.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images('test/Kannada/character05/')\n",
    "print(\"Kannada language, 20 samples of the fifth character.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbQuVgXnYwqA"
   },
   "source": [
    "## Training the model on train and test set\n",
    "\n",
    "\n",
    " Please refer to 'Helper Functions for returning (key,value) pair arguments (kwargs)' of Helper Functions sections to know the functions used in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-0DWnSiWZFD"
   },
   "source": [
    "### Initialize the model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "QzeFJo0oWZef",
    "outputId": "3fa3d8fd-29b3-468b-97fe-ddf88627eee6"
   },
   "outputs": [],
   "source": [
    "model = OmniglotModel(args['classes'], **model_kwargs(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"png.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isbHa_05XWBh"
   },
   "source": [
    "### Training the model (20,000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "2oum1XkrXWWK",
    "outputId": "eebe42b9-132c-49a4-8d00-99791a5c340b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if not args['pretrained']:\n",
    "        print('Training...')\n",
    "        train(sess, model, train_set, test_set, save_dir='checkpoints',**train_kwargs(args))\n",
    "    else:\n",
    "        print('Restoring from checkpoint...')\n",
    "        tf.train.Saver().restore(sess, tf.train.latest_checkpoint(args['checkpoint']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, just displaying accuracy for every 1000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training...\n",
    "\n",
    "batch 0: train=0.050000 test=0.150000\n",
    "batch 1000: train=0.650000 test=0.250000\n",
    "batch 2000: train=0.550000 test=0.350000\n",
    "batch 3000: train=0.600000 test=0.650000\n",
    "batch 4000: train=0.600000 test=0.750000\n",
    "batch 5000: train=0.800000 test=0.800000\n",
    "batch 6000: train=0.750000 test=0.600000\n",
    "batch 7000: train=0.650000 test=0.600000\n",
    "batch 8000: train=0.800000 test=0.750000\n",
    "batch 9000: train=0.850000 test=0.750000\n",
    "batch 10000: train=0.850000 test=0.800000\n",
    "batch 11000: train=0.750000 test=0.750000\n",
    "batch 12000: train=0.900000 test=0.700000\n",
    "batch 13000: train=0.750000 test=0.650000\n",
    "batch 14000: train=0.800000 test=0.650000\n",
    "batch 15000: train=0.950000 test=0.700000\n",
    "batch 16000: train=0.950000 test=0.800000\n",
    "batch 17000: train=1.000000 test=0.700000\n",
    "batch 18000: train=0.950000 test=0.800000\n",
    "batch 19000: train=0.900000 test=0.800000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training,close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNpMNQmIYX_p"
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the train and test accuracies after 20,000 epochs\n",
    "    \n",
    "#### Train Accuracy\n",
    "<img src=\"train.png\" style=\"display:inline;\"></img>\n",
    "#### Test Accuracy\n",
    "<img src=\"test.png\" style=\"display:inline;\"></img>\n",
    "#### Train vs Test\n",
    "<img src=\"train_test.png\" style=\"display:inline;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The n-shot, k-way task\n",
    "\n",
    "The ability of an algorithm to perform few-shot learning is typically measured by its performance on n-shot, k-way tasks. These are run as follows:\n",
    "\n",
    ">A model is given a query sample belonging to a new, previously unseen class\n",
    "\n",
    ">It is also given a support set, S, consisting of n examples each from k different unseen classes\n",
    "\n",
    ">The algorithm then has to determine which of the support set classes the query sample belongs to\n",
    "\n",
    "We are using one-shot 20-way tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generate n-shot, k-way tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fucntion which uses the make_one_shot fucntion of FOML to return predictions and samples of the given dataset.\n",
    "\n",
    "def create_one_shot(sess,\n",
    "             model,\n",
    "             dataset,\n",
    "             num_classes,\n",
    "             num_shots,\n",
    "             eval_inner_batch_size,\n",
    "             eval_inner_iters,\n",
    "             replacement,\n",
    "             num_samples,\n",
    "             transductive,\n",
    "             model_fn,\n",
    "             weight_decay_rate):\n",
    "    \"\"\"\n",
    "    Create a few-shot tasks on a dataset.\n",
    "    \"\"\"\n",
    "    foml = model_fn(sess,\n",
    "                         transductive=transductive,\n",
    "                         pre_step_op=weight_decay(weight_decay_rate))\n",
    "    pred_sample = foml.make_one_shot(dataset, model.input_ph, model.label_ph,\n",
    "                                          model.minimize_op, model.predictions,\n",
    "                                          num_classes=num_classes, num_shots=num_shots,\n",
    "                                          inner_batch_size=eval_inner_batch_size,\n",
    "                                          inner_iters=eval_inner_iters, replacement=replacement)\n",
    "    return pred_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call that function to generate one-shot 20-way tasks and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    prediction_samples = create_one_shot(sess,model,test_set, **eval_kwargs)\n",
    "prediction = []\n",
    "samples = []\n",
    "for pred, sample in prediction_samples:\n",
    "    prediction.append(pred)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting one-shot 20-way tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions to concatenate support set into one plot and to plot the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(samples):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc = len(samples)\n",
    "    h , w = samples[0][0].shape\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros(((n-1)*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i in range(len(samples)):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = samples[i][0]\n",
    "        y += 1\n",
    "        if y >= n and x <= (n-1):\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "def plot_oneshot_task(prediction , samples):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    p = samples[prediction][0]\n",
    "    ax1.set_title('Sample')\n",
    "    ax2.set_title('Support Set')\n",
    "    ax1.matshow(p, cmap='gray')\n",
    "    img = concat_images(samples)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a random prediction and plot the sample and support set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.randint(0,len(prediction)-1)\n",
    "plot_oneshot_task(prediction[x],samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation \n",
    "\n",
    "Given this sample from unseen data and support set to the model, it determines the class of the query sample from the support set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training accuracy for 100 random one-shot 20-way tasks generated from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiKPgR_u4Ly7"
   },
   "outputs": [],
   "source": [
    "args['eval_samples'] = 100\n",
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    print('Train accuracy: ' + str(evaluate(sess, model, train_set, **eval_kwargs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing accuracy for 100 random one-shot 20-way tasks generated from testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['eval_samples'] = 100\n",
    "eval_kwargs = evaluate_kwargs(args)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('checkpoints/model.ckpt-19999.meta')\n",
    "    saver.restore(sess, 'checkpoints/model.ckpt-19999')\n",
    "    print('Test accuracy: ' + str(evaluate(sess, model, test_set, **eval_kwargs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis  (20-dimensional data)\n",
    "\n",
    "Please refer to 'Helper function to plot images in a row' of Helper Functions sections to know the functions used in this section.\n",
    "\n",
    "256 points plotted over 20-dimensions\n",
    "\n",
    "\n",
    "- Total Variance for Fig 1 (Dense/Kernel): 42.7%\n",
    "- Total Variance for Fig 2 (Dense/Kernel/Adam): 99.2%\n",
    "- Total Variance for Fig 3 (Dense/Kernel/Adam_1): 98.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = cv2.imread(\"B_A.png\")\n",
    "img_B = cv2.imread(\"B_B.png\")\n",
    "img_C = cv2.imread(\"B_C.png\")\n",
    "\n",
    "titles = [\"Dense/kernel\", \n",
    "          \"Dense/kernel/Adam\", \n",
    "          \"Dense/kernel/Adam_1\"]\n",
    "images = [img_A,img_B,img_C]\n",
    "\n",
    "grid_display(images, titles, 3, (35,35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did the model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-of-the-art solution\n",
    "\n",
    "The state-of-the-art solution [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/pdf/1703.03400v3.pdf) on Few-Shot learning presented us with an accuracy of 98.7%. Taking into consideration the number of epochs trained and the GPU availability, I trained for 20,000 epochs and achieved an accuracy of 87.15%. If the model had trained for more epochs, I guess we might nearly achieve the state-of-the-art accuracy presented in ICML 2017. By Chelsea Finn • Pieter Abbeel • Sergey Levine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial training algorithm (Scaled updates)\n",
    "\n",
    "Intially in the train_step function of class FOML, model state variables are updated from the last backup and this session's state variables, pushing the difference into an updates array and then average of these updates are scaled with meta step size and adding then to the old varibales of the model. \n",
    "\n",
    "But training on this way of train_step implementation, the accuracy was not improving over a lot of epochs. Please see the train vs test accuracy of scaled updates type of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in meta_iter:\n",
    "    #declare mini_batches\n",
    "    for batch in mini_batches:\n",
    "        inputs, labels = zip(*batch)\n",
    "        last_backup = self._model_state.export_variables()\n",
    "        self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "    updates.append(subtract(self._model_state.export_variables(), last_backup))\n",
    "    self._model_state.import_variables(old_vars)\n",
    "update = average(updates)\n",
    "self._model_state.import_variables(add(old_vars, scale(update, meta_step_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train vs Test Accuracy (Scaled Updates)\n",
    "\n",
    "Legend: \n",
    "\n",
    "Orange - Train , Blue - Test\n",
    "\n",
    "<img src=\"train_test_su.png\">\n",
    "\n",
    "\n",
    "As you can see over 4k epoch (12k - 16k), accuracy was not improving, so I had to stop training and tweak the train_step implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaked Training algorithm (Interpolating updates)\n",
    "\n",
    "This tweaked algorithms appends the new state varibales into an array and we calculate the average of it. Now, we save the new model state variables by interpolating the old variables and new variables of the model state with the meta step size.\n",
    "\n",
    "At present, the model was trained on this tweaked train_step function implementation. We achieved an accuracy of 87.15% over 20,000 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in meta_iter:\n",
    "    #declare mini_batches\n",
    "    for batch in mini_batches:\n",
    "        inputs, labels = zip(*batch)\n",
    "        self.session.run(minimize_op, feed_dict={input_ph: inputs, label_ph: labels})\n",
    "    new_vars.append(self._model_state.export_variables())\n",
    "    self._model_state.import_variables(old_vars)\n",
    "new_vars = average(new_vars)\n",
    "self._model_state.import_variables(interpolate(old_vars,new_vars,meta_step_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train vs Test Accuracy (Interpolated Updates)\n",
    "\n",
    "Legend: \n",
    "\n",
    "Blue - Train , Orange - Test\n",
    "\n",
    "<img src=\"train_test.png\">\n",
    "\n",
    "As you can see, compared to the initial algorithm, this one performed better over the training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "\n",
    "All the above plots and model architecture were obtained using Tensorboard. To use Tensorboard, during or after the training, provide the log directory to tensorboard command in terminal.\n",
    "\n",
    "Open the terminal, go to the repo and run this command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "* Training the present model on more epochs,on a better GPU to nearly achieve the state-of-the-art solution accuracy\n",
    "* Refering to a paper [How to Train your MAML](https://openreview.net/forum?id=HJGven05Y7) by Antreas Antoniou, Harrison Edwards, Amos Storkey, we get more insights on the problems of vanilla MAML.\n",
    "* This paper proposes several improvements for the MAML algorithm that improve its stability and performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MOkkRtKgP8vn",
    "SDbXkSguzcRl",
    "YGoqzrxCRT08",
    "KvSfs6DtQ8AQ",
    "VXLnl9VOTGSs",
    "X0v7CU4MPl81",
    "T1EdHROGSN3n",
    "Di9-cWimTB9g",
    "tTaGIO69VNnn",
    "k3AjdjwwTrKM",
    "WXl3jnkRFBb2",
    "9jK0PRHaNHWB"
   ],
   "name": "One-Shot Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
